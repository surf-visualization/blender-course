{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Warning This course material is in the process of being updated to Blender 3.1 Welcome to the course Introduction to Scientific Visualization with Blender ! This course is provided by SURF through the PRACE Training Center . We have been providing this course since 2018, usually twice a year, and always enjoy teaching it in-person. Due to the restrictions during the COVID-19 pandemic we have decided to turn this course into a fully online version, based on positive experiences with the first advanced Blender course we provided online in 2020.","title":null},{"location":"advanced/advanced_materials/advanced_materials_assignment/","text":"\ud83d\udcbb The Shader Editor and advanced material topics \u00b6 In this exercise you will use the Blender Shader Editor on the familiar iso-surface of a CT scan of a fish from the basic course and try to make a visualization by using an advanced node setup. After that you will make a render of the moon with the high resolution textures of NASA with adaptive subdivision. The fish \u00b6 When you opened the exercise blend file advanced_materials_assignment.blend you'll see the white fish iso-surface above a plane white plane. We are going to pimp this scene with advanced materials. Shader editor materials - Coloring the scene \u00b6 First we will add materials and give each object a different color. First activate the Rendered shading to see what kind of materials we are actually applying by pressing Z in the 3D Viewport panel and selecting Rendered from the radial pie-menu. Select the fishskin object and add a new material by clicking the New button in the middle of the top bar of the Shader Editor panel. Now we see a graph appearing with 2 nodes a Principled BSDF -node and a Material output -node also in the side panel you will see the familiar material settings. Change the Base Color to an appropriate color of a fish. Repeat step 2 and 3 for each 3D object in the scene (see Outliner ) and give them a color of your choice. Texture mapping - Placing the fish on a picknick table \u00b6 Now that the scene has some color we can start applying some realistic colors and texture to the ground plane or should we say table? We will do that by adding wood textures to the ground plane and connecting those textures to their appropriate parameters of the Principle BSDF. Select the groundplane 3D object. Add a Image texture -node to the Shader Editor graph of the groundplane with Shift-A > Texture > Image Texture . Connect the Color output of this node to the Base color input of the Principled BSDF -node. Now the groundplane doesn't look anything like a picknick table, its pink. This pink color comes from the fact that an image is missing from the Image Texture -node. Open an image by pressing the Open -button on the Image Texture -node, this will open a file browser window. Now select the blue_painted_planks_diff_4k.png image from the data/wood_textures/ directory and press Open Image . Now we have our first image mapped on an object! Although you might have noticed that the fish is really small or rather the planks are very big. We are gonna solve that by scaling the texture coordinates. Before we can do that we first need to add the texture coordinates to the graph with Shift-A > Input > Texture Coordinates and connect the UV output to the Vector input of the Image Texture -node. Nothing changed because we didn't apply the scaling yet. Now add a Mapping node with Shift-A > Vector > Mapping and drag it on top of the edge between the Texture Coordinate -node and the Image Texture -node and let it go. As you can see it is automatically connected in between the nodes. Now on the Mapping -node change the Scale parameter x , y and z to 2 . As you can see that reduced the planks to a smaller and better size. Tip! : With the Node Wrangler Blender add-on you can just select a texture node and press CTRL+T to automatically add the Texture Coordinate and Mapping node. Node Wrangler can be added with: Menu-bar Edit > Preferences > Add-ons tab > Type 'Node Wranger' in search > check Node Wrangler add-on to activate . Now we'll roughen the planks a bit with a Roughness map , a texture that will be use to change the Roughness parameter of the Principled BSDF. Select the previously added Image Texture -node and press SHIFT-D and place the new duplicated node underneath the other Image Texture -node. Connect its Vector input to the Vector output of the Mapping -node just like the other Image Texture -node and connect the Color output to the Roughness input of the Principled BSDF -node. As you can see became shiny, which wood is not (rotate the view around the object in the 3D Viewport to see the plane from different angles). This is because we haven't changed the texture yet. In this new Image Texture -node Open the blue_painted_planks_rough_4k.png from data/wood_textures . Now it is still a bit too shiny for wood. This is because the output is interpreted as an sRGB value. We need to change the Color Space parameter of this Image Texture -node to Non-color . Now the ground plane has the right rough look like wood. The look of the wood is still very \"flat\" (the light still bounces of it at a straight angle), this is because we didn't add a normal map to the material yet. This normal map will accentuate all the nooks and crannies naturally present in wood which normally catch light to. As the previous Image Texture -node we again need to make a new one by duplicating (see step 8 ). Again the Mapping -node Vector output needs to be connected to the new Image Texture -node Vector input. The Color output however needs to go to a Normal Map -node. Add a Normal Map -node with Shift-A > Vector > Normal Map and connect the Image Texture -node Color output to the Normal Map -node Color input and connect the Normal Map -node Normal output to the Principled BSDF -node Normal input. Again this is also not a color so the Color Space needs to be set to Non-color . Now you have a fully textured wooden ground plane! To see the full effect, rotate the view around it and see the light bounce off the surface based on the different texture types you just applied. Multiple materials one object - Window to the inside of the fish \u00b6 We only see the fish, not the fish bones. In the Blender Basics course we learned how to reveal the bones on the inside by using a Boolean modifier, but we can achieve the same with just materials! Select the fishskin 3D object. If everything in the first couple of assignments the fish should already have one material called Material . For administrative reasons lets rename the material by clicking its name Material in the middle of the top bar of the Shader Editor panel and typing the new name called fishskinmat . Now left next to the rename box you have drop-down menu called Slot 1 when you click this you will see the material slots menu. In our case its only one material called fishskinmat . Now add a new Material slot by clicking the plus icon in this menu. The added material slot is still empty and needs a second material. Add a new material by clicking the New button in the middle of the top bar of the Shader Editor panel. Rename this material to fishskintransparentmat . Now as you can see adjusting any value on the Principled BSDF -node doesn't seem to do anything. This is because there aren't any vertices assigned to this material slot yet (by default all vertices are assigned to the first material slot). To assign vertices we need to be able to select them and this can be done in the Edit Mode of the 3D Viewport -panel. With the fishskin 3D object selected and the focus on the 3D Viewport -panel (hovering over the 3D Viewport panel with your mouse) press TAB . First press 1 to see the vertices and then select a window of vertices on the side of the fish with the Border select tool by pressing B in the 3D Viewport -panel and dragging over the area you want to select. With these vertices selected press the Material slots button, select the fishskintransparentmat -material and press the Assign -button. Now you can see the selected faces in that selection look different! This is because they are assigned to the second material. Now we'll make the fishskintransparentmat actually transparent with a combination of the Transparent BSDF and Principled BSDF through a Mix Shader . That way we can control the amount of transparency! In the Shader editor add a Mix Shader -node with Shift-A > Shader > Mix Shader . Drag this Mix Shader -node over the edge connecting the Principled BSDF -node and the Material Output -node to place it connected in between. Now add a Transparent BSDF with Shift-A > Shader > Transparent BSDF . Connect the BSDF output to the Mix Shader -node Shader input. Now the material is half shaded by the Transparent BSDF -node and half by the Principled BSDF -node. Experiment with the Mix shader -node's fac parameter to see how it changes the transparency of the fishskintransparentmat . Now you have a window looking inside the fish! Now it's time to give the fish some actually fishy colors with the Project from view UV-mapping! Bonus (Only when you have time left): As you can see the bones also contain the swim bladder which looks the same as the bones because the same material is assigned to it. Try to select the swim bladders vertices and assign a different more fitting material to the swim bladder. Project from view UV-mapping - Add actual skin to the fish. \u00b6 To add a real fish texture, or actually a photo from a carp, to the fishskin 3D object you can use the technique called Project from view UV-mapping. For this we introduce a new panel called the UV Editor . Before we go to the UV Editor we need to add a Image Texture -node to the fishskinmat . In the Shader Editor select the fishskinmat (slot 1) from the Material slot menu in the middle left of the top bar of the Shader Editor . Add a Image Texture -node to the material with Shift-A > Texture > Image Texture and connect the Color output to the Principled BSDF -node Base Color input and open the carp.jpg texture from the data/ directory. Next add a Texture Coordinate node with Shift-A > Input > Texture Coordinates and connect the UV output to the Image texture -node Vector input. This fish is now black because the UV coordinates are not defined yet. That is what we will do in the UV Editor . Now that we do not need the Shader editor anymore we can replace it with the UV Editor . In the corner of the panel click the Editor Type-button and select the UV Editor from the list. Before we can start UV-mapping we need to be in Edit mode in the 3D viewport . In the 3D viewport panel press TAB to enter edit mode. Now select all geometry by pressing A . To properly project from view you have to choose the right view to project from. We are gonna map a photo of a carp which has been taken from the side. In order to properly map the photo on the 3D object we also need to look at it from the side. Press BACK-TICK to open the view radial pie-menu and select Right or through the 3D Viewport menu in the header ( View > Viewpoint > Camera ). Now press U to open the UV-mapping -menu and select Project from view . Now you can see the UV coordinates are mapped in the UV Editor but they are not properly scaled to fit the photo of the carp. Make sure that everything is still selected and then within the UV Editor press S and scale the UV-coordinates until they aligns with the photo of the carp. Scaling it alone is not enough. The UV-coordinates need to be moved a bit, use G to grab the UV-coordinates and translate them to better match the photo. As you might have noticed it is not possible to completely match the photo without deforming the UV-coordinates. Before we start deforming parts of the UV-coordinates you need to activate Proportional editing by pressing the Proportional editing button in the top bar of the UV Editor . This proportional editing moves all UV-coordinates in the adjacent defined radius along with the currently selected UV-coordinates. Now select a UV-coordinate in the UV Editor that needs to be moved and press G . While grabbing, scroll with you mouse wheel to decrease or increase the Proportional editing radius and move your mouse to see the effect. Now with this Proportional editing try to match the UV-coordinates to the photo of the carp as good as possible. Tip! : Whenever you are editing the UV-map in the UV editor it can be difficult to see how the texture is mapped on the 3D-object because of the visibility of all vertices, edges and faces because of the activated Edit mode . You can toggle between Edit mode and Object mode in the 3D Viewport panel to have a better look at the mapped texture. The moon (EXERCISE) \u00b6 The moon exercise doesn't have a prepared blend file because you are gonna make it all by yourself! So open a new blend file and start to make the moon. The basic scene - Sphere, sun and the darkness of space \u00b6 To create the moon we first need to prepare a very simple scene. First off we need to remove the Default cube (the cube that comes with a new blend file which only function is to be removed :'( ). Add a UV Sphere instead with Shift-A > Mesh > UV sphere . Set the UV Sphere 's shading to smooth through the 3D Viewport menu in at the top of the 3D Viewport ( Object > Shade Smooth ). Select the default Light object in the Outliner and change it to a Sun light in the Light -tab in the Properties -panel on the right. Now change the shading in the 3D viewport to Rendered by pressing Z and then select Rendered . This rendered view is by default set to Eevee , to change that to Cycles for more realistic lighting go to the Render Properties -tab in the Properties -panel and change the Render Engine to Cycles . As you can see the sun is now way too bright. Lower the Strength of the sun from 1000 to 10 in the Light -tab in the Properties -panel. No need to have the power of a 1000 suns. Now that we have the sun we need to disable the World -lighting (the grey ambient light) since we only need the sun as a direct light source like it is in space. Go to the World properties -tab in the Properties -panel and set the Color in the Surface -section all the way to black. Now we have the basic scene of a sphere in space, now we are gonna make it look like the moon by adding textures. Applying a material and texturing the moon - That's one small step... \u00b6 Before we can edit the material we need to open the Shader Editor . For this we need to slightly modify the interface. Grab the edge between the 3D viewport -panel and the Timeline -panel by hovering above the edge until you see resize cursor then click and drag the edge until half of the Blender window. Now click the upper left Editor type dropdown menu (now the Timeline -icon ) and select the Shader Editor . In the Shader Editor add a new material. In this material add 2 Image Texture -nodes, 1 Texture Coordinate -node and 1 Displacement -node ( Shift-A > Vector > Displacement ). Connect the Texture Coordinate -node UV output to both Image Texture -nodes Vector inputs. Connect one of the Image Texture -nodes Color output to the Principled BSDF -node Base Color input and the others Color output to the Displacement -node Height input. Finally connect the Displacement -node Displacement output to the Material output -node Displacement input. Open the data/moon_textures/lroc_color_poles_8k.tif in the Image Texture -node that is connected to the Principled BSDF -node Base Color . Open the data/moon_textures/ldem_16.tif in the Image Texture -node that is connected to the Displacement -node Height input. Then finaly set the Image Texture -node Color Space -parameter of the node with the displacement texture to ** Non-Color . Initially the Displacement -node Scale parameter is set way too high making the moon look horrible. Set this parameter to 0.001 . As you can see it already looks quite like the moon but with some final tweaking you will get even more realism. Adaptive displacement - Revealing the craters! Mooore details! \u00b6 Everything we have seen until now has been rendered in the default EEVEE rendering engine, which is for visualization purposes very powerful, but if you want to add that extra little realism with adaptive displacement you have to use the Cycles rendering engine. Active the Cycles rendering engine with the Render Engine setting in the Rendering properties -tab of the Properties -panel. While we are there, to be able to use adaptive displacement, we need to activate the Cycles experimental feature set. Set the Feature Set to Experimental . This Experimental feature set added an extra section in the current properties panel tab called Subdivision . In this section set Viewport to 2 . Now we need to add a Subdivision modifier that also got a new setting from the Experimental feature set that enables the adaptive displacement . Add a Subdivision modfier in the Modifier properties -tab of the Properties -panel. Enable the Adaptive Subdivision setting in this modifier. Until now you only saw some slight differences because there is only one setting that has to be changed to make all of this worth it. Change the Displacement setting to Displacement Only in the Properties -panel > Material properties -tab > Settings -section > Surface -subsection. Now zoom in and toggle to the Edit mode and back, which re-triggers the adaptive subdivision computations, and see the craters in their full glory. Bonus : For an artists rendition of the moon change the Displacement -node Scale parameter to a higher value and see how the craters get more noticeable (although less realistic).","title":"\ud83d\udcbb The Shader Editor and advanced material topics"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#the-shader-editor-and-advanced-material-topics","text":"In this exercise you will use the Blender Shader Editor on the familiar iso-surface of a CT scan of a fish from the basic course and try to make a visualization by using an advanced node setup. After that you will make a render of the moon with the high resolution textures of NASA with adaptive subdivision.","title":"\ud83d\udcbb The Shader Editor and advanced material topics"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#the-fish","text":"When you opened the exercise blend file advanced_materials_assignment.blend you'll see the white fish iso-surface above a plane white plane. We are going to pimp this scene with advanced materials.","title":"The fish"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#shader-editor-materials-coloring-the-scene","text":"First we will add materials and give each object a different color. First activate the Rendered shading to see what kind of materials we are actually applying by pressing Z in the 3D Viewport panel and selecting Rendered from the radial pie-menu. Select the fishskin object and add a new material by clicking the New button in the middle of the top bar of the Shader Editor panel. Now we see a graph appearing with 2 nodes a Principled BSDF -node and a Material output -node also in the side panel you will see the familiar material settings. Change the Base Color to an appropriate color of a fish. Repeat step 2 and 3 for each 3D object in the scene (see Outliner ) and give them a color of your choice.","title":"Shader editor materials - Coloring the scene"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#texture-mapping-placing-the-fish-on-a-picknick-table","text":"Now that the scene has some color we can start applying some realistic colors and texture to the ground plane or should we say table? We will do that by adding wood textures to the ground plane and connecting those textures to their appropriate parameters of the Principle BSDF. Select the groundplane 3D object. Add a Image texture -node to the Shader Editor graph of the groundplane with Shift-A > Texture > Image Texture . Connect the Color output of this node to the Base color input of the Principled BSDF -node. Now the groundplane doesn't look anything like a picknick table, its pink. This pink color comes from the fact that an image is missing from the Image Texture -node. Open an image by pressing the Open -button on the Image Texture -node, this will open a file browser window. Now select the blue_painted_planks_diff_4k.png image from the data/wood_textures/ directory and press Open Image . Now we have our first image mapped on an object! Although you might have noticed that the fish is really small or rather the planks are very big. We are gonna solve that by scaling the texture coordinates. Before we can do that we first need to add the texture coordinates to the graph with Shift-A > Input > Texture Coordinates and connect the UV output to the Vector input of the Image Texture -node. Nothing changed because we didn't apply the scaling yet. Now add a Mapping node with Shift-A > Vector > Mapping and drag it on top of the edge between the Texture Coordinate -node and the Image Texture -node and let it go. As you can see it is automatically connected in between the nodes. Now on the Mapping -node change the Scale parameter x , y and z to 2 . As you can see that reduced the planks to a smaller and better size. Tip! : With the Node Wrangler Blender add-on you can just select a texture node and press CTRL+T to automatically add the Texture Coordinate and Mapping node. Node Wrangler can be added with: Menu-bar Edit > Preferences > Add-ons tab > Type 'Node Wranger' in search > check Node Wrangler add-on to activate . Now we'll roughen the planks a bit with a Roughness map , a texture that will be use to change the Roughness parameter of the Principled BSDF. Select the previously added Image Texture -node and press SHIFT-D and place the new duplicated node underneath the other Image Texture -node. Connect its Vector input to the Vector output of the Mapping -node just like the other Image Texture -node and connect the Color output to the Roughness input of the Principled BSDF -node. As you can see became shiny, which wood is not (rotate the view around the object in the 3D Viewport to see the plane from different angles). This is because we haven't changed the texture yet. In this new Image Texture -node Open the blue_painted_planks_rough_4k.png from data/wood_textures . Now it is still a bit too shiny for wood. This is because the output is interpreted as an sRGB value. We need to change the Color Space parameter of this Image Texture -node to Non-color . Now the ground plane has the right rough look like wood. The look of the wood is still very \"flat\" (the light still bounces of it at a straight angle), this is because we didn't add a normal map to the material yet. This normal map will accentuate all the nooks and crannies naturally present in wood which normally catch light to. As the previous Image Texture -node we again need to make a new one by duplicating (see step 8 ). Again the Mapping -node Vector output needs to be connected to the new Image Texture -node Vector input. The Color output however needs to go to a Normal Map -node. Add a Normal Map -node with Shift-A > Vector > Normal Map and connect the Image Texture -node Color output to the Normal Map -node Color input and connect the Normal Map -node Normal output to the Principled BSDF -node Normal input. Again this is also not a color so the Color Space needs to be set to Non-color . Now you have a fully textured wooden ground plane! To see the full effect, rotate the view around it and see the light bounce off the surface based on the different texture types you just applied.","title":"Texture mapping - Placing the fish on a picknick table"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#multiple-materials-one-object-window-to-the-inside-of-the-fish","text":"We only see the fish, not the fish bones. In the Blender Basics course we learned how to reveal the bones on the inside by using a Boolean modifier, but we can achieve the same with just materials! Select the fishskin 3D object. If everything in the first couple of assignments the fish should already have one material called Material . For administrative reasons lets rename the material by clicking its name Material in the middle of the top bar of the Shader Editor panel and typing the new name called fishskinmat . Now left next to the rename box you have drop-down menu called Slot 1 when you click this you will see the material slots menu. In our case its only one material called fishskinmat . Now add a new Material slot by clicking the plus icon in this menu. The added material slot is still empty and needs a second material. Add a new material by clicking the New button in the middle of the top bar of the Shader Editor panel. Rename this material to fishskintransparentmat . Now as you can see adjusting any value on the Principled BSDF -node doesn't seem to do anything. This is because there aren't any vertices assigned to this material slot yet (by default all vertices are assigned to the first material slot). To assign vertices we need to be able to select them and this can be done in the Edit Mode of the 3D Viewport -panel. With the fishskin 3D object selected and the focus on the 3D Viewport -panel (hovering over the 3D Viewport panel with your mouse) press TAB . First press 1 to see the vertices and then select a window of vertices on the side of the fish with the Border select tool by pressing B in the 3D Viewport -panel and dragging over the area you want to select. With these vertices selected press the Material slots button, select the fishskintransparentmat -material and press the Assign -button. Now you can see the selected faces in that selection look different! This is because they are assigned to the second material. Now we'll make the fishskintransparentmat actually transparent with a combination of the Transparent BSDF and Principled BSDF through a Mix Shader . That way we can control the amount of transparency! In the Shader editor add a Mix Shader -node with Shift-A > Shader > Mix Shader . Drag this Mix Shader -node over the edge connecting the Principled BSDF -node and the Material Output -node to place it connected in between. Now add a Transparent BSDF with Shift-A > Shader > Transparent BSDF . Connect the BSDF output to the Mix Shader -node Shader input. Now the material is half shaded by the Transparent BSDF -node and half by the Principled BSDF -node. Experiment with the Mix shader -node's fac parameter to see how it changes the transparency of the fishskintransparentmat . Now you have a window looking inside the fish! Now it's time to give the fish some actually fishy colors with the Project from view UV-mapping! Bonus (Only when you have time left): As you can see the bones also contain the swim bladder which looks the same as the bones because the same material is assigned to it. Try to select the swim bladders vertices and assign a different more fitting material to the swim bladder.","title":"Multiple materials one object - Window to the inside of the fish"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#project-from-view-uv-mapping-add-actual-skin-to-the-fish","text":"To add a real fish texture, or actually a photo from a carp, to the fishskin 3D object you can use the technique called Project from view UV-mapping. For this we introduce a new panel called the UV Editor . Before we go to the UV Editor we need to add a Image Texture -node to the fishskinmat . In the Shader Editor select the fishskinmat (slot 1) from the Material slot menu in the middle left of the top bar of the Shader Editor . Add a Image Texture -node to the material with Shift-A > Texture > Image Texture and connect the Color output to the Principled BSDF -node Base Color input and open the carp.jpg texture from the data/ directory. Next add a Texture Coordinate node with Shift-A > Input > Texture Coordinates and connect the UV output to the Image texture -node Vector input. This fish is now black because the UV coordinates are not defined yet. That is what we will do in the UV Editor . Now that we do not need the Shader editor anymore we can replace it with the UV Editor . In the corner of the panel click the Editor Type-button and select the UV Editor from the list. Before we can start UV-mapping we need to be in Edit mode in the 3D viewport . In the 3D viewport panel press TAB to enter edit mode. Now select all geometry by pressing A . To properly project from view you have to choose the right view to project from. We are gonna map a photo of a carp which has been taken from the side. In order to properly map the photo on the 3D object we also need to look at it from the side. Press BACK-TICK to open the view radial pie-menu and select Right or through the 3D Viewport menu in the header ( View > Viewpoint > Camera ). Now press U to open the UV-mapping -menu and select Project from view . Now you can see the UV coordinates are mapped in the UV Editor but they are not properly scaled to fit the photo of the carp. Make sure that everything is still selected and then within the UV Editor press S and scale the UV-coordinates until they aligns with the photo of the carp. Scaling it alone is not enough. The UV-coordinates need to be moved a bit, use G to grab the UV-coordinates and translate them to better match the photo. As you might have noticed it is not possible to completely match the photo without deforming the UV-coordinates. Before we start deforming parts of the UV-coordinates you need to activate Proportional editing by pressing the Proportional editing button in the top bar of the UV Editor . This proportional editing moves all UV-coordinates in the adjacent defined radius along with the currently selected UV-coordinates. Now select a UV-coordinate in the UV Editor that needs to be moved and press G . While grabbing, scroll with you mouse wheel to decrease or increase the Proportional editing radius and move your mouse to see the effect. Now with this Proportional editing try to match the UV-coordinates to the photo of the carp as good as possible. Tip! : Whenever you are editing the UV-map in the UV editor it can be difficult to see how the texture is mapped on the 3D-object because of the visibility of all vertices, edges and faces because of the activated Edit mode . You can toggle between Edit mode and Object mode in the 3D Viewport panel to have a better look at the mapped texture.","title":"Project from view UV-mapping - Add actual skin to the fish."},{"location":"advanced/advanced_materials/advanced_materials_assignment/#the-moon-exercise","text":"The moon exercise doesn't have a prepared blend file because you are gonna make it all by yourself! So open a new blend file and start to make the moon.","title":"The moon (EXERCISE)"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#the-basic-scene-sphere-sun-and-the-darkness-of-space","text":"To create the moon we first need to prepare a very simple scene. First off we need to remove the Default cube (the cube that comes with a new blend file which only function is to be removed :'( ). Add a UV Sphere instead with Shift-A > Mesh > UV sphere . Set the UV Sphere 's shading to smooth through the 3D Viewport menu in at the top of the 3D Viewport ( Object > Shade Smooth ). Select the default Light object in the Outliner and change it to a Sun light in the Light -tab in the Properties -panel on the right. Now change the shading in the 3D viewport to Rendered by pressing Z and then select Rendered . This rendered view is by default set to Eevee , to change that to Cycles for more realistic lighting go to the Render Properties -tab in the Properties -panel and change the Render Engine to Cycles . As you can see the sun is now way too bright. Lower the Strength of the sun from 1000 to 10 in the Light -tab in the Properties -panel. No need to have the power of a 1000 suns. Now that we have the sun we need to disable the World -lighting (the grey ambient light) since we only need the sun as a direct light source like it is in space. Go to the World properties -tab in the Properties -panel and set the Color in the Surface -section all the way to black. Now we have the basic scene of a sphere in space, now we are gonna make it look like the moon by adding textures.","title":"The basic scene - Sphere, sun and the darkness of space"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#applying-a-material-and-texturing-the-moon-thats-one-small-step","text":"Before we can edit the material we need to open the Shader Editor . For this we need to slightly modify the interface. Grab the edge between the 3D viewport -panel and the Timeline -panel by hovering above the edge until you see resize cursor then click and drag the edge until half of the Blender window. Now click the upper left Editor type dropdown menu (now the Timeline -icon ) and select the Shader Editor . In the Shader Editor add a new material. In this material add 2 Image Texture -nodes, 1 Texture Coordinate -node and 1 Displacement -node ( Shift-A > Vector > Displacement ). Connect the Texture Coordinate -node UV output to both Image Texture -nodes Vector inputs. Connect one of the Image Texture -nodes Color output to the Principled BSDF -node Base Color input and the others Color output to the Displacement -node Height input. Finally connect the Displacement -node Displacement output to the Material output -node Displacement input. Open the data/moon_textures/lroc_color_poles_8k.tif in the Image Texture -node that is connected to the Principled BSDF -node Base Color . Open the data/moon_textures/ldem_16.tif in the Image Texture -node that is connected to the Displacement -node Height input. Then finaly set the Image Texture -node Color Space -parameter of the node with the displacement texture to ** Non-Color . Initially the Displacement -node Scale parameter is set way too high making the moon look horrible. Set this parameter to 0.001 . As you can see it already looks quite like the moon but with some final tweaking you will get even more realism.","title":"Applying a material and texturing the moon - That's one small step..."},{"location":"advanced/advanced_materials/advanced_materials_assignment/#adaptive-displacement-revealing-the-craters-mooore-details","text":"Everything we have seen until now has been rendered in the default EEVEE rendering engine, which is for visualization purposes very powerful, but if you want to add that extra little realism with adaptive displacement you have to use the Cycles rendering engine. Active the Cycles rendering engine with the Render Engine setting in the Rendering properties -tab of the Properties -panel. While we are there, to be able to use adaptive displacement, we need to activate the Cycles experimental feature set. Set the Feature Set to Experimental . This Experimental feature set added an extra section in the current properties panel tab called Subdivision . In this section set Viewport to 2 . Now we need to add a Subdivision modifier that also got a new setting from the Experimental feature set that enables the adaptive displacement . Add a Subdivision modfier in the Modifier properties -tab of the Properties -panel. Enable the Adaptive Subdivision setting in this modifier. Until now you only saw some slight differences because there is only one setting that has to be changed to make all of this worth it. Change the Displacement setting to Displacement Only in the Properties -panel > Material properties -tab > Settings -section > Surface -subsection. Now zoom in and toggle to the Edit mode and back, which re-triggers the adaptive subdivision computations, and see the craters in their full glory. Bonus : For an artists rendition of the moon change the Displacement -node Scale parameter to a higher value and see how the craters get more noticeable (although less realistic).","title":"Adaptive displacement - Revealing the craters! Mooore details!"},{"location":"advanced/advanced_materials/introduction/","text":"Introduction \u00b6 This chapter will introduce the Shader Editor and UV Editor of Blender which lets you create advanced materials to improve the look of your visualizations. The Shader editor and UV editor go hand in hand, with the UV-editor (and 3D viewport) you'll learn how to UV-unwrap your meshes and manipulate the UV-coordinates and with the Shader editor you'll project procedural or image textures based on the created UV-coordinates. You'll learn how to apply PBR (Physically based rendering) style textures and where to find them, to make your objects look photo real. And lastly a commonly used experimental feature called Adaptive Subdivision will be combined with vertex displacement to create some great looking micro-displacement details on the surfaces of your objects. Before you start with the exercises the following video will give you the theoretical and practical background to make these exercises. In this video there are some Blender walk-throughs, if you want to follow along you can use the walk-through files in the ./walkthroughs/advanced_materials directory. After you watched the video about simple mesh-editing you are ready for the exercises!","title":"Introduction"},{"location":"advanced/advanced_materials/introduction/#introduction","text":"This chapter will introduce the Shader Editor and UV Editor of Blender which lets you create advanced materials to improve the look of your visualizations. The Shader editor and UV editor go hand in hand, with the UV-editor (and 3D viewport) you'll learn how to UV-unwrap your meshes and manipulate the UV-coordinates and with the Shader editor you'll project procedural or image textures based on the created UV-coordinates. You'll learn how to apply PBR (Physically based rendering) style textures and where to find them, to make your objects look photo real. And lastly a commonly used experimental feature called Adaptive Subdivision will be combined with vertex displacement to create some great looking micro-displacement details on the surfaces of your objects. Before you start with the exercises the following video will give you the theoretical and practical background to make these exercises. In this video there are some Blender walk-throughs, if you want to follow along you can use the walk-through files in the ./walkthroughs/advanced_materials directory. After you watched the video about simple mesh-editing you are ready for the exercises!","title":"Introduction"},{"location":"advanced/advanced_materials/node-wrangler/","text":"Node-wrangler reference \u00b6 The node-wrangler brings a wide variety of new features and hot-keys to automate steps within the Shader Editor to make life easier. In the walk-through only 2 features where shown, the 'Shader viewer' ( Ctrl+Shift+LMB ) and 'Add Texture Setup' ( Ctrl+T ), 2 very useful hot-keys but this is only the tip of the iceberg. To see the full set of features/hotkeys that node-wrangler provides you need to go to Menu bar 'Edit' > Preferences... > Tab 'Add-ons' > Search for 'Node wrangler' > Show Hotkey List (see image below). For additional information on what each individual feature does please refer to the official documentation . Warning The hotkeys in the official documentation are not updated yet to 2.8+ therefor please refer only for the information of each feature and use the \"Show Hotkey List\" for the current hotkeys.","title":"Node-wrangler reference"},{"location":"advanced/advanced_materials/node-wrangler/#node-wrangler-reference","text":"The node-wrangler brings a wide variety of new features and hot-keys to automate steps within the Shader Editor to make life easier. In the walk-through only 2 features where shown, the 'Shader viewer' ( Ctrl+Shift+LMB ) and 'Add Texture Setup' ( Ctrl+T ), 2 very useful hot-keys but this is only the tip of the iceberg. To see the full set of features/hotkeys that node-wrangler provides you need to go to Menu bar 'Edit' > Preferences... > Tab 'Add-ons' > Search for 'Node wrangler' > Show Hotkey List (see image below). For additional information on what each individual feature does please refer to the official documentation . Warning The hotkeys in the official documentation are not updated yet to 2.8+ therefor please refer only for the information of each feature and use the \"Show Hotkey List\" for the current hotkeys.","title":"Node-wrangler reference"},{"location":"advanced/advanced_materials/vertex_colors/","text":"Visualizing vertex colors with the Attribute node \u00b6 In the basics course we already introduced the use of vertex colors with the Material -tab in the Properties -panel. What happens under the hood is that you basically add an Attribute -node to the node-network and attached its Color -output to the Base Color -input of the Principled BSDF shader -node (see images below). Shader Editor node-network 3D viewport result The blend file for the image above, vertex-color.blend , can be found among the walk-through files in the ./walkthroughs/advanced_materials directory.","title":"Visualizing vertex colors with the Attribute node"},{"location":"advanced/advanced_materials/vertex_colors/#visualizing-vertex-colors-with-the-attribute-node","text":"In the basics course we already introduced the use of vertex colors with the Material -tab in the Properties -panel. What happens under the hood is that you basically add an Attribute -node to the node-network and attached its Color -output to the Base Color -input of the Principled BSDF shader -node (see images below). Shader Editor node-network 3D viewport result The blend file for the image above, vertex-color.blend , can be found among the walk-through files in the ./walkthroughs/advanced_materials directory.","title":"Visualizing vertex colors with the Attribute node"},{"location":"advanced/animation/2_assignment_cars/","text":"\ud83d\udcbb \"Cars\": the movie \u00b6 In this exercise you can do some more complex keyframe animation by having multiple objects move to create a city full of driving cars. You will need basic keyframing skills and use of the Graph Editor. Load cars.blend This scene has a very simple city with some building and some cars. An animation of 250 frames has been set up in the file, starting at frame 1, ending at frame 250. Tip All the geometry of the buildings is in the so-called collection \"Collection 2\". You can hide all these objects by clicking the eye icon right of \"Collection 2\" in the outliner. Change to the first frame in the animation with Shift-Left . Note that you can see the current frame you're working in by the blue vertical line in the Timeline at the bottom. Also, in the 3D view there's a piece of text in the upper-left that reads (1) Scene Collection | Plane : the current frame is listed between the parentheses. In the scene there's two cars behind each other. Select the front car of the two. Enter a keyframe for the car's location and rotation: press I followed by picking LocRot Change to the last frame in the animation with Shift-Right Move the car to the end of the road it's on, along the Y axis Enter another LocRot keyframe with I Check the car movement by playing back the animation with Space , or by changing the time in the Timeline editor with Shift-RMB The car's speed currently is not constant: it speeds up near the beginning of the animation and slows down starting somewhere halfway. We can edit the curve for the Y location channel in the Graph Editor to influence this behaviour. In the Graph Editor on the left of the screen show all the location and rotation values being animated for the selected car by using the little triangle left of the name Object Transforms . Below the Object Transforms you should now see the 6 channels for which you created keyframes in steps 4 and 7: X, Y and Z Location, and X, Y and Z Euler Rotation. Click the eye icon next to Object Transforms to hide all the channels. Then click the eye next to Y Location to only show the graph for the Y location. Note that you can use the Home key to zoom to the full extent of the graph. You should now see a curved line in green with two orange filled circles at the times of the beginning and end of the animation, i.e. frames 1 and 250. Attached to the squares are \"handles\" (the lines that end in open circles) that influence the shape of the curve. Select the open circular endpoints of the handles and move them around. See what this does for the shape of the curve and the subsequent behaviour of the car in the animation. The two curve points are selectable with Shift-LMB , but also, for example, border select ( B key). This works just like you normally select objects. Deleting keyframes can then be done with X . Select both curve points with A , Press V to bring up the Keyframe Handle type. This menu allows you to change how the curve is shaped based on the position of the handles. Select Vector . Notice how the curve's shape changes. See what happens when you move the handle endpoints. Press V again and choose Free . Again change the handle endpoints. Try out how the different curve shapes you can produce influence the car behaviour. Now let's animate another car: the one at the start of the road with the bend in it. Animate the second to move over the bended road all the way to the end. Bonus \u00b6 Make the cars drive over the road, choosing yourself which cars goes in what direction, how fast, which turns are made, etc. But don't make cars go through each other and have them wait if needed. Add a camera that shows the busy streets in action :)","title":"\ud83d\udcbb \"Cars\": the movie"},{"location":"advanced/animation/2_assignment_cars/#cars-the-movie","text":"In this exercise you can do some more complex keyframe animation by having multiple objects move to create a city full of driving cars. You will need basic keyframing skills and use of the Graph Editor. Load cars.blend This scene has a very simple city with some building and some cars. An animation of 250 frames has been set up in the file, starting at frame 1, ending at frame 250. Tip All the geometry of the buildings is in the so-called collection \"Collection 2\". You can hide all these objects by clicking the eye icon right of \"Collection 2\" in the outliner. Change to the first frame in the animation with Shift-Left . Note that you can see the current frame you're working in by the blue vertical line in the Timeline at the bottom. Also, in the 3D view there's a piece of text in the upper-left that reads (1) Scene Collection | Plane : the current frame is listed between the parentheses. In the scene there's two cars behind each other. Select the front car of the two. Enter a keyframe for the car's location and rotation: press I followed by picking LocRot Change to the last frame in the animation with Shift-Right Move the car to the end of the road it's on, along the Y axis Enter another LocRot keyframe with I Check the car movement by playing back the animation with Space , or by changing the time in the Timeline editor with Shift-RMB The car's speed currently is not constant: it speeds up near the beginning of the animation and slows down starting somewhere halfway. We can edit the curve for the Y location channel in the Graph Editor to influence this behaviour. In the Graph Editor on the left of the screen show all the location and rotation values being animated for the selected car by using the little triangle left of the name Object Transforms . Below the Object Transforms you should now see the 6 channels for which you created keyframes in steps 4 and 7: X, Y and Z Location, and X, Y and Z Euler Rotation. Click the eye icon next to Object Transforms to hide all the channels. Then click the eye next to Y Location to only show the graph for the Y location. Note that you can use the Home key to zoom to the full extent of the graph. You should now see a curved line in green with two orange filled circles at the times of the beginning and end of the animation, i.e. frames 1 and 250. Attached to the squares are \"handles\" (the lines that end in open circles) that influence the shape of the curve. Select the open circular endpoints of the handles and move them around. See what this does for the shape of the curve and the subsequent behaviour of the car in the animation. The two curve points are selectable with Shift-LMB , but also, for example, border select ( B key). This works just like you normally select objects. Deleting keyframes can then be done with X . Select both curve points with A , Press V to bring up the Keyframe Handle type. This menu allows you to change how the curve is shaped based on the position of the handles. Select Vector . Notice how the curve's shape changes. See what happens when you move the handle endpoints. Press V again and choose Free . Again change the handle endpoints. Try out how the different curve shapes you can produce influence the car behaviour. Now let's animate another car: the one at the start of the road with the bend in it. Animate the second to move over the bended road all the way to the end.","title":"\ud83d\udcbb \"Cars\": the movie"},{"location":"advanced/animation/2_assignment_cars/#bonus","text":"Make the cars drive over the road, choosing yourself which cars goes in what direction, how fast, which turns are made, etc. But don't make cars go through each other and have them wait if needed. Add a camera that shows the busy streets in action :)","title":"Bonus"},{"location":"advanced/animation/3_assignment_flipbook/","text":"\ud83d\udcbb Flipbook animation \u00b6 Get more familiar with the flipbook approach, in which a meshes is animated over time by switching a single object's mesh data each frame. Extract dambreak.tar.gz in the same directory as animated_ply_imports.blend Load animated_ply_imports.blend This blend file contains not only a 3D scene, but also some Python scripts we use to set up the flipbook animation. The first step is to load all the timesteps in the dataset using one of the scripts. This might take a bit of time, depending on the speed of your system. Tip By default, only the first 100 steps are loaded. You can increase the number of files to the full 300 if you like by updating the variable N in both the import script and the animation handler script. Execute the script that imports the PLY files for the time steps. To do this step make sure the script called \"1. import ply files\" is shown in the text editor panel. Then press the button in the top bar to run the script. The cursor changes to a numbered black square indicating the percentage of loading that has been completed. In case you get the idea something is wrong check the console output in the terminal where you started Blender. After all PLY files are loaded execute the script that installs the frame change handler. This script is called \"2. register anim handler\". Make sure the text editor is switched to this script and press the play button. Verify that the flipbook animation works with Space and/or moving the time slider in the Timeline with Shift-RMB . The playback speed will depend on your system's performance, but also on the framerate setting. Change the Frame Rate value (in the Output properties tab at the right side of the screen, icon ) to different values to see how your system handles it. Is 60 fps feasible? Use your skills with keyframe animation to do one of the following things (or both if you feel like it ;-)): 8a. Have a camera follow the moving water in some cool way 8b. Place a surfer on the moving wave of water. You can import the PLY model silver_surfer_by_melic.ply to use as 3D model. You can load it in Blender with File > Import > Stanford (.ply) .","title":"\ud83d\udcbb Flipbook animation"},{"location":"advanced/animation/3_assignment_flipbook/#flipbook-animation","text":"Get more familiar with the flipbook approach, in which a meshes is animated over time by switching a single object's mesh data each frame. Extract dambreak.tar.gz in the same directory as animated_ply_imports.blend Load animated_ply_imports.blend This blend file contains not only a 3D scene, but also some Python scripts we use to set up the flipbook animation. The first step is to load all the timesteps in the dataset using one of the scripts. This might take a bit of time, depending on the speed of your system. Tip By default, only the first 100 steps are loaded. You can increase the number of files to the full 300 if you like by updating the variable N in both the import script and the animation handler script. Execute the script that imports the PLY files for the time steps. To do this step make sure the script called \"1. import ply files\" is shown in the text editor panel. Then press the button in the top bar to run the script. The cursor changes to a numbered black square indicating the percentage of loading that has been completed. In case you get the idea something is wrong check the console output in the terminal where you started Blender. After all PLY files are loaded execute the script that installs the frame change handler. This script is called \"2. register anim handler\". Make sure the text editor is switched to this script and press the play button. Verify that the flipbook animation works with Space and/or moving the time slider in the Timeline with Shift-RMB . The playback speed will depend on your system's performance, but also on the framerate setting. Change the Frame Rate value (in the Output properties tab at the right side of the screen, icon ) to different values to see how your system handles it. Is 60 fps feasible? Use your skills with keyframe animation to do one of the following things (or both if you feel like it ;-)): 8a. Have a camera follow the moving water in some cool way 8b. Place a surfer on the moving wave of water. You can import the PLY model silver_surfer_by_melic.ply to use as 3D model. You can load it in Blender with File > Import > Stanford (.ply) .","title":"\ud83d\udcbb Flipbook animation"},{"location":"advanced/animation/introduction/","text":"Introduction \u00b6 The basic of (keyframe) animation in Blender were ready discussed in the Basics course, but if you need to refresh your memory then you can use this video:","title":"Introduction"},{"location":"advanced/animation/introduction/#introduction","text":"The basic of (keyframe) animation in Blender were ready discussed in the Basics course, but if you need to refresh your memory then you can use this video:","title":"Introduction"},{"location":"advanced/animation/shape_keys/","text":"Shape keys \u00b6 Overview \u00b6 Shape keys can be used for a very specific type of animation: to morph one mesh into another over time, or to blend multiple meshes together into one result. This can be used, for example, to show the time-evolution of some object or the highlight differences between two meshes. Although this is a fairly specific use case, shape keys aren't too difficult too understand and use, hence we include this section. There are some limitations to using shape keys: The two meshes must have the same number of vertices Preferably the two meshes should have the same topology (i.e. the way in which the vertices are connected to form polygons). If the topology doesn't match then strange results during morphing can occur. The above is a fairly annoying limitation, but there is no easy way around it in Blender, unfortunately. \ud83d\udcbb Poor Bunny \u00b6 Load bunny_shape_keys.blend This scene contains the Stanford Bunny and a completely flattened version of the Bunny Verify that these meshes have the same number of vertices. Do a visual comparison in wireframe mode ( Z > Wireframe ) We'll now add some shape keys: Select the regular Bunny. Add a shape key under Shape Keys in the Mesh properties using the + button. The new shape keys will be called Basis . Add a second shape key, it will be called Key 1 and have a default influence of 0. Select the Key 1 shape key and enter mesh edit mode in the 3D view with TAB and make sure you're in vertex mode by pressing 1 Select parts of the Bunny mesh and transform them as you like. The changes should be clearly visible. Exit mesh edit mode with TAB . You should notice that the mesh returns to its normal shape. Change the influence value of Key 1 to see what happens to the resulting mesh. You can either click on it and enter a number, of click and drag the value. Let's add another shape key: Add a third shape key, it will be called Key 2 . Select Key 2 and apply a second set of mesh changes in edit mode. Once again exit edit mode. Play around with the influence values of both shape keys, as well as the checkboxes next to the influence values. Checking the difference between relative and absolute shape keys: Uncheck the Relative checkbox to switch to absolute shape keys. Notice that the influence values have now disappeared. Change the Evolution Time value to understand how the morphing of the meshes is done now. Using another mesh to define a shape key: Delete shape keys Key 1 and Key 2 using the - button and change back to relative shape keys by checking the Relative checkbox. Select the flattened mesh and the Shift-click the Bunny mesh. Open the shape key menu using the downwards arrow below the + and - buttons. Select Join as Shapes . There should now be a new shape key called flattened mesh . Vary the influence of the flattened mesh shape key to see the Bunny melt. Delete the flattened mesh object in the Outliner. Does the shape key morphing to melt the Bunny still work? Looking closer at the behaviour of the mesh morphing: Try to reason why the head of the Bunny is the last part to melt. Zoom in a bit to see if you can spot the twisting motion that mesh makes as it melts. Try to transform the mesh in the melted shape key in such as way as to minimize the twist. Or toy around with other mesh transforms to see what morphs come out.","title":"Shape keys"},{"location":"advanced/animation/shape_keys/#shape-keys","text":"","title":"Shape keys"},{"location":"advanced/animation/shape_keys/#overview","text":"Shape keys can be used for a very specific type of animation: to morph one mesh into another over time, or to blend multiple meshes together into one result. This can be used, for example, to show the time-evolution of some object or the highlight differences between two meshes. Although this is a fairly specific use case, shape keys aren't too difficult too understand and use, hence we include this section. There are some limitations to using shape keys: The two meshes must have the same number of vertices Preferably the two meshes should have the same topology (i.e. the way in which the vertices are connected to form polygons). If the topology doesn't match then strange results during morphing can occur. The above is a fairly annoying limitation, but there is no easy way around it in Blender, unfortunately.","title":"Overview"},{"location":"advanced/animation/shape_keys/#poor-bunny","text":"Load bunny_shape_keys.blend This scene contains the Stanford Bunny and a completely flattened version of the Bunny Verify that these meshes have the same number of vertices. Do a visual comparison in wireframe mode ( Z > Wireframe ) We'll now add some shape keys: Select the regular Bunny. Add a shape key under Shape Keys in the Mesh properties using the + button. The new shape keys will be called Basis . Add a second shape key, it will be called Key 1 and have a default influence of 0. Select the Key 1 shape key and enter mesh edit mode in the 3D view with TAB and make sure you're in vertex mode by pressing 1 Select parts of the Bunny mesh and transform them as you like. The changes should be clearly visible. Exit mesh edit mode with TAB . You should notice that the mesh returns to its normal shape. Change the influence value of Key 1 to see what happens to the resulting mesh. You can either click on it and enter a number, of click and drag the value. Let's add another shape key: Add a third shape key, it will be called Key 2 . Select Key 2 and apply a second set of mesh changes in edit mode. Once again exit edit mode. Play around with the influence values of both shape keys, as well as the checkboxes next to the influence values. Checking the difference between relative and absolute shape keys: Uncheck the Relative checkbox to switch to absolute shape keys. Notice that the influence values have now disappeared. Change the Evolution Time value to understand how the morphing of the meshes is done now. Using another mesh to define a shape key: Delete shape keys Key 1 and Key 2 using the - button and change back to relative shape keys by checking the Relative checkbox. Select the flattened mesh and the Shift-click the Bunny mesh. Open the shape key menu using the downwards arrow below the + and - buttons. Select Join as Shapes . There should now be a new shape key called flattened mesh . Vary the influence of the flattened mesh shape key to see the Bunny melt. Delete the flattened mesh object in the Outliner. Does the shape key morphing to melt the Bunny still work? Looking closer at the behaviour of the mesh morphing: Try to reason why the head of the Bunny is the last part to melt. Zoom in a bit to see if you can spot the twisting motion that mesh makes as it melts. Try to transform the mesh in the melted shape key in such as way as to minimize the twist. Or toy around with other mesh transforms to see what morphs come out.","title":"\ud83d\udcbb Poor Bunny"},{"location":"advanced/final_project/final_project/","text":"\ud83d\udcbb Final project: making a visualization of your own data \u00b6 We would like you to spend the remainder of your time in this course on doing this little project. We have two options for you to choose from. The first and recommended one is making a visualization of your own (research) data. The second option is that you work on a visualization of data we have prepared. Do not forget that if you are stuck to join us on Discord or in a feedback webinar so we can help. See the Course overview for more information. If you made a nice visualization and still have time left in the course, why not make an animation? Option 1: your own data \u00b6 So far you have learned how to make meshes and vertex colors in Blender using Python. So, think about if you can visualize your data using these techniques. You need to think about what you need to do to transform your data into a form that can be used to generate vertices, faces and vertex colors. And how do you want to visualize your data values? Can you visualize them through the Cartesian coordinates of the vertices and faces and maybe some colors? Do you need to use vertex coloring? Or do you need something else? Note that volumetric data will be difficult in Blender and you may need to think of some tricks. Option 2: visualize a computer model of a proto-planetary disk \u00b6 Although we highly recommend you to work on your own data, if you have none to use, you can use the following data to work on. Here we give a brief introduction to the data. What is a proto-planetary disk \u00b6 A proto-planetary disk is a disk-like structure around a newly born star. This disk is filled with dust (solid-state particles with a diameter in the order of 1 micrometer) and gas. In the course of time this dust and gas can coalesce into planets. In this option we will look at a computer model of the dust in such a disk. The model calculates the temperature and density of the dust in the disk, taking the radiation and gravity of the star into account. The calculations of the software (called MCMax ) are done iteratively using Monte Carlo techniques. Packages of photons are emitted by the star in random directions and their wavelength sampled from the radiation distribution of the star (by default a blackbody). Using the absorption, scattering and emission properties of the dust grains in the disk, the scattering, absorption and re-emission of the photons are calculated throughout the disk. This is used to calculate a temperature structure in the disk. This temperature is then used to adapt the starting density structure of the disk after which a new pass is done by tracking a next set of photons and adapting the density subsequently. This is repeated until convergence is reached. The code uses a two dimensional (adaptable) grid in the radial and theta direction. The disk is assumed to be cylindrically symmetric around the polar axis (z-axis, see Fig. 1). The grid cell size is lowered in regions where the density becomes high. Figure 1: definition of coordinates How to start visualizing such a proto-planetary disk \u00b6 You could create a 3D model of the disk at constant density and display the temperature as colors on the surface of the model. You could use this to make nice renders and animations to show the temperature structure of the disk. For this we need to pre-process the data from the model to get the spatial coordinates of the disk at a constant density. These coordinates then need to be converted into Cartesian coordinates of vertices and faces before creating the geometry in Blender. You can then add the temperatures to the faces using vertex coloring and by adding the needed shaders to the model. How the model data is structured \u00b6 You can download the data here . An example output file of modeling code MCMax is shown below. # Format number 5 # NR, NT, NGRAINS, NGRAINS2 100 100 1 1 # Spherical radius grid [cm] (middle of cell) 7479900216981.22 7479900572789.07 [...] # Theta grid [rad, from pole] (middle of cell) 9.233559849414326E-003 2.365344804038962E-002 [...] # Density array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-050 1.001753516582521E-050 [...] # Temperature array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1933.54960366819 1917.22966277529 [...] # Composition array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.00000000000000 1.00000000000000 [...] # Gas density array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-048 1.001753516582521E-048 [...] # Density0 array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-050 1.001753516582521E-050 [...] The file is structured in a way the scientist thought best at the time using the tools at hand. For us it is important to notice the NR and NT , which stands for number of radial and theta points respectively (NGRAINS is related to the number of different types of dust grains in the disk and you can ignore this). Further, the output file then lists the radius points and after that the theta points. Subsequently temperature and density values are listed by iterating over the radius and then the theta indices. The units of all the values in the MCMax output are: R[cm], Theta[radians], Density[gr/cm^3], Temperature[K]. The data from the MCMax code is in spherical coordinates, while the system in Blender works with Cartesian coordinates. The theta in the output is defined as the angle with the z-axis (See Fig. 1). How it could look \u00b6 To help you get an idea of what the data of the proto-planetary disk might look like, check this video we made:","title":"\ud83d\udcbb Final project: making a visualization of your own data"},{"location":"advanced/final_project/final_project/#final-project-making-a-visualization-of-your-own-data","text":"We would like you to spend the remainder of your time in this course on doing this little project. We have two options for you to choose from. The first and recommended one is making a visualization of your own (research) data. The second option is that you work on a visualization of data we have prepared. Do not forget that if you are stuck to join us on Discord or in a feedback webinar so we can help. See the Course overview for more information. If you made a nice visualization and still have time left in the course, why not make an animation?","title":"\ud83d\udcbb Final project: making a visualization of your own data"},{"location":"advanced/final_project/final_project/#option-1-your-own-data","text":"So far you have learned how to make meshes and vertex colors in Blender using Python. So, think about if you can visualize your data using these techniques. You need to think about what you need to do to transform your data into a form that can be used to generate vertices, faces and vertex colors. And how do you want to visualize your data values? Can you visualize them through the Cartesian coordinates of the vertices and faces and maybe some colors? Do you need to use vertex coloring? Or do you need something else? Note that volumetric data will be difficult in Blender and you may need to think of some tricks.","title":"Option 1: your own data"},{"location":"advanced/final_project/final_project/#option-2-visualize-a-computer-model-of-a-proto-planetary-disk","text":"Although we highly recommend you to work on your own data, if you have none to use, you can use the following data to work on. Here we give a brief introduction to the data.","title":"Option 2: visualize a computer model of a proto-planetary disk"},{"location":"advanced/final_project/final_project/#what-is-a-proto-planetary-disk","text":"A proto-planetary disk is a disk-like structure around a newly born star. This disk is filled with dust (solid-state particles with a diameter in the order of 1 micrometer) and gas. In the course of time this dust and gas can coalesce into planets. In this option we will look at a computer model of the dust in such a disk. The model calculates the temperature and density of the dust in the disk, taking the radiation and gravity of the star into account. The calculations of the software (called MCMax ) are done iteratively using Monte Carlo techniques. Packages of photons are emitted by the star in random directions and their wavelength sampled from the radiation distribution of the star (by default a blackbody). Using the absorption, scattering and emission properties of the dust grains in the disk, the scattering, absorption and re-emission of the photons are calculated throughout the disk. This is used to calculate a temperature structure in the disk. This temperature is then used to adapt the starting density structure of the disk after which a new pass is done by tracking a next set of photons and adapting the density subsequently. This is repeated until convergence is reached. The code uses a two dimensional (adaptable) grid in the radial and theta direction. The disk is assumed to be cylindrically symmetric around the polar axis (z-axis, see Fig. 1). The grid cell size is lowered in regions where the density becomes high. Figure 1: definition of coordinates","title":"What is a proto-planetary disk"},{"location":"advanced/final_project/final_project/#how-to-start-visualizing-such-a-proto-planetary-disk","text":"You could create a 3D model of the disk at constant density and display the temperature as colors on the surface of the model. You could use this to make nice renders and animations to show the temperature structure of the disk. For this we need to pre-process the data from the model to get the spatial coordinates of the disk at a constant density. These coordinates then need to be converted into Cartesian coordinates of vertices and faces before creating the geometry in Blender. You can then add the temperatures to the faces using vertex coloring and by adding the needed shaders to the model.","title":"How to start visualizing such a proto-planetary disk"},{"location":"advanced/final_project/final_project/#how-the-model-data-is-structured","text":"You can download the data here . An example output file of modeling code MCMax is shown below. # Format number 5 # NR, NT, NGRAINS, NGRAINS2 100 100 1 1 # Spherical radius grid [cm] (middle of cell) 7479900216981.22 7479900572789.07 [...] # Theta grid [rad, from pole] (middle of cell) 9.233559849414326E-003 2.365344804038962E-002 [...] # Density array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-050 1.001753516582521E-050 [...] # Temperature array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1933.54960366819 1917.22966277529 [...] # Composition array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.00000000000000 1.00000000000000 [...] # Gas density array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-048 1.001753516582521E-048 [...] # Density0 array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-050 1.001753516582521E-050 [...] The file is structured in a way the scientist thought best at the time using the tools at hand. For us it is important to notice the NR and NT , which stands for number of radial and theta points respectively (NGRAINS is related to the number of different types of dust grains in the disk and you can ignore this). Further, the output file then lists the radius points and after that the theta points. Subsequently temperature and density values are listed by iterating over the radius and then the theta indices. The units of all the values in the MCMax output are: R[cm], Theta[radians], Density[gr/cm^3], Temperature[K]. The data from the MCMax code is in spherical coordinates, while the system in Blender works with Cartesian coordinates. The theta in the output is defined as the angle with the z-axis (See Fig. 1).","title":"How the model data is structured"},{"location":"advanced/final_project/final_project/#how-it-could-look","text":"To help you get an idea of what the data of the proto-planetary disk might look like, check this video we made:","title":"How it could look"},{"location":"advanced/mesh_editing/introduction/","text":"Introduction \u00b6 Warning This chapter is fresh of the mill and does not have any slides or a walkthrough yet, but with the knowledge of the basics course (see video below) you might be able to follow the assignments. This chapter will give you an introduction on the Edit mode of the 3D viewport where you will learn how to patch up your imported meshes/visualizations and even learn how to generate your own 3D shapes. To refresh your memory on basic mesh editing you can watch the Simple mesh editing video of the \"Introduction to Scientific Visualization with Blender\" course below:","title":"Introduction"},{"location":"advanced/mesh_editing/introduction/#introduction","text":"Warning This chapter is fresh of the mill and does not have any slides or a walkthrough yet, but with the knowledge of the basics course (see video below) you might be able to follow the assignments. This chapter will give you an introduction on the Edit mode of the 3D viewport where you will learn how to patch up your imported meshes/visualizations and even learn how to generate your own 3D shapes. To refresh your memory on basic mesh editing you can watch the Simple mesh editing video of the \"Introduction to Scientific Visualization with Blender\" course below:","title":"Introduction"},{"location":"advanced/mesh_editing/mesh_editing_assignment/","text":"\ud83d\udcbb Mesh Editing with the Edit mode \u00b6 This assignment will be a brief introduction on the Edit mode in the 3D viewport . Once you opened the exercise blend file sme_assignment.blend you'll see the familiar fish iso-surface above a plane. Getting familiar with the Edit mode \u00b6 To edit the mesh we first need to go the Edit mode with the fish. Select the fish and enter the Edit mode by pressing Tab . Depending on the speed of the system you're working on edit mode might be entered instantly or might take 1/2 second. In general, for larger meshes switching to edit may take longer. Now you will be able to see all the vertices, edges and faces that make up the 3D model. You will now try to select and move around some vertices, edges and/or faces. Change the Mesh Select Mode to Vertex by pressing 1 (or click the left icon in at the 3D view header). Before you start selecting, de-select all all current selected vertices by pressing Alt-A or double 'A' rapidly. Now try to select a single vertex by clicking on it with the LMB , or multiple with Shift-LMB . You might have to zoom in a bit to separate the vertices enough. Another method is to use the selection tools: Box selection by pressing B and dragging a box around the vertices you want to select. Hold Shift to de-select. Circle selection by pressing C and left-clicking and dragging with the mouse over the vertices you want to select. To increase the size of the Circle selection tool simply scroll with your mouse Wheel . With MMB and dragging you can de-select vertices. Press Enter to exit circle select mode (or with RMB ). Once you selected your vertices you can transform them the same way you can do with objects by pressing the hotkeys G for translation, R for rotation, and S for scaling, etc. Probably now you did the vertex editing the fish looks a bit scrabbled. One way to clean it up is, of course, using Ctrl-Z to undo it. Another way is simply deleting the vertices by using the Delete popup menu X > Vertices . Try to remove part of the fish skin to it leaves a hole in the mesh which will reveal a part of the inside of the fish. Tip! : If your fish has been \"meshed-up\" beyond repair you can always revert it to the last saved state with: File > Revert > Confirm . Filling the holes \u00b6 An imported mesh from a 3D visualization program can sometimes contain unwanted holes or separations in parts of the mesh, these can also be fixed in the edit mode. Conveniently the fish in the exercise file was already poked full of holes so you can fix these. In between : To better inspect if there are any holes left you can switch back and forth between the Object mode and Edit mode because in the Object mode they are easier to see. First, make sure the whole mesh is selected by pressing a and then remove the small holes (the size of one triangle/quad) by pressing F3 in the 3D viewport in Edit mode and type in fill holes and press enter or click on it with LMB (this might take some time). Now this already cleaned up a lot of the holes in the geometry! Through inspection you might notices there are some bigger wholes that were not filled yet because they were skipped by the previous step since they were to large. To fill these they first need to be selected by first de-selecting everything with alt-a and then press F3 and type in non manifold and press enter or click on it with LMB . This selected the big holes but also other non- manifold geometry. To select only one of the holes hold CTRL+SHIFT and drag with LMB over one of the holes. This de-selects everything excepts what was in the drag-box. Now this selected hole can easily be fixed by pressing f . Repeat step 2 to 4 for the other 2 holes. Tip! : The fill with f fills the hole with an n-gon, a face with more then 4 vertices. These can sometimes create shading artifacts in your final render. Another way to fill these holes is to use grid-fill ( ctrl+f ), this tries to fill the whole with a grid of quad shaped faces. This however might not always work for numerous reasons (uneven amount of vertices, closed loops etc) which can be fixed with additional mesh editing but the easy route would be to fill it with an n-gon face. Separating skin from bones \u00b6 Now that you got a little familiar with mesh editing you can try to separate the skin from the bones by using mesh separation. While still in edit mode (press Tab if not), try to select all the outside skin with the select linked selection by hovering the mouse cursor over the geometry and pressing L . This will only select a connected part of the skin so continue this step until you think you selected all the outside skin. Note that it is difficult to do this perfectly, as some of the insides of the fish are sometimes also selectable. Unfortunately, this occurs frequently with this type of sensor-based 3D data. Once you think all the skin is selected you can press P and select Selection to separate the selected surfaces from the main mesh into another mesh object. This new mesh will be added to the Outliner with the name fish.001 . In the Outliner double-click LMB on the mesh object fish.001 to rename it to fishskin . Do the same for the fish mesh object and rename it to fishbones . If you now select the fishskin mesh object and hide it by clicking the little icon in the Outliner will reveal the insides of the fish. Tips! : - To reverse the separation of the mesh into bone and skin you can select both the mesh objects in Object mode and press Ctrl-J to join them back together into a single mesh. - Sometimes X-ray mode, toggled with Alt-Z can be useful when editing a complex mesh, as it makes all geometry in a mesh partly transparent (BONUS) Make your own annotation arrow \u00b6 Since the content of this course is mostly geared towards imported geometry or scripted geometry, you might not directly think about manually created geometry. This bonus exercise however will show you that it is relatively easy to create your own geometry in Blender. Lets start your manual mesh creation with an annotation arrow! In the 3D viewport make sure you are in Object mode and add a new cylinder with Shift-A > Mesh > Cylinder . Press / to isolate the mesh so that there are no distractions. This can be reversed again by pressing / . Press Tab to go into Edit mode . Grab the selected geometry by pressing g and press z to move it along the z-axis only and press 1 to move it 1 unit up so that the origin is at the bottom. De-select all the geometry with Alt-A and press 1 to set the select mode to Vertex and select all the bottom vertices (with LMB-drag over the vertices or with the b Box-select). Press s to scale them to a tiny point and press LMB to confirm. Now select the top vertices the same way you did with the bottom vertices, make sure that none of the bottom vertices are selected. Press i to inset the faces and move your mouse until you are satisfied with the width of the arrow shaft. Press e to extrude the selection and move the mouse up until you are satisfied with the length of the arrow shaft. Now press Tab and admire your newly created arrow! The arrow might now be a bit too big compared to the fish so scale the arrow down with s , move it to a point of interest with g and rotate the arrow to your liking with r (which is made relatively easy because we made it so that the origin is at the point) Since the introduction of the Edit mode and switching back and forth between it and the Object mode you do need to make sure in which mode you are before adding new geometry or before using one of the transform operations (grab, scale and rotate). Otherwise you might add geometry to an already existing object instead of adding a new 3D object or you might move, scale or rotate 3D object geometry in the Edit mode and inadvertently change the origin of the object. This can be confusing sometimes but you'll get used to it!","title":"\ud83d\udcbb Mesh Editing with the Edit mode"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#mesh-editing-with-the-edit-mode","text":"This assignment will be a brief introduction on the Edit mode in the 3D viewport . Once you opened the exercise blend file sme_assignment.blend you'll see the familiar fish iso-surface above a plane.","title":"\ud83d\udcbb Mesh Editing with the Edit mode"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#getting-familiar-with-the-edit-mode","text":"To edit the mesh we first need to go the Edit mode with the fish. Select the fish and enter the Edit mode by pressing Tab . Depending on the speed of the system you're working on edit mode might be entered instantly or might take 1/2 second. In general, for larger meshes switching to edit may take longer. Now you will be able to see all the vertices, edges and faces that make up the 3D model. You will now try to select and move around some vertices, edges and/or faces. Change the Mesh Select Mode to Vertex by pressing 1 (or click the left icon in at the 3D view header). Before you start selecting, de-select all all current selected vertices by pressing Alt-A or double 'A' rapidly. Now try to select a single vertex by clicking on it with the LMB , or multiple with Shift-LMB . You might have to zoom in a bit to separate the vertices enough. Another method is to use the selection tools: Box selection by pressing B and dragging a box around the vertices you want to select. Hold Shift to de-select. Circle selection by pressing C and left-clicking and dragging with the mouse over the vertices you want to select. To increase the size of the Circle selection tool simply scroll with your mouse Wheel . With MMB and dragging you can de-select vertices. Press Enter to exit circle select mode (or with RMB ). Once you selected your vertices you can transform them the same way you can do with objects by pressing the hotkeys G for translation, R for rotation, and S for scaling, etc. Probably now you did the vertex editing the fish looks a bit scrabbled. One way to clean it up is, of course, using Ctrl-Z to undo it. Another way is simply deleting the vertices by using the Delete popup menu X > Vertices . Try to remove part of the fish skin to it leaves a hole in the mesh which will reveal a part of the inside of the fish. Tip! : If your fish has been \"meshed-up\" beyond repair you can always revert it to the last saved state with: File > Revert > Confirm .","title":"Getting familiar with the Edit mode"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#filling-the-holes","text":"An imported mesh from a 3D visualization program can sometimes contain unwanted holes or separations in parts of the mesh, these can also be fixed in the edit mode. Conveniently the fish in the exercise file was already poked full of holes so you can fix these. In between : To better inspect if there are any holes left you can switch back and forth between the Object mode and Edit mode because in the Object mode they are easier to see. First, make sure the whole mesh is selected by pressing a and then remove the small holes (the size of one triangle/quad) by pressing F3 in the 3D viewport in Edit mode and type in fill holes and press enter or click on it with LMB (this might take some time). Now this already cleaned up a lot of the holes in the geometry! Through inspection you might notices there are some bigger wholes that were not filled yet because they were skipped by the previous step since they were to large. To fill these they first need to be selected by first de-selecting everything with alt-a and then press F3 and type in non manifold and press enter or click on it with LMB . This selected the big holes but also other non- manifold geometry. To select only one of the holes hold CTRL+SHIFT and drag with LMB over one of the holes. This de-selects everything excepts what was in the drag-box. Now this selected hole can easily be fixed by pressing f . Repeat step 2 to 4 for the other 2 holes. Tip! : The fill with f fills the hole with an n-gon, a face with more then 4 vertices. These can sometimes create shading artifacts in your final render. Another way to fill these holes is to use grid-fill ( ctrl+f ), this tries to fill the whole with a grid of quad shaped faces. This however might not always work for numerous reasons (uneven amount of vertices, closed loops etc) which can be fixed with additional mesh editing but the easy route would be to fill it with an n-gon face.","title":"Filling the holes"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#separating-skin-from-bones","text":"Now that you got a little familiar with mesh editing you can try to separate the skin from the bones by using mesh separation. While still in edit mode (press Tab if not), try to select all the outside skin with the select linked selection by hovering the mouse cursor over the geometry and pressing L . This will only select a connected part of the skin so continue this step until you think you selected all the outside skin. Note that it is difficult to do this perfectly, as some of the insides of the fish are sometimes also selectable. Unfortunately, this occurs frequently with this type of sensor-based 3D data. Once you think all the skin is selected you can press P and select Selection to separate the selected surfaces from the main mesh into another mesh object. This new mesh will be added to the Outliner with the name fish.001 . In the Outliner double-click LMB on the mesh object fish.001 to rename it to fishskin . Do the same for the fish mesh object and rename it to fishbones . If you now select the fishskin mesh object and hide it by clicking the little icon in the Outliner will reveal the insides of the fish. Tips! : - To reverse the separation of the mesh into bone and skin you can select both the mesh objects in Object mode and press Ctrl-J to join them back together into a single mesh. - Sometimes X-ray mode, toggled with Alt-Z can be useful when editing a complex mesh, as it makes all geometry in a mesh partly transparent","title":"Separating skin from bones"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#bonus-make-your-own-annotation-arrow","text":"Since the content of this course is mostly geared towards imported geometry or scripted geometry, you might not directly think about manually created geometry. This bonus exercise however will show you that it is relatively easy to create your own geometry in Blender. Lets start your manual mesh creation with an annotation arrow! In the 3D viewport make sure you are in Object mode and add a new cylinder with Shift-A > Mesh > Cylinder . Press / to isolate the mesh so that there are no distractions. This can be reversed again by pressing / . Press Tab to go into Edit mode . Grab the selected geometry by pressing g and press z to move it along the z-axis only and press 1 to move it 1 unit up so that the origin is at the bottom. De-select all the geometry with Alt-A and press 1 to set the select mode to Vertex and select all the bottom vertices (with LMB-drag over the vertices or with the b Box-select). Press s to scale them to a tiny point and press LMB to confirm. Now select the top vertices the same way you did with the bottom vertices, make sure that none of the bottom vertices are selected. Press i to inset the faces and move your mouse until you are satisfied with the width of the arrow shaft. Press e to extrude the selection and move the mouse up until you are satisfied with the length of the arrow shaft. Now press Tab and admire your newly created arrow! The arrow might now be a bit too big compared to the fish so scale the arrow down with s , move it to a point of interest with g and rotate the arrow to your liking with r (which is made relatively easy because we made it so that the origin is at the point) Since the introduction of the Edit mode and switching back and forth between it and the Object mode you do need to make sure in which mode you are before adding new geometry or before using one of the transform operations (grab, scale and rotate). Otherwise you might add geometry to an already existing object instead of adding a new 3D object or you might move, scale or rotate 3D object geometry in the Edit mode and inadvertently change the origin of the object. This can be confusing sometimes but you'll get used to it!","title":"(BONUS) Make your own annotation arrow"},{"location":"advanced/python_scripting/1_api_basics/","text":"Blender API basics \u00b6 Introduction \u00b6 Blender contains a Python interpreter that, at startup, runs scripts to construct the interface and run internal tools. The user can run scripts directly on this interpreter and also access modules provided by Blender like bpy and mathutils . The bpy module gives access to Blender's data, functions and classes. In this section we will focus on using the Python API for automation, custom data import and manipulating geometry, but this is not all that is possible with the API, of course. The official API manual states the following things are possible using the Python API: Edit any data the user interface can (Scenes, Meshes, Particles etc.). Modify user preferences, key-maps and themes. Run tools with own settings. Create user interface elements such as menus, headers and panels. Create new tools. Create interactive tools. Create new rendering engines that integrate with Blender. Subscribe to changes to data and it's properties. Define new settings in existing Blender data. Draw in the 3D view using Python. All in all, the Python API is very powerful. Good to know \u00b6 Before we continue, we list some bits of information and some tricks that are good to know. Blender uses Python 3.x since Blender 2.5 You can access the online API documentation from within Blender with Help > Python API Reference Starting Blender from the console will allow you to see important outputs channels (warnings, exceptions, print() statements, etc). See the next section how to do this. By enabling the Python Tooltips option in the Preferences under Interface > Display you can hover over almost any button, option, menu, etc and after a second a tool-tip is shown. This tool-tip shows information on how to access this element from the Python API. Right clicking on almost any button, option, menu, etc in Blender gives you the option to 1) directly go to the API documentation with Online Manual or 2) Copy Data Path . Option 2 copies Python API properties related to that element to your clipboard to paste into your script. Note however, that not always the full path is copied, but only the last part. The Python Console area in Blender is great for testing Python one-liners. It also has auto-completion so you can inspect the API quickly. Example code shown with >>> lines in our course notes is assumed to be running in the Python Console. Python Console versus terminal console The Python Console is something different than the console we refer to below. The Python Console is an area within the Blender user interface in which you can enter and execute Python commands: While the other type of \"console\" is a terminal window or DOS box from which you start Blender. This console will then contain any output and exceptions from Python scripts that you run: In the upcoming sections we will first look at how to run Python scripts in Blender. Then we look at how to access Blenders data through scripts and we follow this up with creating geometry, vertex colors and materials in the last section. Starting Blender from the command line \u00b6 It is important, when scripting, to start Blender from a command line interface (macOS and Linux). Warnings, messages and print() statements will output into the console. How to start Blender from the command line depends on your operating system. For macOS it would be like this: /Applications/Blender.app/Contents/MacOS/Blender For Linux it would be something like: $ <blender installation directory>/blender On Windows you can start Blender normally (i.e. from the Start menu) and then use Window > Toggle System Console to open the console window from within Blender. More information on where the Blender executable is located on your system and where Blender directories of interest are located see this manual page . \ud83d\udcbb Starting Blender from the console \u00b6 Find the Blender executable on your machine. Open Blender through the console. Delete the cube in the default project of Blender, what output is shown in the console? Run scripts within the Blender interface \u00b6 When scripting inside Blender it is convenient to use the Scripting workspace (Fig. 1: see the arrow). It gives you a Python Console (Fig 1: A) and a Text Editor (Fig 1: B). Running scripts within Blender you have a two main options: Interactive Python Console in the Blender interface (Fig. 1: A) Use the built-in Text Editor (Fig. 1: B) The Python Console is nice for exploring the API using auto-complete ( TAB in 2.82+, Ctrl-Space in older versions) to see what is available. The keyboard shortcuts are a bit different than you might be used to in other text editors. See this section in the Blender manual for an overview of menu options and shortcut keys. Blender also has its own built-in editor which you can use (Fig. 1: B) and directly execute the script by pressing the button in the top bar. If you want to use your own editor to edit your scripts you can do this by opening the script in both the Blender Text Editor and your own editor. To refresh the Blender Text Editor use Text > Reload or Alt R (or Option R on the Mac). You can also make a script that you open in the Blender Text Editor that executes an external script you edit in your own editor. See for example the script in Fig. 1: B. Figure 1: The Scripting workspace in Blender Run scripts from the command-line \u00b6 You can also run Python scripts in Blender directly from the command-line interface. An example of executing a script ( -P ) without opening the Blender GUI ( -b , for background) on a Mac would be: blender -b -P script.py Or if you want to render the first frame ( -f 1 ) from an example test.blend file. The output will go to the directory of the blender file ( -o //... ) and it will generate a PNG image file ( -F PNG ). blender -b test.blend -o //render_ -F PNG -f 1 More information on command line arguments is here . Load modules in Blender \u00b6 You can add startup scripts to your Blender installation. These scripts can then be easily imported into your script. These scripts can be placd here: scripts\u2028/startup/ in the Blender directory (for example on a Mac: /Applications/Blender.app/Contents/Resources/2.81/scripts\u2028/startup/ ). For scripts you want to import for just one project or Blender file you can use the normal Python method of editing sys.path : import sys sys . path . append ( \"/some_directory/\" ) import python_module Tip The official binaries of Blender from blender.org include the numpy Python module, so import numpy should work out of the box","title":"Blender API basics"},{"location":"advanced/python_scripting/1_api_basics/#blender-api-basics","text":"","title":"Blender API basics"},{"location":"advanced/python_scripting/1_api_basics/#introduction","text":"Blender contains a Python interpreter that, at startup, runs scripts to construct the interface and run internal tools. The user can run scripts directly on this interpreter and also access modules provided by Blender like bpy and mathutils . The bpy module gives access to Blender's data, functions and classes. In this section we will focus on using the Python API for automation, custom data import and manipulating geometry, but this is not all that is possible with the API, of course. The official API manual states the following things are possible using the Python API: Edit any data the user interface can (Scenes, Meshes, Particles etc.). Modify user preferences, key-maps and themes. Run tools with own settings. Create user interface elements such as menus, headers and panels. Create new tools. Create interactive tools. Create new rendering engines that integrate with Blender. Subscribe to changes to data and it's properties. Define new settings in existing Blender data. Draw in the 3D view using Python. All in all, the Python API is very powerful.","title":"Introduction"},{"location":"advanced/python_scripting/1_api_basics/#good-to-know","text":"Before we continue, we list some bits of information and some tricks that are good to know. Blender uses Python 3.x since Blender 2.5 You can access the online API documentation from within Blender with Help > Python API Reference Starting Blender from the console will allow you to see important outputs channels (warnings, exceptions, print() statements, etc). See the next section how to do this. By enabling the Python Tooltips option in the Preferences under Interface > Display you can hover over almost any button, option, menu, etc and after a second a tool-tip is shown. This tool-tip shows information on how to access this element from the Python API. Right clicking on almost any button, option, menu, etc in Blender gives you the option to 1) directly go to the API documentation with Online Manual or 2) Copy Data Path . Option 2 copies Python API properties related to that element to your clipboard to paste into your script. Note however, that not always the full path is copied, but only the last part. The Python Console area in Blender is great for testing Python one-liners. It also has auto-completion so you can inspect the API quickly. Example code shown with >>> lines in our course notes is assumed to be running in the Python Console. Python Console versus terminal console The Python Console is something different than the console we refer to below. The Python Console is an area within the Blender user interface in which you can enter and execute Python commands: While the other type of \"console\" is a terminal window or DOS box from which you start Blender. This console will then contain any output and exceptions from Python scripts that you run: In the upcoming sections we will first look at how to run Python scripts in Blender. Then we look at how to access Blenders data through scripts and we follow this up with creating geometry, vertex colors and materials in the last section.","title":"Good to know"},{"location":"advanced/python_scripting/1_api_basics/#starting-blender-from-the-command-line","text":"It is important, when scripting, to start Blender from a command line interface (macOS and Linux). Warnings, messages and print() statements will output into the console. How to start Blender from the command line depends on your operating system. For macOS it would be like this: /Applications/Blender.app/Contents/MacOS/Blender For Linux it would be something like: $ <blender installation directory>/blender On Windows you can start Blender normally (i.e. from the Start menu) and then use Window > Toggle System Console to open the console window from within Blender. More information on where the Blender executable is located on your system and where Blender directories of interest are located see this manual page .","title":"Starting Blender from the command line"},{"location":"advanced/python_scripting/1_api_basics/#starting-blender-from-the-console","text":"Find the Blender executable on your machine. Open Blender through the console. Delete the cube in the default project of Blender, what output is shown in the console?","title":"\ud83d\udcbb Starting Blender from the console"},{"location":"advanced/python_scripting/1_api_basics/#run-scripts-within-the-blender-interface","text":"When scripting inside Blender it is convenient to use the Scripting workspace (Fig. 1: see the arrow). It gives you a Python Console (Fig 1: A) and a Text Editor (Fig 1: B). Running scripts within Blender you have a two main options: Interactive Python Console in the Blender interface (Fig. 1: A) Use the built-in Text Editor (Fig. 1: B) The Python Console is nice for exploring the API using auto-complete ( TAB in 2.82+, Ctrl-Space in older versions) to see what is available. The keyboard shortcuts are a bit different than you might be used to in other text editors. See this section in the Blender manual for an overview of menu options and shortcut keys. Blender also has its own built-in editor which you can use (Fig. 1: B) and directly execute the script by pressing the button in the top bar. If you want to use your own editor to edit your scripts you can do this by opening the script in both the Blender Text Editor and your own editor. To refresh the Blender Text Editor use Text > Reload or Alt R (or Option R on the Mac). You can also make a script that you open in the Blender Text Editor that executes an external script you edit in your own editor. See for example the script in Fig. 1: B. Figure 1: The Scripting workspace in Blender","title":"Run scripts within the Blender interface"},{"location":"advanced/python_scripting/1_api_basics/#run-scripts-from-the-command-line","text":"You can also run Python scripts in Blender directly from the command-line interface. An example of executing a script ( -P ) without opening the Blender GUI ( -b , for background) on a Mac would be: blender -b -P script.py Or if you want to render the first frame ( -f 1 ) from an example test.blend file. The output will go to the directory of the blender file ( -o //... ) and it will generate a PNG image file ( -F PNG ). blender -b test.blend -o //render_ -F PNG -f 1 More information on command line arguments is here .","title":"Run scripts from the command-line"},{"location":"advanced/python_scripting/1_api_basics/#load-modules-in-blender","text":"You can add startup scripts to your Blender installation. These scripts can then be easily imported into your script. These scripts can be placd here: scripts\u2028/startup/ in the Blender directory (for example on a Mac: /Applications/Blender.app/Contents/Resources/2.81/scripts\u2028/startup/ ). For scripts you want to import for just one project or Blender file you can use the normal Python method of editing sys.path : import sys sys . path . append ( \"/some_directory/\" ) import python_module Tip The official binaries of Blender from blender.org include the numpy Python module, so import numpy should work out of the box","title":"Load modules in Blender"},{"location":"advanced/python_scripting/2_accessing_data/","text":"Accessing Blender data \u00b6 Using bpy.data \u00b6 All data in a Blender file can be accessed through bpy.data . This contains for example all objects ( bpy.data.objects ), all meshes ( bpy.data.meshes ), all scenes ( bpy.data.scenes ) and all materials ( bpy.data.materials ). The data is stored in a data-type called bpy_collection whose members (data blocks) can be accessed with both an index as well as a string (this in contrary to regular Python dictionaries). For example. bpy.data.objects[\"Cube\"] and bpy.data.objects[1] will be equivalent if Cube is the second object in the collection. Attributes of data blocks (e.g an object, collection or material) can be accessed with a period. For example: >>> bpy . data . objects [ 0 ] . name 'Camera' Two examples of changing attributes (note that some operations only work if Blender is in the right mode): bpy . data . objects [ \"Cube\" ] . location . z += 1 # this works in both edit and object mode bpy . data . objects [ \"Cube\" ] . data . vertices [ 0 ] . co . z += 10 # this works only in object mode Tips Use the Python Console in Blender and the auto-complete functionality to see what attributes bpy.data has. The Info Editor in Blender shows the python commands being executed when you do operations manually in Blender (See Fig. 2.) Hovering over buttons and input boxes in Blender shows how to access the underlying values through the Python API. Figure 2: The Info Editor is a nice way to see what python commands are executed when you use Blender. In this figure we see that we deleted the initial cube, made a UV Sphere and translated it. Some notes on bpy.context and bpy.ops \u00b6 In this section we want to briefly introduce how you can access the so-called context and use operators in the Blender Python API. bpy.context stores information about a user's selections and the context Blender is in. For example, if you want to check which mode is currently active in Blender you can check the value of bpy.context.mode . Now if you want to change the mode, you can use an operator. Operators are tools that are usually accessed through the user interface with buttons and menus. You can access these operators with Python through bpy.ops . If we would like to change the mode we can do this using an operator, e.g. bpy.ops.object.mode_set(mode='OBJECT') Of course switching to, say, edit mode, depends on which objects are selected, which can be checked with bpy.context.selected_objects . But keep in mind that many of the variables in the context are read-only, altering bpy.context.selected_objects directly is not possible. Instead, you can select an object with the select_set() method of the object, e.g. bpy.data.objects['Cube'].select_set(True) . \ud83d\udcbb Running a script and rendering from the console \u00b6 Write an external script that removes the Cube object that is part of the default scene 1 Then, from the command line and without opening the Blender GUI execute this script and render the first frame. Let it output a PNG image file in the directory of the blender file. Was the cube indeed removed from the rendered image? Extra question: is the cube removed from the blender file? Although you might have altered your startup scene to not have the cube \u21a9","title":"Accessing Blender data"},{"location":"advanced/python_scripting/2_accessing_data/#accessing-blender-data","text":"","title":"Accessing Blender data"},{"location":"advanced/python_scripting/2_accessing_data/#using-bpydata","text":"All data in a Blender file can be accessed through bpy.data . This contains for example all objects ( bpy.data.objects ), all meshes ( bpy.data.meshes ), all scenes ( bpy.data.scenes ) and all materials ( bpy.data.materials ). The data is stored in a data-type called bpy_collection whose members (data blocks) can be accessed with both an index as well as a string (this in contrary to regular Python dictionaries). For example. bpy.data.objects[\"Cube\"] and bpy.data.objects[1] will be equivalent if Cube is the second object in the collection. Attributes of data blocks (e.g an object, collection or material) can be accessed with a period. For example: >>> bpy . data . objects [ 0 ] . name 'Camera' Two examples of changing attributes (note that some operations only work if Blender is in the right mode): bpy . data . objects [ \"Cube\" ] . location . z += 1 # this works in both edit and object mode bpy . data . objects [ \"Cube\" ] . data . vertices [ 0 ] . co . z += 10 # this works only in object mode Tips Use the Python Console in Blender and the auto-complete functionality to see what attributes bpy.data has. The Info Editor in Blender shows the python commands being executed when you do operations manually in Blender (See Fig. 2.) Hovering over buttons and input boxes in Blender shows how to access the underlying values through the Python API. Figure 2: The Info Editor is a nice way to see what python commands are executed when you use Blender. In this figure we see that we deleted the initial cube, made a UV Sphere and translated it.","title":"Using bpy.data"},{"location":"advanced/python_scripting/2_accessing_data/#some-notes-on-bpycontext-and-bpyops","text":"In this section we want to briefly introduce how you can access the so-called context and use operators in the Blender Python API. bpy.context stores information about a user's selections and the context Blender is in. For example, if you want to check which mode is currently active in Blender you can check the value of bpy.context.mode . Now if you want to change the mode, you can use an operator. Operators are tools that are usually accessed through the user interface with buttons and menus. You can access these operators with Python through bpy.ops . If we would like to change the mode we can do this using an operator, e.g. bpy.ops.object.mode_set(mode='OBJECT') Of course switching to, say, edit mode, depends on which objects are selected, which can be checked with bpy.context.selected_objects . But keep in mind that many of the variables in the context are read-only, altering bpy.context.selected_objects directly is not possible. Instead, you can select an object with the select_set() method of the object, e.g. bpy.data.objects['Cube'].select_set(True) .","title":"Some notes on bpy.context and bpy.ops"},{"location":"advanced/python_scripting/2_accessing_data/#running-a-script-and-rendering-from-the-console","text":"Write an external script that removes the Cube object that is part of the default scene 1 Then, from the command line and without opening the Blender GUI execute this script and render the first frame. Let it output a PNG image file in the directory of the blender file. Was the cube indeed removed from the rendered image? Extra question: is the cube removed from the blender file? Although you might have altered your startup scene to not have the cube \u21a9","title":"\ud83d\udcbb Running a script and rendering from the console"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/","text":"Creating geometry, colors and materials \u00b6 Creating an object with a mesh \u00b6 If we want to create a new mesh we can do this by calling the new function like this: mesh = bpy . data . meshes . new ( \"newMesh\" ) This will create the mesh but it is not linked to an object (it will not show in the Outliner ). So we make a new object and link the object to the mesh: obj = bpy . data . objects . new ( \"newObject\" , mesh ) We can actually verify this worked correctly by checking the value of obj.data : >>> obj.data bpy.data.meshes['newMesh'] If you check the Outliner in the user interface you will see both the object newObject and the mesh newMesh linked to it. Now we have an empty mesh, linked to an object. We will now construct a simple piece geometry to show how this is done in Blender. Vertices are defined by their x, y and z values like this: verts = [ ( 0 , 0 , 0 ), ( 0 , 2 , 0 ), ( 0 , 1 , 2 ) ] Edges are defined as a tuple holding two indices pointing to two vertices in the verts list. So (0,1) refers to a line from vertex (0,0,0) (index 0 in verts ) to (0,2,0) (index 1 in verts ) in this example. We make the following edges: edges = [ ( 0 , 1 ), ( 1 , 2 ), ( 2 , 0 ) ] To make faces we need three or more vertices. Per face you make a tuple of three or more indices pointing to three vertices in the verts list. For example the face (0,1,2) is a face made up from the vertices (0,0,0), (0,2,0) and (0,1,2), which are at index 0, 1 and 2 in the verts list. For now lets make one face: faces = [ ( 0 , 1 , 2 ) ] We now use a function from the Python API to make a mesh from our verts, edges and faces: mesh . from_pydata ( verts , edges , faces ) Now the mesh and the object are created, but it does not yet show in the 3D viewport or the Outliner . This is because we still need to link the new object to an existing collection and in so doing to a scene. bpy . data . collections [ 0 ] . objects . link ( obj ) To summarize here is the full code to generate this geometry: import bpy # Create a new mesh ob_name = \"triangle\" mesh = bpy . data . meshes . new ( ob_name + \"_mesh\" ) # Create a new object with the mesh ob = bpy . data . objects . new ( ob_name , mesh ) # Define some geometry verts = [ ( 0 , 0 , 0 ), ( 0 , 2 , 0 ), ( 0 , 1 , 2 ) ] edges = [ ( 0 , 1 ), ( 1 , 2 ), ( 2 , 0 ) ] # These are indices pointing to elements in the list verts faces = [ ( 0 , 1 , 2 ) ] # These are indices pointing to elements in the list verts # Add it to the mesh mesh . from_pydata ( verts , edges , faces ) # Link the object to the first collection bpy . data . collections [ 0 ] . objects . link ( ob ) Tips Note that in general you do not need to explicitly specify edges, as these will be generated automatically based on the faces specified. It's only when you want to have edges that are not connected to faces that you need to specify them explicitly. All objects in Blender (and object data of the same type, i.e. all meshes) are enforced to have unique names. When using the Python API this is no different. So if you create an object with bpy.data.objects.new(\"obj\", mesh) and there already is an object named \"obj\" the name of the new object will be automatically set to something else. This can become important if you generate many objects (say in a loop) but still want to be able to refer to them later by name. \ud83d\udcbb A filled disk from scratch \u00b6 In the text above we created a triangle, now as an exercise let's create a spherical disk. First create a ring of vertices, then create edges and a face. Adding vertex colors to a mesh \u00b6 Vertex coloring is a way to color a mesh without using textures or uv-mapping. It works by assigning for every face that a vertex is a member of a color to that vertex. So a vertex can have different colors for each of the different faces it is in. Let's say we have a mesh, named \"triangle_mesh\": mesh = bpy.data.meshes['triangle_mesh'] , the vertex colors for this mesh will be stored in mesh.vertex_colors . If the mesh does not have a vertex color layer yet, you can make a new one with: mesh.vertex_colors.new(name='vert_colors') . Now we have a color layer to work with: color_layer = mesh.vertex_colors['vert_colors'] . \ud83d\udcbb Making triangles and make a vertex color layer \u00b6 Let's take the triangle we made in section 4.1, but let's add another triangle to it, attached to the first. The code would look like this: import bpy # Create a new mesh ob_name = \"triangle\" mesh = bpy . data . meshes . new ( ob_name + \"_mesh\" ) # Create a new object with the mesh ob = bpy . data . objects . new ( ob_name , mesh ) # Define some geometry verts = [ ( 0 , 0 , 0 ), ( 0 , 2 , 0 ), ( 0 , 1 , 2 ) , ( 0 , 3 , 2 ) ] edges = [ ( 0 , 1 ), ( 1 , 2 ), ( 2 , 0 ), ( 1 , 3 ), ( 3 , 2 ) ] # These are indices pointing to elements in the list verts faces = [ ( 0 , 1 , 2 ), ( 1 , 3 , 2 ) ] # These are indices pointing to elements in the list verts # Add it to the mesh mesh . from_pydata ( verts , edges , faces ) # Link the object to the first collection bpy . data . collections [ 0 ] . objects . link ( ob ) Now make a vertex color layer for your triangles. Then inspect how many entries are in color_layer = mesh.vertex_colors['vert_colors'] . Why are they the same or different from the total number of vertices in the mesh? In exercise 4 we saw that color_layer.data contains six entries while we only have four vertices in the mesh. This is because a vertex has a color for every face it is in. So vertex (0,2,0) and (0,1,2) are each in two faces, while the other two vertices are only in one face. So the former vertices have two entries in the color layer, one for each face they are in, the latter only one color entry. The link between vertex indices in a mesh and those in the vertex color layer can be deduced from the polygons in mesh.polygons . Let's take one polygon from the triangles, lets say the first ( poly = mesh.polygons[0] ). Now, for one vertex in the polygon, poly.vertices gives you the index of the vertex in the mesh and poly.loop_indices gives you the index of the vertex in color_layer.data . See Fig. 3. Figure 3: Sketch of the two triangles from Exercise 4. For the vertices are shown the coordinates (in black italic (x, x, x)), indices of the vertex in its mesh (green, outside of the face) and the indices in the loop_indices of the polygon (red, italic and inside the faces.) Once you have set colors for your vertices you need to set up the shader of the object 1 . For this go to the Shading workspace. Create a Vertex Color node and connect it to a Principled BSDF (connect Color output to Base Color input). And then make a Material Output and connect the Principled BSDF to the Surface input of the Material Output . See Fig. 4. Figure 4: Shader setup for vertex colors \ud83d\udcbb Coloring your triangles \u00b6 Let's take the two connected triangles of exercise 4. We will color them in two different ways, using vertex coloring and Python scripting. a) Make the first triangle (face (0,1,2)) green and the second (face (1,3,2)) red. b) Now color vertex (0,0,0) and (0,3,2) red and (0,2,0) and (0,1,2) green. Adding a material \u00b6 You can also add materials through the Python API. As an example to show how you could do this, let's add a material to the triangle from exercise 4 in the last section. Materials are stored in bpy.data.material and we can make a new material: # Make material triangle_material_name = \"triangle_mat\" mat = bpy . data . materials . new ( triangle_material_name ) The nodes and the node tree are stored in the material 1 . mat . use_nodes = True nodes = mat . node_tree . nodes Before we start making nodes we remove the automatically generated nodes. nodes . clear () We will make two nodes, one Principled BSDF shader and an output node. We can make the shader by making a new node. shader = nodes . new ( type = 'ShaderNodeBsdfPrincipled' ) How a node type is called you can search up in Blender in the following way. Go to the Shading workspace and open the add menu in the Shader Editor . Now go to Shader and hover over Principled BSDF until an information pop-up appears. In the pop-up you can find how the node type is called. See Fig. 5. Figure 5: The type name of a node can be found by navigating to the Add menu and hovering over the node of your interest If you also want to organize the nodes in the Shader Editor you can place the node like this: shader . location = 0 , 300 # Location in the node window We can set the inputs of the Principled BSDF shader to a default_value. shader . inputs [ 'Base Color' ] . default_value = ( 1 , 0 , 0 , 1 ) We can now also make an output node and place it in the Shader Editor . node_output = nodes . new ( type = 'ShaderNodeOutputMaterial' ) node_output . location = 400 , 300 Links between nodes can be made using the links in the node_tree. A new link will take outputs and inputs from the nodes you want to link. links = mat . node_tree . links links . new ( shader . outputs [ 0 ], node_output . inputs [ 0 ]) Now we only need to add the material to the mesh containing the spherical disk. mesh . materials . append ( mat ) In summary, the total code for making the material is: # Make material triangle_material_name = \"triangle_mat\" mat = bpy . data . materials . new ( triangle_material_name ) mat . use_nodes = True nodes = mat . node_tree . nodes # Clear default nodes nodes . clear () shader = nodes . new ( type = 'ShaderNodeBsdfPrincipled' ) shader . location = 0 , 300 # Location in the node window shader . inputs [ 'Base Color' ] . default_value = ( 1 , 0 , 0 , 1 ) # Create an output for the shader node_output = nodes . new ( type = 'ShaderNodeOutputMaterial' ) node_output . location = 400 , 300 links = mat . node_tree . links links . new ( shader . outputs [ 'BSDF' ], node_output . inputs [ 'Surface' ]) mesh . materials . append ( mat ) Although we make minimal use of shading in this chapter, you might not be familiar with shading and materials in Blender. Here is a great introduction of how to use materials in Blender. \u21a9 \u21a9","title":"Creating geometry, colors and materials"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#creating-geometry-colors-and-materials","text":"","title":"Creating geometry, colors and materials"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#creating-an-object-with-a-mesh","text":"If we want to create a new mesh we can do this by calling the new function like this: mesh = bpy . data . meshes . new ( \"newMesh\" ) This will create the mesh but it is not linked to an object (it will not show in the Outliner ). So we make a new object and link the object to the mesh: obj = bpy . data . objects . new ( \"newObject\" , mesh ) We can actually verify this worked correctly by checking the value of obj.data : >>> obj.data bpy.data.meshes['newMesh'] If you check the Outliner in the user interface you will see both the object newObject and the mesh newMesh linked to it. Now we have an empty mesh, linked to an object. We will now construct a simple piece geometry to show how this is done in Blender. Vertices are defined by their x, y and z values like this: verts = [ ( 0 , 0 , 0 ), ( 0 , 2 , 0 ), ( 0 , 1 , 2 ) ] Edges are defined as a tuple holding two indices pointing to two vertices in the verts list. So (0,1) refers to a line from vertex (0,0,0) (index 0 in verts ) to (0,2,0) (index 1 in verts ) in this example. We make the following edges: edges = [ ( 0 , 1 ), ( 1 , 2 ), ( 2 , 0 ) ] To make faces we need three or more vertices. Per face you make a tuple of three or more indices pointing to three vertices in the verts list. For example the face (0,1,2) is a face made up from the vertices (0,0,0), (0,2,0) and (0,1,2), which are at index 0, 1 and 2 in the verts list. For now lets make one face: faces = [ ( 0 , 1 , 2 ) ] We now use a function from the Python API to make a mesh from our verts, edges and faces: mesh . from_pydata ( verts , edges , faces ) Now the mesh and the object are created, but it does not yet show in the 3D viewport or the Outliner . This is because we still need to link the new object to an existing collection and in so doing to a scene. bpy . data . collections [ 0 ] . objects . link ( obj ) To summarize here is the full code to generate this geometry: import bpy # Create a new mesh ob_name = \"triangle\" mesh = bpy . data . meshes . new ( ob_name + \"_mesh\" ) # Create a new object with the mesh ob = bpy . data . objects . new ( ob_name , mesh ) # Define some geometry verts = [ ( 0 , 0 , 0 ), ( 0 , 2 , 0 ), ( 0 , 1 , 2 ) ] edges = [ ( 0 , 1 ), ( 1 , 2 ), ( 2 , 0 ) ] # These are indices pointing to elements in the list verts faces = [ ( 0 , 1 , 2 ) ] # These are indices pointing to elements in the list verts # Add it to the mesh mesh . from_pydata ( verts , edges , faces ) # Link the object to the first collection bpy . data . collections [ 0 ] . objects . link ( ob ) Tips Note that in general you do not need to explicitly specify edges, as these will be generated automatically based on the faces specified. It's only when you want to have edges that are not connected to faces that you need to specify them explicitly. All objects in Blender (and object data of the same type, i.e. all meshes) are enforced to have unique names. When using the Python API this is no different. So if you create an object with bpy.data.objects.new(\"obj\", mesh) and there already is an object named \"obj\" the name of the new object will be automatically set to something else. This can become important if you generate many objects (say in a loop) but still want to be able to refer to them later by name.","title":"Creating an object with a mesh"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#a-filled-disk-from-scratch","text":"In the text above we created a triangle, now as an exercise let's create a spherical disk. First create a ring of vertices, then create edges and a face.","title":"\ud83d\udcbb A filled disk from scratch"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#adding-vertex-colors-to-a-mesh","text":"Vertex coloring is a way to color a mesh without using textures or uv-mapping. It works by assigning for every face that a vertex is a member of a color to that vertex. So a vertex can have different colors for each of the different faces it is in. Let's say we have a mesh, named \"triangle_mesh\": mesh = bpy.data.meshes['triangle_mesh'] , the vertex colors for this mesh will be stored in mesh.vertex_colors . If the mesh does not have a vertex color layer yet, you can make a new one with: mesh.vertex_colors.new(name='vert_colors') . Now we have a color layer to work with: color_layer = mesh.vertex_colors['vert_colors'] .","title":"Adding vertex colors to a mesh"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#making-triangles-and-make-a-vertex-color-layer","text":"Let's take the triangle we made in section 4.1, but let's add another triangle to it, attached to the first. The code would look like this: import bpy # Create a new mesh ob_name = \"triangle\" mesh = bpy . data . meshes . new ( ob_name + \"_mesh\" ) # Create a new object with the mesh ob = bpy . data . objects . new ( ob_name , mesh ) # Define some geometry verts = [ ( 0 , 0 , 0 ), ( 0 , 2 , 0 ), ( 0 , 1 , 2 ) , ( 0 , 3 , 2 ) ] edges = [ ( 0 , 1 ), ( 1 , 2 ), ( 2 , 0 ), ( 1 , 3 ), ( 3 , 2 ) ] # These are indices pointing to elements in the list verts faces = [ ( 0 , 1 , 2 ), ( 1 , 3 , 2 ) ] # These are indices pointing to elements in the list verts # Add it to the mesh mesh . from_pydata ( verts , edges , faces ) # Link the object to the first collection bpy . data . collections [ 0 ] . objects . link ( ob ) Now make a vertex color layer for your triangles. Then inspect how many entries are in color_layer = mesh.vertex_colors['vert_colors'] . Why are they the same or different from the total number of vertices in the mesh? In exercise 4 we saw that color_layer.data contains six entries while we only have four vertices in the mesh. This is because a vertex has a color for every face it is in. So vertex (0,2,0) and (0,1,2) are each in two faces, while the other two vertices are only in one face. So the former vertices have two entries in the color layer, one for each face they are in, the latter only one color entry. The link between vertex indices in a mesh and those in the vertex color layer can be deduced from the polygons in mesh.polygons . Let's take one polygon from the triangles, lets say the first ( poly = mesh.polygons[0] ). Now, for one vertex in the polygon, poly.vertices gives you the index of the vertex in the mesh and poly.loop_indices gives you the index of the vertex in color_layer.data . See Fig. 3. Figure 3: Sketch of the two triangles from Exercise 4. For the vertices are shown the coordinates (in black italic (x, x, x)), indices of the vertex in its mesh (green, outside of the face) and the indices in the loop_indices of the polygon (red, italic and inside the faces.) Once you have set colors for your vertices you need to set up the shader of the object 1 . For this go to the Shading workspace. Create a Vertex Color node and connect it to a Principled BSDF (connect Color output to Base Color input). And then make a Material Output and connect the Principled BSDF to the Surface input of the Material Output . See Fig. 4. Figure 4: Shader setup for vertex colors","title":"\ud83d\udcbb Making triangles and make a vertex color layer"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#coloring-your-triangles","text":"Let's take the two connected triangles of exercise 4. We will color them in two different ways, using vertex coloring and Python scripting. a) Make the first triangle (face (0,1,2)) green and the second (face (1,3,2)) red. b) Now color vertex (0,0,0) and (0,3,2) red and (0,2,0) and (0,1,2) green.","title":"\ud83d\udcbb Coloring your triangles"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#adding-a-material","text":"You can also add materials through the Python API. As an example to show how you could do this, let's add a material to the triangle from exercise 4 in the last section. Materials are stored in bpy.data.material and we can make a new material: # Make material triangle_material_name = \"triangle_mat\" mat = bpy . data . materials . new ( triangle_material_name ) The nodes and the node tree are stored in the material 1 . mat . use_nodes = True nodes = mat . node_tree . nodes Before we start making nodes we remove the automatically generated nodes. nodes . clear () We will make two nodes, one Principled BSDF shader and an output node. We can make the shader by making a new node. shader = nodes . new ( type = 'ShaderNodeBsdfPrincipled' ) How a node type is called you can search up in Blender in the following way. Go to the Shading workspace and open the add menu in the Shader Editor . Now go to Shader and hover over Principled BSDF until an information pop-up appears. In the pop-up you can find how the node type is called. See Fig. 5. Figure 5: The type name of a node can be found by navigating to the Add menu and hovering over the node of your interest If you also want to organize the nodes in the Shader Editor you can place the node like this: shader . location = 0 , 300 # Location in the node window We can set the inputs of the Principled BSDF shader to a default_value. shader . inputs [ 'Base Color' ] . default_value = ( 1 , 0 , 0 , 1 ) We can now also make an output node and place it in the Shader Editor . node_output = nodes . new ( type = 'ShaderNodeOutputMaterial' ) node_output . location = 400 , 300 Links between nodes can be made using the links in the node_tree. A new link will take outputs and inputs from the nodes you want to link. links = mat . node_tree . links links . new ( shader . outputs [ 0 ], node_output . inputs [ 0 ]) Now we only need to add the material to the mesh containing the spherical disk. mesh . materials . append ( mat ) In summary, the total code for making the material is: # Make material triangle_material_name = \"triangle_mat\" mat = bpy . data . materials . new ( triangle_material_name ) mat . use_nodes = True nodes = mat . node_tree . nodes # Clear default nodes nodes . clear () shader = nodes . new ( type = 'ShaderNodeBsdfPrincipled' ) shader . location = 0 , 300 # Location in the node window shader . inputs [ 'Base Color' ] . default_value = ( 1 , 0 , 0 , 1 ) # Create an output for the shader node_output = nodes . new ( type = 'ShaderNodeOutputMaterial' ) node_output . location = 400 , 300 links = mat . node_tree . links links . new ( shader . outputs [ 'BSDF' ], node_output . inputs [ 'Surface' ]) mesh . materials . append ( mat ) Although we make minimal use of shading in this chapter, you might not be familiar with shading and materials in Blender. Here is a great introduction of how to use materials in Blender. \u21a9 \u21a9","title":"Adding a material"},{"location":"advanced/python_scripting/4_volumetric_data/","text":"Visualizing volumetric data \u00b6 In this section we will show a simple example of how to visualize custom volumetric data with Blender and Python. The current support in Blender for volumetric data is directly tied to the OpenVDB file format. In fact, the only way to create a volume object is to load an OpenVDB file. This is a file format and data structure that originated from the motion-picture industry, where it is often used to show clouds, smoke and fire in computer graphics like movies and games. Here's an example of such a volumetric rendering: Gasoline explosion. Free example from Embergen. The reason OpenVDB is used for many volumetric data applications in computer graphics is that is allows sparse volumes to be stored efficiently, while also providing easy querying of the data, for example during rendering. OpenVDB is also a bit more than just a file format, as the OpenVDB library also supports more advanced operations. From the OpenVDB website: OpenVDB is an Academy Award-winning C++ library comprising a hierarchical data structure and a suite of tools for the efficient manipulation of sparse, time-varying, volumetric data discretized on three-dimensional grids. It is based on VDB, which was developed by Ken Museth at DreamWorks Animation, and it offers an effectively infinite 3D index space, compact storage, fast data access, and a collection of algorithms specifically optimized for the data structure for common tasks such as filtering, CSG, compositing, numerical simulation, sampling, and voxelization from other geometric representations. For more documentation on OpenVDB see here . Some example OpenVDB files can be found here , under Sample Models. Example \u00b6 OpenVDB models are mostly generated with specialized software like Houdini and Embergen . Volumetric data in general is also used for scientific visualizations, for example in ParaView , but support for OpenVDB is still lacking somewhat. In this section we will explain how OpenVDB files can be made from scratch. For example for when you have you own volumetric data in your own data format and you want to visualize or animate this in Blender. To convert your data to a OpenVDB format we will use the Python package pyopenvdb . First we will create data in Python and write it to an OpenVDB file using OpenVDB and the Python package pyopenvdb . Installation of pyopenvdb \u00b6 Installing the Python module to access the openVDB functionality can be very easy or more difficult depending on your operating system. See the installation instructions on the pyopenvdb website . Tip If you cannot get it to work that way, we made a simple Docker container you can use to run it, see here for the github repository. Making a VDB file with pyopenvdb \u00b6 Let us make a simple volumetric cube using pyopenvdb . To start we first load pyopenvdb and numpy: import numpy as np import pyopenvdb as vdb And we make a zero filled array of size 400x400x400: dimension = 400 array = np . zeros (( dimension , dimension , dimension )) We then fill a cube sized portion of the array with the value 1: for i in range ( dimension ): for j in range ( dimension ): for k in range ( dimension ): if i < 200 and i >= 100 and \\ j < 200 and j >= 100 and \\ k < 200 and k >= 100 : array [ i , j , k ] = 1.0 Now we come to the openvdb part, where we first need to make a grid. In this case we make a float grid (there are more grids besides a float grid for example a BoolGrid and Vec3SGrid are also standardly available). grid = vdb . FloatGrid () We now copy the values in the array into the grid: grid . copyFromArray ( array ) The last important thing we need to do before we save it to file is to name the grid. You will use this name later when using the grid in Blender. grid . name = \"cube\" The last thing left to do is to save the grid to file: vdb . write ( 'cube.vdb' , grids = [ grid ]) Loading a VDB file into Blender \u00b6 Open a new Blender file and if its there, remove the starting cube. In the 3D viewport choose Add => Volume => Import OpenVDB or use the shortcut Shift-A . Locate the cube.vdb file we just made through the script. You will most likely not see anything yet, so scale the cube down using the shortcut s until you can see the outline of the cube. Now if you change the Viewport shading in the top right of the 3D viewport to Render preview (see Fig. 1, #1), you will not see anything beside the outline since we still need to make a shader for the model. Figure 1: definition of coordinates Change to the Shading workspace (see Fig. 1, #2) and in the Shader editor click on new to make a new material (see Fig. 1, #3). You see Blender makes a Principled volume and Material output node. To make the cube appear we need to change one thing and for this we need to know the name of the grid in the VDB file. From the Python script we know this is cube , but you can also figure out the grids and their names in a VDB file from within Blender. In the Properties panel go to Object Data Properties tab (see Fig. 1, #4). Here under Grids you can see the names of the grids in the VDB file. For now, in the Principled Volume node, add the name of the grid ( cube ) into the field next to Density Attribute (see Fig. 1, #5). This tells the node to use the values in the grid for the scattering density of the voxels. \ud83d\udcbb Coloring the cube \u00b6 Now make a cube similar to the one we just made, but color it blue on one side and red on the other (See Fig. 2). First alter the Python script to include a second grid in the VDB file. In this second grid set one side of the cube to value 1 and the other to zero. Use an Attribute node (do not forget to add the grid name to the Name: field in the attribute node) to feed the second grid into a ColorRamp node (and choose the colors you want). Now feed the ColorRamp into the Color field of the Principled Volume . Do not forget to set the original grid in the Density Attribute . Does it come out right? Maybe you need to play a bit with settings, like set the Density to 1. You might also need to play with the lighting. If you still have the original light in your scene, try increasing its Power and location. Now also see how it looks in Cycles compared to Eevee . Figure 2: Colored cube","title":"Visualizing volumetric data"},{"location":"advanced/python_scripting/4_volumetric_data/#visualizing-volumetric-data","text":"In this section we will show a simple example of how to visualize custom volumetric data with Blender and Python. The current support in Blender for volumetric data is directly tied to the OpenVDB file format. In fact, the only way to create a volume object is to load an OpenVDB file. This is a file format and data structure that originated from the motion-picture industry, where it is often used to show clouds, smoke and fire in computer graphics like movies and games. Here's an example of such a volumetric rendering: Gasoline explosion. Free example from Embergen. The reason OpenVDB is used for many volumetric data applications in computer graphics is that is allows sparse volumes to be stored efficiently, while also providing easy querying of the data, for example during rendering. OpenVDB is also a bit more than just a file format, as the OpenVDB library also supports more advanced operations. From the OpenVDB website: OpenVDB is an Academy Award-winning C++ library comprising a hierarchical data structure and a suite of tools for the efficient manipulation of sparse, time-varying, volumetric data discretized on three-dimensional grids. It is based on VDB, which was developed by Ken Museth at DreamWorks Animation, and it offers an effectively infinite 3D index space, compact storage, fast data access, and a collection of algorithms specifically optimized for the data structure for common tasks such as filtering, CSG, compositing, numerical simulation, sampling, and voxelization from other geometric representations. For more documentation on OpenVDB see here . Some example OpenVDB files can be found here , under Sample Models.","title":"Visualizing volumetric data"},{"location":"advanced/python_scripting/4_volumetric_data/#example","text":"OpenVDB models are mostly generated with specialized software like Houdini and Embergen . Volumetric data in general is also used for scientific visualizations, for example in ParaView , but support for OpenVDB is still lacking somewhat. In this section we will explain how OpenVDB files can be made from scratch. For example for when you have you own volumetric data in your own data format and you want to visualize or animate this in Blender. To convert your data to a OpenVDB format we will use the Python package pyopenvdb . First we will create data in Python and write it to an OpenVDB file using OpenVDB and the Python package pyopenvdb .","title":"Example"},{"location":"advanced/python_scripting/4_volumetric_data/#installation-of-pyopenvdb","text":"Installing the Python module to access the openVDB functionality can be very easy or more difficult depending on your operating system. See the installation instructions on the pyopenvdb website . Tip If you cannot get it to work that way, we made a simple Docker container you can use to run it, see here for the github repository.","title":"Installation of pyopenvdb"},{"location":"advanced/python_scripting/4_volumetric_data/#making-a-vdb-file-with-pyopenvdb","text":"Let us make a simple volumetric cube using pyopenvdb . To start we first load pyopenvdb and numpy: import numpy as np import pyopenvdb as vdb And we make a zero filled array of size 400x400x400: dimension = 400 array = np . zeros (( dimension , dimension , dimension )) We then fill a cube sized portion of the array with the value 1: for i in range ( dimension ): for j in range ( dimension ): for k in range ( dimension ): if i < 200 and i >= 100 and \\ j < 200 and j >= 100 and \\ k < 200 and k >= 100 : array [ i , j , k ] = 1.0 Now we come to the openvdb part, where we first need to make a grid. In this case we make a float grid (there are more grids besides a float grid for example a BoolGrid and Vec3SGrid are also standardly available). grid = vdb . FloatGrid () We now copy the values in the array into the grid: grid . copyFromArray ( array ) The last important thing we need to do before we save it to file is to name the grid. You will use this name later when using the grid in Blender. grid . name = \"cube\" The last thing left to do is to save the grid to file: vdb . write ( 'cube.vdb' , grids = [ grid ])","title":"Making a VDB file with pyopenvdb"},{"location":"advanced/python_scripting/4_volumetric_data/#loading-a-vdb-file-into-blender","text":"Open a new Blender file and if its there, remove the starting cube. In the 3D viewport choose Add => Volume => Import OpenVDB or use the shortcut Shift-A . Locate the cube.vdb file we just made through the script. You will most likely not see anything yet, so scale the cube down using the shortcut s until you can see the outline of the cube. Now if you change the Viewport shading in the top right of the 3D viewport to Render preview (see Fig. 1, #1), you will not see anything beside the outline since we still need to make a shader for the model. Figure 1: definition of coordinates Change to the Shading workspace (see Fig. 1, #2) and in the Shader editor click on new to make a new material (see Fig. 1, #3). You see Blender makes a Principled volume and Material output node. To make the cube appear we need to change one thing and for this we need to know the name of the grid in the VDB file. From the Python script we know this is cube , but you can also figure out the grids and their names in a VDB file from within Blender. In the Properties panel go to Object Data Properties tab (see Fig. 1, #4). Here under Grids you can see the names of the grids in the VDB file. For now, in the Principled Volume node, add the name of the grid ( cube ) into the field next to Density Attribute (see Fig. 1, #5). This tells the node to use the values in the grid for the scattering density of the voxels.","title":"Loading a VDB file into Blender"},{"location":"advanced/python_scripting/4_volumetric_data/#coloring-the-cube","text":"Now make a cube similar to the one we just made, but color it blue on one side and red on the other (See Fig. 2). First alter the Python script to include a second grid in the VDB file. In this second grid set one side of the cube to value 1 and the other to zero. Use an Attribute node (do not forget to add the grid name to the Name: field in the attribute node) to feed the second grid into a ColorRamp node (and choose the colors you want). Now feed the ColorRamp into the Color field of the Principled Volume . Do not forget to set the original grid in the Density Attribute . Does it come out right? Maybe you need to play a bit with settings, like set the Density to 1. You might also need to play with the lighting. If you still have the original light in your scene, try increasing its Power and location. Now also see how it looks in Cycles compared to Eevee . Figure 2: Colored cube","title":"\ud83d\udcbb Coloring the cube"},{"location":"advanced/python_scripting/api_summary/","text":"The Blender API in detail \u00b6 Introduction \u00b6 The Blender Python API mostly consists of a thin layer on top of the underlying Blender C/C++ data structures and methods. The underlying C/C++ code is used to automatically generate the Python API during the build process of the Blender executable, which means the API is always up-to-date with respect to the underlying code. The API isn't the only part of Blender that uses Python. Large parts of the user interface, the import/export scripts and all addons are written in Python. It is therefore relatively easy to extend Blender with, say, new UI dialogs or an importer. This is one of the strengths of the Blender Python API. Danger Since the API provides access to Blender internals at a very low level you can screw up the Blender state quite easily, causing unexpected behaviour, data corruption or even crashes. In the worst case you can end up with a file that will no longer load in Blender at all, although that's rare. So when working with Python scripting, save your session to file often, preferably in a number of incremental versions, so you can recover or go a step back when needed. In cases where you suspect Blender's internal state has been corrupted you can save the current state to a temporary file, start a second instance of Blender (keeping the first Blender running!) and then open the temporary file in the second instance to help ensure you start from a known-good state. This prevents you from saving correct Blender state and overwriting your last known-good file. API modules \u00b6 The Blender Python API is comprised of several modules, with bpy being the main one. But there's also useful routines in mathutils , bmesh and a few others. Tip The API documentation on these modules can be easily accessed from within Blender using Help > Python API Reference . By default none of the API modules, not even bpy , is loaded in the environment where a script file runs, so you need to import the ones you need explicitly. The Python Console does import quite a few things by defaults and also sets some useful variables, like C to access bpy.context and D to access bpy.data with less typing: PYTHON INTERACTIVE CONSOLE 3.9.4 ( default , Apr 20 2021 , 15 : 51 : 38 ) [ GCC 10.2.0 ] Builtin Modules : bpy , bpy . data , bpy . ops , bpy . props , bpy . types , bpy . context , bpy . utils , bgl , blf , mathutils Convenience Imports : from mathutils import * ; from math import * Convenience Variables : C = bpy . context , D = bpy . data >>> D . objects . values () [ bpy . data . objects [ 'Camera' ], bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Light' ]] Developer settings \u00b6 When developing Python scripts in Blender it can be useful to enable a few extra settings: The Python Tooltips under Interface > Display > Python Tooltips . When enabled a tooltip will show the corresponding Python command or a path to the data for a UI element. The Developer Extras under Interface > Display > Developer Extras . When enabled this provides multiple things: The 3D viewport overlay for a mesh in edit mode will now have an extra setting Indices to show the low-level indices of selected vertices/edges/faces. This can be very useful when debugging Python code that works on mesh geometry. The right-click menu for a UI item, such as a button or menu entry, will now also contain an entry called Online Python Reference linking to the relevant Python documentation page. It will enable Operator Search , which will add entries to the F3 search menu for operators. These will be listed after the regular menu entries in the search results. It adds a new menu option Help > Operator Cheat Sheet that will create a new text area called OperatorList.txt , which contains all available operators (see below ) and their default parameters. This list can give you a quick overview of the available operators, with the API documentation providing all the details. As mentioned in the video in the introductory chapter the Info area can be useful if you want to inspect which Python calls Blender performs for certain operations. This certainly will not provide all the details in all cases, but can give some insight. You can either switch to the default Scripting workspace to check the Info area, or use the normal UI area operations to add/change an area to an Info area. Data-blocks \u00b6 The different types of data in Blender are stored in data-blocks . For example, there's Mesh, Object, Texture and Shader data-blocks, but there's quite a few more . One of the clever bits in the way Blender is programmed is that data-blocks written to file contain enough information about their content (i.e. metadata) to make them readable by both older and newer versions of Blender than the one they were written with. This metadata system also allows the Python API for accessing those data-blocks to get generated without much manual development. Data-blocks are available through Python, per type, under bpy.data , e.g. bpy.data.objects or bpy.data.meshes . The type of a data-block is the corresponding class under bpy.types , e.g. >>> type ( bpy . data . objects [ 'Cube' ]) < class ' bpy_types . Object '> >>> bpy . types . Object < class ' bpy_types . Object '> Each type of data-block has its own set of attributes and methods, particular to that type. Learning the Blender Python API involves getting to know the details of the data-block types you want to work with and how they interact. Blender keeps track of which data-blocks are no longer being referenced to decide when a data-block does not need to be saved (so-called garbage collection). Usually you don't need to explicitly interact with this system, but it is good to be aware that it is there, see this section below for more details. Unique data-block names \u00b6 Per type of data all the data-blocks need to have a unique name . This is enforced automatically by Blender when a data-block is created by appending a number to make the name unique. For example: >>> bpy . data . meshes . new ( 'my object' ) bpy . data . meshes [ 'my object' ] >>> bpy . data . meshes . new ( 'my object' ) bpy . data . meshes [ 'my object.001' ] >>> bpy . data . meshes . new ( 'my object' ) bpy . data . meshes [ 'my object.002' ] This usually isn't an issue, but just something to be aware of when working with referencing objects by name, as the name of a data-block you created might actually be something different than you expect. Objects and object data \u00b6 When we use the word Object in these pages we mean one of the object types that can be present in a 3D scene, e.g. camera, mesh or light. Such objects are of type bpy.types.Object and all have general properties related to their presence in the 3D scene. For example, their name, 3D transformation, visibility flags, parent, etc. But a Light object needs to specify different properties than, say, a Camera object and these per-type properties are stored as object data . The object data can be accessed through the data attribute of an Object: # Both lights and cameras are Objects >>> type ( bpy . data . objects [ 'Light' ]) < class ' bpy_types . Object '> >>> type ( bpy . data . objects [ 'Camera' ]) < class ' bpy_types . Object '> # But their object data are of a different type >>> type ( bpy . data . objects [ 'Camera' ] . data ) < class ' bpy . types . Camera '> >>> type ( bpy . data . objects [ 'Light' ] . data ) < class ' bpy . types . PointLight '> # And have different attributes, relevant to that type >>> dir ( bpy . data . objects [ 'Camera' ] . data ) [ ... , 'angle' , ... , 'clip_start' , ... , 'dof' , ... ] >>> dir ( bpy . data . objects [ 'Light' ] . data ) [ ... , 'color' , ... , 'distance' , 'energy' , ... , 'falloff_type' , ... ] Objects of a specific type \u00b6 Sometimes you want to iterate over all objects in a scene, but only perform some operation on a specific type of object. You can use the type attribute for checking an object's type: >>> bpy . data . objects [ 'Camera' ] . type 'CAMERA' >>> bpy . data . objects [ 'Light' ] . type 'LIGHT' Native Blender data structures \u00b6 When working with the Python API will you frequently use internal Blender types that appear similar to regular Python types, like lists and dictionaries. However, the Blender types are not real native Python types and behave differently in certain aspects. For example, the different collections of scene elements (such as objects or meshes) that are available under bpy.data are of type bpy_prop_collection . This type is a combination of a Python list and a dictionary, sometimes called an ordered dictionary, as it allows indexing by both array position and key: >>> type ( bpy . data . objects ) < class ' bpy_prop_collection '> # Some of its methods match those of native Python data types >>> dir ( bpy . data . objects ) [ '__bool__' , '__contains__' , '__delattr__' , '__delitem__' , '__doc__' , '__doc__' , '__getattribute__' , '__getitem__' , '__iter__' , '__len__' , '__module__' , '__setattr__' , '__setitem__' , '__slots__' , 'bl_rna' , 'find' , 'foreach_get' , 'foreach_set' , 'get' , 'items' , 'keys' , 'new' , 'remove' , 'rna_type' , 'tag' , 'values' ] # Index by position >>> bpy . data . objects [ 0 ] bpy . data . objects [ 'Camera' ] # Index by key >>> bpy . data . objects [ 'Camera' ] bpy . data . objects [ 'Camera' ] # (key, value) pairs >>> bpy . data . objects . items () [( 'Camera' , bpy . data . objects [ 'Camera' ]), ( 'Cube' , bpy . data . objects [ 'Cube' ]), ( 'Light' , bpy . data . objects [ 'Light' ])] Note that the position of an item in the collection, and hence its index, can change during a Blender session. Inspecting values \u00b6 One of the more annoying aspects when working in the Blender Python Console inspecting these kinds of values is that the elements in a bpy_prop_collection (or other Blender types) aren't printed by default, this in contrast to a regular Python dictionary. You need to, for example, cast to a list or call its values() method: # Regular Python dict, prints both keys and values >>> d = dict ( a = 1 , b = 2 , c = 3 ) >>> d { 'a' : 1 , 'b' : 2 , 'c' : 3 } # No items printed >>> bpy . data . objects < bpy_collection [ 3 ], BlendDataObjects > # values() returns a list, so gets printed in detail >>> type ( bpy . data . objects . values ()) < class ' list '> >>> bpy . data . objects . values () [ bpy . data . objects [ 'Camera' ], bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Light' ]] # Difference in list() result: >>> list ( d ) [ 'a' , 'b' , 'c' ] # Returns dict *keys* >>> list ( bpy . data . objects ) [ bpy . data . objects [ 'Camera' ], bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Light' ]] # Returns collection *values* The choice for not printing the values inside a bpy_prop_collection is (most likely) that in many cases the collection will contain large numbers of objects, so printing them all would not be too useful, or might even make the UI non-responsive for a short time. Data organization \u00b6 In certain cases Blender uses a more elaborate data structure in cases where you might except low-level values, like lists. For example, the set of vertices that make up a mesh are only accessible as a collection of MeshVertex objects: >>> m bpy . data . meshes [ 'Cube' ] >>> type ( m . vertices ) < class ' bpy_prop_collection '> >>> len ( m . vertices ) 8 >>> m . vertices [ 0 ] bpy . data . meshes [ 'Cube' ] . vertices [ 0 ] >>> type ( m . vertices [ 0 ]) < class ' bpy . types . MeshVertex '> >>> dir ( m . vertices [ 0 ]) [ '__doc__' , '__module__' , '__slots__' , 'bevel_weight' , 'bl_rna' , 'co' , 'groups' , 'hide' , 'index' , 'normal' , 'rna_type' , 'select' , 'undeformed_co' ] # Vertex coordinate (object space) >>> m . vertices [ 0 ] . co Vector (( 1.0 , 1.0 , 1.0 )) # Vertex normal >>> m . vertices [ 0 ] . normal Vector (( 0.5773491859436035 , 0.5773491859436035 , 0.5773491859436035 )) The reason for this is that there's several types of data associated with a single vertex, which are all centralized in a MeshVertex object. In short, Blender uses a so-called array-of-structs design. The alternative design choice would have been to have separate arrays for vertex coordinates, vertex normals, etc (which would be a struct-of-arrays design). Vertices and matrices \u00b6 The example above also shows that even a vertex coordinate is not accessed as a low-level Python data type, like a tuple, but by the Vector type (which is in the mathutils module). This has the advantage of providing many useful methods for operating on vector values: >>> v = m . vertices [ 0 ] . normal >>> v Vector (( 0.5773491859436035 , 0.5773491859436035 , 0.5773491859436035 )) >>> v . length 0.999998137353116 # Return a new vector that's orthogonal >>> w = v . orthogonal () >>> w Vector (( 0.5773491859436035 , 0.5773491859436035 , - 1.154698371887207 )) # Dot product (should be zero as v and w are orthogonal) >>> v . dot ( w ) 0.0 # Note: v*w is element-wise product, not dot product! >>> v * w Vector (( 0.3333320915699005 , 0.3333320915699005 , - 0.666664183139801 )) # Cross product between two vectors >>> v . cross ( w ) Vector (( - 0.9999963045120239 , 0.9999963045120239 , 0.0 )) # Swizzling (returning vector elements in a different order) >>> w Vector (( 0.5773491859436035 , 0.5773491859436035 , - 1.154698371887207 )) >>> w . zxy Vector (( - 1.154698371887207 , 0.5773491859436035 , 0.5773491859436035 )) The builtin mathutils module contains many useful data types and methods for working with 3D data, including vectors and matrices, but also different methods for working with transformations (like quaternion) and colors spaces. # Transformation matrix for an object with uniform scale 2 and # translation in Z of 3. These values will match with the Transform UI area >>> o bpy . data . objects [ 'Cube' ] >>> o . matrix_world Matrix ((( 2.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 2.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 2.0 , 3.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # Create a rotation matrix >>> m = Matrix . Rotation ( radians ( 90.0 ), 4 , 'X' ) >>> m Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 7.549790126404332e-08 , - 1.0 , 0.0 ), ( 0.0 , 1.0 , 7.549790126404332e-08 , 0.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) >>> v = Vector (( 1 , 2 , 3 )) # Transform the vector using the matrix. Note the different outcomes # depending on the multiplication order. >>> m @ v Vector (( 1.0 , - 2.999999761581421 , 2.000000238418579 )) >>> v @ m Vector (( 1.0 , 3.000000238418579 , - 1.999999761581421 )) # Also, a 3-vector is assumed to have a fourth element equal to *one* when # multiplying with a matrix: >>> m = Matrix . Translation (( 4 , 5 , 6 )) >>> m Matrix ((( 1.0 , 0.0 , 0.0 , 4.0 ), ( 0.0 , 1.0 , 0.0 , 5.0 ), ( 0.0 , 0.0 , 1.0 , 6.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) >>> m @ Vector (( 1 , 2 , 3 )) Vector (( 5.0 , 7.0 , 9.0 )) >>> m @ Vector (( 1 , 2 , 3 , 0 )) Vector (( 1.0 , 2.0 , 3.0 , 0.0 )) Selections \u00b6 In a lot of cases you want to operate on a set of objects. You can access (read only) the current selection with bpy.context.selected_objects : >>> bpy . context . selected_objects [ bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Plane' ]] Changing the current selection can be done in several ways. Control over selection state per object can be controlled with the select_get() and select_set() methods: >>> bpy . context . selected_objects [] >>> bpy . data . objects [ 'Camera' ] . select_get () False >>> bpy . data . objects [ 'Camera' ] . select_set ( True ) >>> bpy . context . selected_objects [ bpy . data . objects [ 'Camera' ]] The full selection set can also be changed: # Select all visible objects >>> bpy . ops . object . select_all ( action = 'SELECT' ) # Deselect all objects >>> bpy . ops . object . select_all ( action = 'DESELECT' ) # Toggle the selection state for each object >>> bpy . ops . object . select_all ( action = 'TOGGLE' ) Note that the default mode for bpy.ops.object.select_all() when not specified is TOGGLE . Also note that the selection methods above operate only on objects that are currently visible objects in the scene (in terms of the outliner eye icon), just like for the selection hotkeys (like A ) in the 3D viewport. Often used values and operations \u00b6 Here, we list some frequently used parts of the API, for varying types of data. Scene \u00b6 Current scene: bpy.context.scene (read-only) Objects \u00b6 Active object: bpy.context.active_object (read-only) Selected objects: bpy.context.selected_objects (read-only) Delete selected objects: bpy.ops.object.delete() Camera \u00b6 Active camera object: Scene.camera (this is the camera object , not camera object data ) Type: Camera.type (\"PERSP\", \"ORTHO\", ...) Focal length: Camera.lens (in mm) Clipping distances: Camera.clip_start , Camera.clip_end Rendering \u00b6 Image resolution: Width: Scene.render.settings.resolution_x Height: Scene.render.settings.resolution_y Percentage: Scene.render.settings.resolution_percentage Output file: Scene.render.filepath Image output type: Scene.render.image_settings.file_format (\"PNG\", \"JPEG\", ...) Number of samples per pixel (Cycles): Scene.cycles.samples Render current scene: bpy.ops.render.render() . See parameters how to control the specific type of render (still image versus animation) and whether to save output Animation \u00b6 Current frame Scene.frame_current Frame range: Scene.frame_start , Scene.frame_end Frame rate: Scene.render.fps File I/O \u00b6 Save the current session to a specific file: bpy.ops.wm.save_as_mainfile() Open Blend file bpy.ops.wm.open_mainfile() Import a file (call depends on file type): bpy.ops.import_scene.obj() (OBJ scene), bpy.ops.import_scene.gltf (glTF scene), bpy.ops.import_mesh.ply (PLY mesh), etc. See here and here Export a file (call depends on file type) follows the same call names, see here and here Object transformations \u00b6 The matrix_world attribute of an Object contains the object-to-world transform that places the object in the 3D scene: >>> o = bpy . context . active_object >>> o bpy . data . objects [ 'Cube' ] >>> o . matrix_world Matrix ((( 1.3376139402389526 , 0.0 , 0.0 , 0.3065159320831299 ), ( 0.0 , 1.3376139402389526 , 0.0 , 2.2441697120666504 ), ( 0.0 , 0.0 , 1.3376139402389526 , 1.2577730417251587 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) Comparing this matrix with the values set in the Transform panel, you can see the location is stored in the right-most column of the matrix and the scaling along the diagonal. If there was a rotation set on this object some of these values would not be as recognizable anymore. The location, rotation and scale values can also be inspected and set separately: >>> o . location Vector (( 0.3065159320831299 , 2.2441697120666504 , 1.2577730417251587 )) >>> o . rotation_euler Euler (( 0.0 , 0.0 , 0.0 ), 'XYZ' ) >>> o . scale Vector (( 1.3376139402389526 , 1.3376139402389526 , 1.3376139402389526 )) >>> o . location = ( 1 , 2 , 3 ) # Needs value in radians >>> o . rotation_euler . x = radians ( 45 ) >>> o . scale = ( 2 , 1 , 1 ) >>> o . matrix_world Matrix ((( 2.0 , 0.0 , 0.0 , 1.0 ), ( 0.0 , 0.7071067690849304 , - 0.7071067690849304 , 2.0 ), ( 0.0 , 0.7071067690849304 , 0.7071067690849304 , 3.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) See the section on parenting below for some subtle effects on transformations in cases object parenting is used. Geometry coordinates \u00b6 Mesh geometry in Blender stores vertex coordinates (and other geometric information) in object-space coordinates. But a mesh (or object in general) will usually get transformed to a specific position, scaling and orientation in the scene. As described above the net transform from object-space to world-space coordinates, also called the object-to-world transform, is available through matrix_world . In cases where you need to have access to geometric data in world-space , say vertex coordinates, you need to apply the matrix_world transform manually. For example, given the cube transformed as shown above, with vertex 7 selected (visible bottom-left in the image below): >>> o bpy . data . objects [ 'Cube' ] >>> m = o . data >>> o . matrix_world Matrix ((( 1.3376139402389526 , 0.0 , 0.0 , 0.3065159320831299 ), ( 0.0 , 1.3376139402389526 , 0.0 , 2.2441697120666504 ), ( 0.0 , 0.0 , 1.3376139402389526 , 1.2577730417251587 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # The object-space coordinate of this vertex >>> m . vertices [ 7 ] . co Vector (( - 1.0 , - 1.0 , - 1.0 )) # The world-space coordinate of this vertex, which matches # what the Transform UI shows. Note the Global display mode # select in the UI, if we select Local if will show (-1, -1, -1). >>> o . matrix_world @ m . vertices [ 7 ] . co Vector (( - 1.0310980081558228 , 0.9065557718276978 , - 0.07984089851379395 )) API quirks \u00b6 Working with the Blender Python API has some peculiarities compared to your average Python scripting. These have to do with the way the API is structured, but also how it interacts with the Blender internals. The API manual contains a lengthy page on some gotchas, but here we list some of the common ones. Object modes \u00b6 An object is always in one of several modes. These modes are the same ones you work with in the UI: Object mode, Edit mode, etc. The current mode for an object can be retrieved through the mode property: >>> o = bpy . data . objects [ 'Cube' ] >>> o . mode 'OBJECT' # <enter edit mode with TAB> >>> o . mode 'EDIT' Depending on the current mode of a mesh object certain data might not be up-to-date or even unavailable when accessing it through the Python API, this is especially true when an object is in Edit Mode . This is because the edit mode uses its own copy of the data to let you edit, which is synced with the underlying mesh data when going in and out of edit mode. See here for the relevant section in the Blender API docs. An example continuing with the Cube mesh above: >>> o . mode 'OBJECT' >>> m = o . data >>> m bpy . data . meshes [ 'Cube' ] # Check UV map data >>> len ( m . uv_layers [ 0 ] . data ) 24 # <enter edit mode with TAB> >>> o . mode 'EDIT' # UV map data now empty... >>> len ( m . uv_layers [ 0 ] . data ) 0 In most cases when working on low-level data such as mesh geometry you want the object to be in object mode (or use the bmesh module when you need the object be in edit mode). It's usually a good idea to add a check at the top of your script to verify the current mode is what you expect: o = bpy . context . active_object if o . mode != 'OBJECT' : raise ValueError ( 'Active object needs to be in object mode!' ) There are alternatives for still allowing a mesh to be in edit-mode when accessing its data from a script, see the API docs for details. Interrupting (long-running) scripts \u00b6 During script development you might get in a situation where your code is stuck in a loop, or takes much longer than you like. Interrupting a running script can usually be done by pressing Ctrl-C in the terminal console window : >>> while True : ... pass ... # Uh oh, execution stuck in a loop and the UI will now have become unresponsive # Pressing Ctrl-C in the terminal console window interrupts script execution, # as it raises a KeyboardInterrupt Traceback ( most recent call last ): File \"<blender_console>\" , line 2 , in < module > KeyboardInterrupt Interaction with the Undo system \u00b6 When you undo certain operations Blender might re-create certain data, which might cause existing references to the original data to become invalid. This can be especially noticeable when working interactively in the Python Console. For example, with a cube object as active object in the 3D viewport: # The Cube is the active object >>> bpy . context . active_object bpy . data . objects [ 'Cube' ] # Save a reference to it >>> o = bpy . context . active_object # <Grab the object in the 3D viewport and move it somewhere else> # Object reference still valid >>> o bpy . data . objects [ 'Cube' ] # <Undo the object translation in the 3D viewport> # Object reference has now become invalid >>> o < bpy_struct , Object invalid > # Reason: object referenced under name 'Cube' has changed >>> bpy . data . objects [ 'Cube' ] == o False >>> id ( o ) 140543077302976 >>> id ( bpy . data . objects [ 'Cube' ]) 140543077308608 # Will need to reacquire active object, or consistently use bpy.data.objects['Cube'] >>> o = bpy . context . active_object >>> o bpy . data . objects [ 'Cube' ] Operators \u00b6 A special class of important API routines are the so-called operators . These are usually higher-level operations, such as adding a new cube mesh, deleting the current set of selected objects or running a file importer. As noted above many parts of the Blender UI are set up with Python scripts and in a lot of cases the operations you perform in the UI through menu actions or shortcut keys will simply call the relevant operator from Python to do the actual work. The Info area will show most operators as they get executed, but you can also check what API call is made for a certain UI element (this requires Python Tooltips to be enabled, see above ). For example, adding a plane mesh through the Add menu will call the operator bpy.ops.mesh.primitive_plane_add() , as the tooltip shows: You can simply call the operator directly from Python to add a plane in exactly the same way as with the menu option: >>> bpy . data . objects . values () [] >>> bpy . ops . mesh . primitive_plane_add () { 'FINISHED' } # A plane mesh is now added to the scene >>> bpy . data . objects . values () [ bpy . data . objects [ 'Plane' ]] Many of the operators take parameters, to influence the results. For example, with bpy.ops.mesh.primitive_plane_add() you can set the initial size and location of the plane (see the API docs for all the parameters): >>> bpy . ops . mesh . primitive_plane_add ( size = 3 , location = ( 1 , 2 , 3 )) { 'FINISHED' } Info Note that operator parameters can only be passed using keyword arguments . Operator context \u00b6 This is all very nice and powerful, but operators have a few inherent properties that can make them tricky to work with. An operator's execution crucially depends on the context in which it is called, where it gets most of the data it needs. As shown above simple parameter values can usually be passed, but values like the object(s) to operate on are retrieved implicitly. For example, to join a set of mesh objects into a single mesh you can call the operator bpy.ops.object.join() . But the current context needs to be correctly set for the operator to work: # We have no objects selected >>> bpy . context . selected_objects [] >>> bpy . ops . object . join () Warning : Active object is not a selected mesh { 'CANCELLED' } # With 3 objects selected >>> bpy . context . selected_objects [ bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Cube.001' ], bpy . data . objects [ 'Cube.002' ]] # Now it works >>> bpy . ops . object . join () { 'FINISHED' } As can be seen above an operator only returns a value indicating the execution status. When calling the operator in the Python Console as above some extra info is printed. But when calling operators from scripts the status return value is all you have to go on, as the extra message isn't printed when the script is executed. And in some cases the reason an operator fails can be quite unclear: >>> bpy . context . selected_objects [ bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Camera' ]] >>> bpy . ops . mesh . intersect_boolean () Traceback ( most recent call last ): File \"<blender_console>\" , line 1 , in < module > File \"/usr/share/blender/2.92/scripts/modules/bpy/ops.py\" , line 132 , in __call__ ret = _op_call ( self . idname_py (), None , kw ) RuntimeError : Operator bpy . ops . mesh . intersect_boolean . poll () failed , context is incorrect This merely shows that the so-called poll function failed. The poll function is used by operators to determine if they can execute in the current context, by checking certain preconditions on things like the selected object(s), the type of data or an object mode. In this case the bpy.ops.mesh.intersect_boolean() operator doesn't perform a boolean intersection on multiple meshes, but only on the faces of a single object in edit mode, but you can't tell from the error message (nor does the documentation make that clear): To actually perform a boolean intersection on two objects from a Python script requires us to do what we would be do in the UI: add a Boolean modifier on one of the objects and set its parameters. We could take advantage of the Python Tooltips to see which operator we need: This would suggest that using bpy.ops.modifier_add(type='BOOLEAN') would be what we need, but then setting the required parameters on the modifier (i.e. the object to subtract) would become tricky. So for a boolean operation, and setting object modifiers in general, there's an easier way: >>> o = bpy . data . objects [ 'Cube' ] # Add a modifier on the object and set its parameters >>> mod = o . modifiers . new ( name = 'boolmod' , type = 'BOOLEAN' ) >>> mod . object = bpy . data . objects [ 'Cube.001' ] >>> mod . operation = 'DIFFERENCE' # At this point the modifier is all set up. We hide # the object we subtract to make the boolean result visible. >>> bpy . data . objects [ 'Cube.001' ] . hide_viewport = True Unfortunately, certain operations can only be performed by calling operators. So there's a good chance that you will need to use them at some point when doing Python scripting. Hopefully this section gives some clues as how to work with them. See this section for more details on all the above subtleties and issues relating to working with operators. The bpy.ops documentation also contains useful information on operators, including how to override an operator's implicit context with values you set yourself. Meshes \u00b6 One of the more common scene data types to work with from Python are 3D meshes. Meshes in Blender can contain polygons of an arbitrary number of vertices (so-called N-gons), can contain wire edges and support extra layers of data, such as vertex colors and UV coordinates. We go into a fair amount of detail on how to create and access mesh data, in several ways. As usual, the Blender API docs on the Mesh type contain many more details, but we feel the discussion below is a good summary to get you started for many use cases. Creating a mesh (high-level) \u00b6 As shown earlier the Mesh.from_pydata(vertices, edges, faces) method allows a simple and high-level way of creating a mesh. This method doesn't offer full control over the created mesh and isn't very fast for large meshes, but it can be good enough in a lot of cases. It takes three lists of values, or actually, any Python iterable that matches the expected form: vertices: a sequence of float triples, e.g. [(1.0, 2.0, 3.0), (4, 5, 6), ...] edges: a sequence of integer pairs (vertex indices), that define edges by. If [] is passed edges are inferred from polygons faces: a sequence of one or more polygons, each defined as a sequence of 3 or more vertex indices. E.g. [(0, 1, 2), (1, 2, 3, 4), ...] Info The choice of how the mesh data is passed might incur an overhead in memory usage and processing time, especially when regular Python data structures, like lists, are used. An alternative would be to pass NumPy arrays. For the examples below we assume that no explicit list of edges is passed. Edges will then be created implicitly based on the polygons specified, which is usually what is preferred. We discuss explicitly specifying edges below . An example of creating a simple mesh: # Create a mesh consisting of 3 polygons using 6 vertices vertices = [ ( 0 , 0 , 0 ), ( 2 , 0 , 0 ), ( 2 , 2 , 0.2 ), ( 0 , 2 , 0.2 ), ( 1 , 3 , 1 ), ( 1 , - 1 , - 1 ), ] polygons = [ ( 0 , 1 , 2 , 3 ), # Quad ( 4 , 3 , 2 ), # Triangle ( 0 , 5 , 1 ) # Triangle ] m = bpy . data . meshes . new ( name = 'my mesh' ) m . from_pydata ( vertices , [], polygons ) At this point we have created a new Mesh Python object, which corresponds to Object Data of type Mesh. Object Data cannot be directly added to a scene, but needs to be referenced by a 3D Object: # Create an object referencing the mesh data o = bpy . data . objects . new ( name = 'my mesh' , object_data = m ) # Add the object to the scene bpy . context . scene . collection . objects . link ( o ) The resulting mesh and outliner entry looks like this: Careful: invalid data \u00b6 Note that it is possible to set up a mesh with invalid/inconsistent data when setting the underlying arrays manually, as is the case here. This can cause weird behaviour or even crashes. For example: # 3 vertices vertices = [ ( 0 , 0 , 0 ), ( 1 , 1 , 1 ), ( - 1 , 2 , - 1 ) ] # Invalid vertex index 3 used! polygons = [ ( 0 , 1 , 2 , 3 ) ] m = bpy . data . meshes . new ( name = 'my invalid mesh' ) m . from_pydata ( vertices , [], polygons ) o = bpy . data . objects . new ( name = 'my invalid mesh' , object_data = m ) bpy . context . scene . collection . objects . link ( o ) When executing the above code a new mesh is added to the scene, but it will show as a triangle in the 3D viewport, instead of a quad. And even though that doesn't appear to be unreasonable behaviour in this case Blender will crash if we subsequently enter edit mode on the mesh! So the lesson here is to be careful when specifying geometry using these low-level API calls. This actually applies to all parts of the Blender Python API in general. In this case, to make sure a created mesh has valid data we can use the validate() method on a Mesh . This will check the mesh data and remove any invalid values, e.g. by deleting the polygon using non-existent vertex index 3 above. This might not result in a mesh that matches what you want based on the data, but at least you can detect this situation and handle it without Blender crashing. The validate() method has two issues to be aware of: The method returns True in case the mesh does not validate, i.e. when it has issues. More specifically, it returns True when changes were made to the mesh data to remove invalid values. It will only report on the specific issues found when called with validate(verbose=True) and then will only output to the console. But it is still a good idea to always validate a mesh when creating it manually: ... m = bpy . data . meshes . new ( name = 'my invalid mesh' ) m . from_pydata ( vertices , [], polygons ) if m . validate ( verbose = True ): print ( 'Mesh had issues and has been altered! See console output for details' ) In the example of the invalid mesh data above this results in these message being printed in the console output: ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:351 BKE_mesh_validate_arrays: Edge 0: v2 index out of range, 3 ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:351 BKE_mesh_validate_arrays: Edge 3: v2 index out of range, 3 ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:605 BKE_mesh_validate_arrays: Loop 3 has invalid vert reference (3) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:782 BKE_mesh_validate_arrays: Loop 0 is unused. ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:782 BKE_mesh_validate_arrays: Loop 1 is unused. ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:782 BKE_mesh_validate_arrays: Loop 2 is unused. ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:782 BKE_mesh_validate_arrays: Loop 3 is unused. After validate() returns we can see in this case that invalid data was indeed removed: >>> vertices = [ ( 0 , 0 , 0 ), ( 1 , 1 , 1 ), ( - 1 , 2 , - 1 ) ] >>> polygons = [ ( 0 , 1 , 2 , 3 ) ] >>> m = bpy . data . meshes . new ( name = 'my invalid mesh' ) >>> m . from_pydata ( vertices , [], polygons ) >>> len ( m . polygons ) 1 >>> len ( m . edges ) 4 >>> len ( m . vertices ) 3 >>> m . validate () True >>> len ( m . polygons ) 0 >>> len ( m . edges ) 2 >>> len ( m . vertices ) 3 Creating a mesh (low-level) \u00b6 A second, and more flexible, way of creating a mesh is using low-level calls for setting the necessary data arrays directly on a Mesh object. This is especially useful in combination with NumPy arrays, as this allows the creation of large meshes with relatively high performance and low memory overhead. Meshes in Blender are stored using 4 arrays, as attributes of the bpy.types.Mesh type: vertices : vertex locations, each specified by 3 floats loops : contains the vertex indices used for defining polygons of a mesh, each polygon as a sequence of indices in the vertices array polygons : defines the start index of each polygon as an index in loops , plus the length of each polygon in number of vertices edges : defines the edges of the mesh, using two vertex indices per edge So to create a mesh at this level we need to set up the necessary values for these arrays. Here, we create the same mesh as in the previous section, using NumPy arrays for storing the data. # Vertices (8): x1 y1 z1 x2 y2 z2 ... vertices = numpy . array ([ 0 , 0 , 0 , 2 , 0 , 0 , 2 , 2 , 0.2 , 0 , 2 , 0.2 , 1 , 3 , 1 , 1 , - 1 , - 1 , 0 , - 2 , - 1 , 2 , - 2 , - 1 ], dtype = numpy . float32 ) # # Polygons, defined in loops # # List of vertex indices of all loops combined vertex_index = numpy . array ([ 0 , 1 , 2 , 3 , # Quad 4 , 3 , 2 , # Triangle 0 , 5 , 1 # Triangle ], dtype = numpy . int32 ) # For each polygon the start of its indices in vertex_index loop_start = numpy . array ([ 0 , 4 , 7 ], dtype = numpy . int32 ) # Length of each polygon in number of vertices loop_total = numpy . array ([ 4 , 3 , 3 ], dtype = numpy . int32 ) We additionally also specify texture coordinates and vertex colors. This is something that is not possible with the high-level from_pydata() API shown above. Note that we need to specify these values per vertex per polygon loop . # Texture coordinates per vertex per polygon loop uv_coordinates = numpy . array ([ 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , # Quad 0.5 , 1 , 0 , 0 , 1 , 0 , # Triangle 0 , 1 , 0.5 , 0 , 1 , 1 # Triangle ], dtype = numpy . float32 ) # Vertex color (RGBA) per vertex per polygon loop vertex_colors = numpy . array ([ 1 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 1 , 1 , ], dtype = numpy . float32 ) Next, we create a new mesh using the above arrays: num_vertices = vertices . shape [ 0 ] // 3 num_vertex_indices = vertex_index . shape [ 0 ] num_loops = loop_start . shape [ 0 ] m = bpy . data . meshes . new ( name = 'my detailed mesh' ) # Vertices m . vertices . add ( num_vertices ) m . vertices . foreach_set ( 'co' , vertices ) # Polygons m . loops . add ( num_vertex_indices ) m . loops . foreach_set ( 'vertex_index' , vertex_index ) m . polygons . add ( num_loops ) m . polygons . foreach_set ( 'loop_start' , loop_start ) m . polygons . foreach_set ( 'loop_total' , loop_total ) # Create UV coordinate layer and set values uv_layer = m . uv_layers . new ( name = 'default' ) uv_layer . data . foreach_set ( 'uv' , uv_coordinates ) # Create vertex color layer and set values vcol_layer = m . vertex_colors . new () vcol_layer . data . foreach_set ( 'color' , vertex_colors ) # Done, update mesh object m . update () # Validate mesh if m . validate ( verbose = True ): print ( 'Mesh data did not validate!' ) # Create an object referencing the mesh data o = bpy . data . objects . new ( name = 'my detailed mesh' , object_data = m ) # Add the object to the scene bpy . context . scene . collection . objects . link ( o ) Info Passing a multi-dimensional NumPy array directly to foreach_set() will not work: >>> vertices = numpy . array ([ ... ( 0 , 0 , 0 ), ( 2 , 0 , 0 ), ( 2 , 2 , 0.2 ), ( 0 , 2 , 0.2 ), ... ( 1 , 3 , 1 ), ( 1 , - 1 , - 1 ), ( 0 , - 2 , - 1 ), ( 2 , - 2 , - 1 ) ... ], 'float32' ) >>> vertices . shape ( 8 , 3 ) >>> m = bpy . data . meshes . new ( name = 'my detailed mesh' ) >>> m . vertices . foreach_set ( 'co' , vertices ) Traceback ( most recent call last ): File \"<blender_console>\" , line 1 , in < module > RuntimeError : internal error setting the array However, passing a flattened array does work: >>> m . vertices . foreach_set ( 'co' , vertices . flatten ()) >>> [ v . co for v in mesh . vertices ] [ Vector (( 0.0 , 0.0 , 0.0 )), Vector (( 2.0 , 0.0 , 0.0 )), Vector (( 2.0 , 2.0 , 0.20000000298023224 )), Vector (( 0.0 , 2.0 , 0.20000000298023224 )), Vector (( 1.0 , 3.0 , 1.0 )), Vector (( 1.0 , - 1.0 , - 1.0 )), Vector (( 0.0 , - 2.0 , - 1.0 )), Vector (( 2.0 , - 2.0 , - 1.0 ))] Specifying edges when creating a mesh \u00b6 In most cases we want to create a mesh consisting of only polygons and in that case don't need to specify edges. For certain mesh objects it can be of interest to also be able to specify edges explicitly, or even to create a mesh that consists only of vertices and edges between them. Edges can be used to add line segments that are not part of polygons. We build upon the example mesh we created above by adding a set of 3 edges: # Create a mesh consisting of 3 polygons using 8 vertices, with 3 extra edges # that are not part of the polygons vertices = [ ( 0 , 0 , 0 ), ( 2 , 0 , 0 ), ( 2 , 2 , 0.2 ), ( 0 , 2 , 0.2 ), ( 1 , 3 , 1 ), ( 1 , - 1 , - 1 ), ( 0 , - 2 , - 1 ), ( 2 , - 2 , - 1 ) ] edges = [ ( 5 , 6 ), ( 6 , 7 ), ( 5 , 7 ) ] polygons = [ ( 0 , 1 , 2 , 3 ), # Quad ( 4 , 3 , 2 ), # Triangle ( 0 , 5 , 1 ) # Triangle ] m = bpy . data . meshes . new ( name = 'my mesh with edges' ) m . from_pydata ( vertices , edges , polygons ) o = bpy . data . objects . new ( name = 'my mesh with edges' , object_data = m ) bpy . context . scene . collection . objects . link ( o ) The resulting mesh and outliner entry looks like this: Note that even though we specified only 3 edges explicitly the polygons in the mesh implicitly define 8 more. These are the edges making up those polygons, with shared edges being present only once. In total this results in 11 edges in the mesh: >>> len ( m . edges ) 11 For the second, low-level, method of mesh creation edges are handled slightly different. Edges can be set explicitly by using Mesh.edges : # Vertices (8): x1 y1 z1 x2 y2 z2 ... vertices = numpy . array ([ 0 , 0 , 0 , 2 , 0 , 0 , 2 , 2 , 0.2 , 0 , 2 , 0.2 , 1 , 3 , 1 , 1 , - 1 , - 1 , 0 , - 2 , - 1 , 2 , - 2 , - 1 ], dtype = numpy . float32 ) # Extra edges (3) not defined implicitly by polygons edges = numpy . array ([ 5 , 6 , 6 , 7 , 5 , 7 ], dtype = numpy . int32 ) # # Polygons, defined in loops # # List of vertex indices of all loops combined vertex_index = numpy . array ([ 0 , 1 , 2 , 3 , # Quad 4 , 3 , 2 , # Triangle 0 , 5 , 1 # Triangle ], dtype = numpy . int32 ) # For each polygon the start of its indices in vertex_index loop_start = numpy . array ([ 0 , 4 , 7 ], dtype = numpy . int32 ) # Length of each polygon in number of vertices loop_total = numpy . array ([ 4 , 3 , 3 ], dtype = numpy . int32 ) num_vertices = vertices . shape [ 0 ] // 3 num_edges = edges . shape [ 0 ] // 2 num_vertex_indices = vertex_index . shape [ 0 ] num_loops = loop_start . shape [ 0 ] m = bpy . data . meshes . new ( name = 'detailed mesh with edges' ) # Vertices m . vertices . add ( num_vertices ) m . vertices . foreach_set ( 'co' , vertices ) # Edges m . edges . add ( num_edges ) m . edges . foreach_set ( 'vertices' , edges ) # Polygons m . loops . add ( num_vertex_indices ) m . loops . foreach_set ( 'vertex_index' , vertex_index ) m . polygons . add ( num_loops ) m . polygons . foreach_set ( 'loop_start' , loop_start ) m . polygons . foreach_set ( 'loop_total' , loop_total ) # Done, update mesh object m . update () # Validate mesh if m . validate ( verbose = True ): print ( 'Mesh data did not validate!' ) Here, we only specify the extra edges and not the polygon edges. But when we try to validate the mesh errors will be reported: ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 0 needs missing edge (0, 1) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 0 needs missing edge (1, 2) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 0 needs missing edge (2, 3) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 0 needs missing edge (3, 0) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 1 needs missing edge (4, 3) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 1 needs missing edge (3, 2) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 1 needs missing edge (2, 4) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 2 needs missing edge (0, 5) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 2 needs missing edge (5, 1) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 2 needs missing edge (1, 0) So the polygon edges, which we did not specify, are being reported. In this case the validate() method will correct this and add the missing edges. But having errors reported for regular polygon edges makes it harder to detect any other issues with the mesh data. So the Mesh.update() method provides the option calc_edges . By default this option is False , but when set to True all edges in the mesh will be recalculated to be consistent with the available vertex indices, polygons and extra edges set. ... # Done, update mesh object and recalculate edges m . update ( calc_edges = True ) Validation now succeeds: >>> m . validate ( verbose = True ) False Accessing mesh data (object mode) \u00b6 Inspecting or using mesh data is straightforward. Here we use one of the meshes created with the low-level methods above and retrieve some of its data. Note that Blender provides a few values derived from the original arrays, such as loop_indices and vertices per polygon, which can be useful for certain operations. m = bpy . data . meshes [ 'my detailed mesh' ] len ( m . vertices ) => 8 len ( m . polygons ) => 3 # 2 triangles + 1 quad = 2*3 + 1*4 = 10 len ( m . loops ) => 10 # 8 implicit edges (for 2 triangles and 1 quad), shared edges only listed once len ( m . edges ) => 8 m . vertices [ 7 ] . co => Vector (( 2.0 , - 2.0 , - 1.0 )) # Coordinate m . vertices [ 7 ] . normal => Vector (( 0.6 .. , - 0.6 .. , - 0.3 .. )) # Normal m . vertices [ 7 ] . select => True # Selected (edit mode) m . polygons [ 2 ] . index => 2 # Useful in 'for p in m.polygons' m . polygons [ 2 ] . loop_start => 7 # First index in loops array m . polygons [ 2 ] . loop_total => 3 # Number of vertices in loop m . polygons [ 2 ] . loop_indices => [ 7 , 8 , 9 ] # Indices in m.loops m . loops [ 7 ] . vertex_index => 0 m . loops [ 8 ] . vertex_index => 5 m . loops [ 9 ] . vertex_index => 1 m . polygons [ 2 ] . vertices => [ 0 , 5 , 1 ] # Actual vertex indices m . polygons [ 2 ] . select => True # Selected (edit mode) m . polygons [ 2 ] . use_smooth => False # Smooth shading enabled # These are automatically computed m . polygons [ 2 ] . area => 1.4142135381698608 m . polygons [ 2 ] . normal => Vector (( 0.0 , - 0.707 ... , 0.707 ... )) m . polygons [ 2 ] . center => Vector (( 1.0 , - 0.333 ... , - 0.333 ... )) m . edges [ 0 ] . vertices => [ 2 , 3 ] # (bpy_prop_array) Vertex colors \u00b6 A mesh can have multiple sets of vertex colors. Each set has a name and for each vertex the associated color (but see below). By default meshes created in Blender do not have a vertex color layer. >>> m bpy . data . meshes [ 'Cube' ] >>> type ( m . vertex_colors ) < class ' bpy_prop_collection '> # Create a new vertex color layer >>> vcol_layer = m . vertex_colors . new ( name = 'My vertex colors' ) >>> vcol_layer bpy . data . meshes [ 'Cube' ] . vertex_colors [ \"My vertex colors\" ] >>> len ( m . vertex_colors ) 1 # Name shown under Object Data -> Vertex Colors >>> vcol_layer . name 'My vertex colors' The vertex colors themselves are accessed through the data member: >>> type ( vcol_layer . data ) < class ' bpy_prop_collection '> >>> len ( vcol_layer . data ) 24 >>> type ( vcol_layer . data [ 0 ] . color ) < class ' bpy_prop_array '> >>> list ( vcol_layer . data [ 0 ] . color ) [ 1.0 , 1.0 , 1.0 , 1.0 ] >>> len ( m . polygons ) 6 >>> len ( m . vertices ) 8 >>> len ( m . loops ) 24 One thing to notice here is that the vertex color array has 24 entries. But the Cube object only has 8 vertices and 6 polygons. The reason for the higher number of vertex colors is that Blender stores separate vertex colors per polygon . So the Cube has 6 polygons, each defined using 4 vertices, hence 6*4=24 vertex colors in total (which is the same number as the length of the loops array). This is more flexible than what most 3D file formats allow, which usually only store one color per vertex. During import Blender will duplicate those colors to set the same color for a vertex in all polygons in which it is used. An example of how to take advantage of the added flexibility is that we can set a random color per cube face by setting each of the 4 vertex colors of a face to the same color: for i in range ( 6 ): r = random () g = random () b = random () for j in range ( 4 ): vcol_layer . data [ 4 * i + j ] . color = ( r , g , b , 1 ) A slightly more Blender-like (and robust) way to write the above code would be to take advantage of the polygon loop indices: for p in m . polygons : r = random () g = random () b = random () for i in p . loop_indices : vcol_layer . data [ i ] . color = ( r , g , b , 1 ) Active set \u00b6 As noted above a mesh can have more than one layer of vertex colors. Among the sets present on a mesh there can be only one that is active. The active vertex color layer set controls, for example, which vertex colors are visible in the 3D viewport and are edited in Vertex Paint mode. When adding a vertex color layer (and similar for UV maps described below) through the UI the active layer is changed to the newly added layer. Also, clicking in the Vertex Color layer UI changes the active layer. Below is a list of 2 vertex color layers on a mesh shown, of which Another layer is the active one. The camera icon right of the vertex color names controls which layer is used during rendering by default (and which is set independently of the active status). But in most cases the shader used on an object will explicitly choose a vertex color layer and so override the setting in the UI list. Controling the active vertex color (or UV map) layer can be done using the active property: >>> m . vertex_colors . active_index 1 >>> m . vertex_colors . active bpy . data . meshes [ 'Cube' ] . vertex_colors [ \"Another layer\" ] >>> m . vertex_colors . active = m . vertex_colors [ 0 ] >>> m . vertex_colors . active bpy . data . meshes [ 'Cube' ] . vertex_colors [ \"Col\" ] UV coordinates \u00b6 UV coordinates follow the same setup as vertex colors, but instead store a 2-tuple of floats per vertex per polygon. Note that just like for vertex colors UV coordinates are also specified per vertex per polygon . Meshes created in Blender will already have a UV map called UVMap : >>> m bpy . data . meshes [ 'Cube' ] >>> len ( m . uv_layers ) 1 >>> m . uv_layers [ 0 ] . name 'UVMap' The actual UV values are once again stored under the data member: >>> uv_map = m . uv_layers [ 0 ] >>> uv_map bpy . data . meshes [ 'Cube' ] . uv_layers [ \"UVMap\" ] >>> type ( uv_map . data ) < class ' bpy_prop_collection '> >>> len ( uv_map . data ) 24 >>> type ( uv_map . data [ 0 ]) < class ' bpy . types . MeshUVLoop '> >>> uv_map . data [ 0 ] . uv Vector (( 0.375 , 0.0 )) In general, UV maps are either set through importing or edited within Blender using the UV Editor, although there can be valid reasons for wanting to control them through the Python API. BMesh \u00b6 There is another method in Blender for creating meshes and accessing their data: the so-called BMesh, which is implemented by the bmesh module and its BMesh class. BMesh is especially interesting when you want to perform more complex geometric operations on an existing mesh, or build up a mesh polygon-by-polygon instead of providing the full mesh in one go as a set of arrays as shown above. Here, we only give a brief overview of BMesh and refer to the API docs for all the details. The differences of BMesh compared to working with the native mesh data structure we showed above: A BMesh holds extra data on mesh connectivity, like the neighbours of a vertex, which can be easily queried for geometric editing. The trade-off is that a BMesh will use more memory to store all this extra data, but that is usually only a limiting factor for very large meshes. It is somewhat slower to create a (large) mesh using a BMesh, as each mesh element (vertex, edge, polygon) takes a Python call to create, plus needs extra calls and Python values to set up. A BMesh cannot be used directly in a scene, it first needs to be converted or copied back to a Mesh (and so mesh data is present twice in memory at some point in time) A large set of high- and low-level geometric operations, such as merging vertices within a given distance, face splitting, edge collapsing or generating a convex hull, is provided in bpy.ops and bmesh.utils . These operations would be tedious and error prone to script manually. Here's a (verbose) example of create a BMesh from scratch that holds a single triangle and edge: import bmesh bm = bmesh . new () # Create 3 vertices v1 = bm . verts . new (( 0 , 0 , 0 )) v2 = bm . verts . new (( 1 , 0 , 1 )) v3 = bm . verts . new (( 0 , 1 , 1 )) v4 = bm . verts . new (( 1 , 1 , 1 )) # Add a triangle bm . faces . new (( v1 , v2 , v3 )) # Add a line edge bm . edges . new (( v3 , v4 )) # Done setting up the BMesh, now copy geometry to a regular Mesh m = bpy . data . meshes . new ( 'mesh' ) bm . to_mesh ( m ) # Release BMesh data, bm will no longer be usable bm . free () # Add regular Mesh as object o = bpy . data . objects . new ( 'mesh' , m ) bpy . context . scene . collection . objects . link ( o ) A BMesh can also be created from an existing Mesh , edited and then copied back to the Mesh : o = bpy . context . active_object m = o . data # Create a new BMesh and copy geometry from the Mesh bm = bmesh . new () bm . from_mesh ( m ) # Edit some geometry bm . verts . ensure_lookup_table () bm . verts [ 4 ] . co . x += 3.14 bm . faces . ensure_lookup_table () bm . faces . remove ( bm . faces [ 0 ]) # Copy back to Mesh bm . to_mesh ( m ) bm . free () If a Mesh is currently in edit mode you can still create a BMesh from it, edit that and the copy the changes back, while keeping the Mesh in edit mode: o = bpy . context . active_object m = o . data assert m . mode == 'EDIT' bm = bmesh . new () # Note the different call, i.e. NOT from_mesh() bm . from_edit_mesh ( m ) # <edit BMesh> # Update edit-mesh of Mesh bm . update_edit_mesh ( m ) bm . free () This can be useful when you're working in edit mode on a mesh and also want to run a script on it that uses BMesh, but don't want to switch in and out of edit-mode to run the script. Note that there are some things to watch out for in synchronizing BMesh state to a Mesh . Some examples of the geometric queries that you can do on a BMesh (see docs for more): bm . verts [ i ] . co # Vertex coordinate as mathutils.Vector bm . verts [ i ] . normal # Vertex normal bm . verts [ i ] . is_boundary # True if vertex is at the mesh boundary bm . verts [ i ] . is_wire # True if vertex is not connected to any faces bm . verts [ i ] . link_edges # Sequence of edges connected to this vertex bm . verts [ i ] . link_faces # Sequence of faces connected to this vertex bm . edges [ i ] . calc_length () # Length of the edge bm . edges [ i ] . is_boundary # True if edge is boundary of a face bm . edges [ i ] . is_wire # True if edge is not connected to any faces bm . edges [ i ] . is_manifold # True if edge is manifold (used in at most 2 faces) v = bm . edges [ i ] . verts [ 0 ] # Get one vertex of this edge bm . edges [ i ] . other_vert ( v ) # Get the other vertex bm . edges [ i ] . link_faces # Sequence of faces connected to this edge bm . faces [ i ] . calc_area () # Face area bm . faces [ i ] . calc_center_median () # Median center bm . faces [ i ] . edges # Sequence of edges defining this face bm . faces [ i ] . verts # Sequence of vertices defining this face bm . faces [ i ] . normal # Face normal Materials \u00b6 As shown in one of the introductory exercises it is possible to use Python to create a node-based shader. In most cases using the node-based editor in the UI is the preferred option due to its interactivity, but for certain cases it can be interesting to use Python. The general workflow for this is to create the necessary shader nodes, connected them through links as needed and then set the material on the relevant mesh. # Create a new material mat = bpy . data . materials . new ( \"my material\" ) # Enable shader nodes on the material mat . use_nodes = True # Remove the default nodes nodes = mat . node_tree . nodes nodes . clear () # Add a Principled BSDF shader node and set its base color shader = nodes . new ( type = 'ShaderNodeBsdfPrincipled' ) shader . location = 0 , 300 shader . inputs [ 'Base Color' ] . default_value = ( 1 , 0 , 0 , 1 ) # Add a Material Output node node_output = nodes . new ( type = 'ShaderNodeOutputMaterial' ) node_output . location = 400 , 300 # Add a link between the nodes links = nodes . links links . new ( shader . outputs [ 'BSDF' ], node_output . inputs [ 'Surface' ]) # Add material to the mesh's material slots mesh . materials . append ( mat ) A node's inputs and outputs can be referenced by name. This can then be used to set values on inputs, or connect outputs to inputs, as shown. For example, for the Principled BSDF node above: >>> shader . inputs . keys () [ 'Base Color' , 'Subsurface' , 'Subsurface Radius' , 'Subsurface Color' , 'Metallic' , 'Specular' , 'Specular Tint' , 'Roughness' , 'Anisotropic' , 'Anisotropic Rotation' , 'Sheen' , 'Sheen Tint' , 'Clearcoat' , 'Clearcoat Roughness' , 'IOR' , 'Transmission' , 'Transmission Roughness' , 'Emission' , 'Emission Strength' , 'Alpha' , 'Normal' , 'Clearcoat Normal' , 'Tangent' ] >>> shader . outputs . keys () [ 'BSDF' ] The location attributes set above are not strictly needed if you're not going to work on the shader network in the Shader Editor in the UI. But they help to make the node network layout somewhat visually pleasing. Material slots \u00b6 The last line in the Python code above adds the created material to the mesh's material slots. An object can have multiple materials assigned to it and each assigned material uses a so-called material slot. Each polygon in a mesh can only use a single material, by specifying the material index (i.e. slot) to use for that polygon. This allows different parts of a mesh to use different shaders. By default all faces in a mesh will reference material slot 0. But here's an example of a cube mesh that uses 3 different materials: Inspecting the underlying material data: # Get the mesh, as the material is linked to the mesh by default >>> o = bpy . data . objects [ 'Cube' ] >>> m = o . data # The material slots used >>> list ( m . materials ) [ bpy . data . materials [ 'red' ], bpy . data . materials [ 'black-white checkered' ], bpy . data . materials [ 'voronoi' ]] # Polygon -> slot index >>> m . polygons [ 0 ] . material_index 2 >>> m . polygons [ 1 ] . material_index 0 >>> m . polygons [ 2 ] . material_index 0 >>> m . polygons [ 3 ] . material_index 0 >>> m . polygons [ 4 ] . material_index 1 >>> m . polygons [ 5 ] . material_index 0 Material indices can be set per polygon, or set as an array in one go: # Material slot index for a single polygon m . polygons [ 0 ] . material_index = 0 # Set all polygon material indices face_materials = [ 0 , 1 , 2 , 2 , 1 , 0 ] m . polygons . foreach_set ( 'material_index' , face_materials ) # Force an update of the mesh, needed in this case m . update () Custom properties \u00b6 Sometimes it can useful to be able to control certain values that you use in a script from the UI. The most flexible, but also most complex, approach would be write an add-on . However, in quite a few cases there's a simpler alternative if all you need to control are simple Python values, like an int, float, string or list. From Python you can set custom properties on pretty much any Blender Python data block (see here for more details) and then access those values from the UI: >>> o bpy . data . objects [ 'Cube' ] >>> o [ 'My prop' ] = 123.4 >>> o [ 'My 2nd prop' ] = ( 1 , 1 , 0.5 ) This works, of course, both ways: adding or editing a value from the UI will update the value(s) available through Python. You can then use these values in a script, for example to control a number of objects to create, set a 3D coordinate, etc. See here for more details and examples. Into the deep end... \u00b6 Here we go deeper into some more exotic topics, but which can be of interest with more advanced Python scripting and complex scene setups. Data-block users and garbage collection \u00b6 Blender uses a system based on reference-counting to decide when data-blocks have become unused and can get purged. In the short video below we show some of the details of this scheme: The video shows the Orphan Data outliner mode, but there are several modes that can be used to get detailed insight into the current state of Blender internals: The Blender File mode gives a high-level overview of a file's contents, including some of the more implicit data block types, such as Workspaces. The Data API mode provides an even more detailed view. It is actually a great way to inspect all the gory details of Blender's internal data structures. It will show all data-blocks by type and their attributes. Some attributes can be even be edited in this outliner mode. The Orphan Data mode shows data blocks that do not have any users and which will not be saved (unless they are marked to have a fake user). Some of the data-blocks you see here might not have been created by you, but are used by Blender internally, for example the Brushes. Although the video only focused on materials, the way data-block lifetime is managed using the user counts is general to all types of data-blocks in Blender. But there are subtle differences in whether a data-block is really deleted or just has a link to it removed: Whenever the term \"unlink\" is used it means that a link to that data-block is removed and its user count decreased, but the data-block itself will still be in memory. An example of this is clicking the X next to a mesh's material in the Material Properties. If the UI uses the term \"delete\" it means the data-block is deleted immediately from memory. Any data-blocks linked from the deleted data-block will have their users count decreased. An example of this is deleting a Camera object in the 3D view: the Camera object's data-block is deleted from memory, but the Camera object data data-block (containing the actual camera settings) is still in memory, which you can check in the Orphan Data mode of the outliner. The usage count of data-blocks can also be queried from Python: # Two cube meshes using the same material >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Cube.001' ]] >>> bpy . data . materials [ 'Material' ] . users 2 # Add a new material, set one of the cubes to use it >>> bpy . data . materials [ 'Material' ] . users 1 >>> bpy . data . materials [ 'Material.001' ] . users 1 # <Delete Cube.001 object in the UI> # Hmmm, still has a user? >>> bpy . data . materials [ 'Material.001' ] . users 1 # The reason is we deleted the Cube.001 *object*, but # the Cube.001 *mesh* is still alive (as its usage count # was merely decremented) and it still references the material >>> bpy . data . objects [ 'Cube.001' ] Traceback ( most recent call last ): File \"<blender_console>\" , line 1 , in < module > KeyError : 'bpy_prop_collection[key]: key \"Cube.001\" not found' >>> bpy . data . meshes [ 'Cube.001' ] bpy . data . meshes [ 'Cube.001' ] >>> bpy . data . meshes [ 'Cube.001' ] . users 0 >>> bpy . data . meshes [ 'Cube.001' ] . materials . values () [ bpy . data . materials [ 'Material' ]] The use_fake_user attribute of a data block controls whether a Fake user is set, similar to the checkbox in the UI. Warning In most cases you probably don't want to manually delete data blocks from a file and only use the normal UI operations for that. But it is possible for cases that need it. Truly purging a data block from Python can be done with the relevant remove() method, e.g. >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Cube' ]] >>> o = bpy . context . active_object >>> o bpy . data . objects [ 'Cube' ] >>> m = o . data >>> m bpy . data . meshes [ 'Cube' ] # Remove the Mesh data-block from the file >>> bpy . data . meshes . remove ( m ) >>> bpy . data . meshes . values () [] >>> bpy . data . objects . values () [] Note that in the case of deleting object data (in this case a Mesh) any Objects referencing that object data also get removed ! A second thing to note is the above code does not actually update the current Blender file on disk. That only happens on an explicit save action (e.g. through the File menu or using the relevant operator from Python). A note on bpy.data , bpy.data.objects , ... \u00b6 We have been using bpy.data.objects in most examples above to access objects in the scene. This is actually not completely clean, as bpy.data.objects holds all objects in the Blender file . Usually, the distinction doesn't matter as you only have one scene, but a Blender file can hold multiple scenes, each with their own set of objects: # A file with two scenes, each with their own set of objects >>> bpy . data . scenes . values () [ bpy . data . scenes [ 'Scene' ], bpy . data . scenes [ 'Scene.001' ]] # Current scene >>> bpy . context . scene bpy . data . scenes [ 'Scene' ] # And its objects >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Bottom cube' ], bpy . data . objects [ 'Top Cube' ]] # <Select different scene> # Different current scene >>> bpy . context . scene bpy . data . scenes [ 'Scene.001' ] # And its objects >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Bottom cube.001' ], bpy . data . objects [ 'Top Cube.001' ]] # All objects in the file >>> bpy . data . objects . values () [ bpy . data . objects [ 'Bottom cube' ], bpy . data . objects [ 'Bottom cube.001' ], bpy . data . objects [ 'Top Cube' ], bpy . data . objects [ 'Top Cube.001' ]] Although objects can also be shared between scenes: # Two scenes >>> bpy . data . scenes . values () [ bpy . data . scenes [ 'Scene' ], bpy . data . scenes [ 'Scene.001' ]] # First scene, cubes are local to scene, torus is shared between scenes >>> bpy . context . scene bpy . data . scenes [ 'Scene' ] >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Torus' ], bpy . data . objects [ 'Bottom cube' ], bpy . data . objects [ 'Top Cube' ]] # Second scene, different cubes, torus is shared >>> bpy . context . scene bpy . data . scenes [ 'Scene.001' ] >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Bottom cube.001' ], bpy . data . objects [ 'Top Cube.001' ], bpy . data . objects [ 'Torus' ]] The point here is that bpy.data.objects , and every other attribute under bpy.data , holds values of the complete Blender file . Per-scene values are available through attributes of a Scene object, e.g. bpy.context.scene.objects . For certain use cases this distinction matters. Parenting \u00b6 An object's parent can be queried or set simply through its parent attribute, which needs to reference another Object (or None ). But when parenting is involved the use of transformation matrices becomes somewhat more complex. Suppose we have two cubes above each other, the top cube transformed to Z=5 and the bottom cube to Z=2: Using the 3D viewport we'll now parent the bottom cube to the top cube ( LMB click bottom cube, Shift-LMB click top cube, Ctrl-P , select Object ) and inspect the values in Python: >>> bpy . data . objects [ 'Bottom cube' ] . parent bpy . data . objects [ 'Top cube' ] # The bottom cube is still located in the scene at Z=2, # even after parenting, as is expected >>> bpy . data . objects [ 'Bottom cube' ] . matrix_world Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 2.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) If an object has a parent its matrix_local attribute will contain the transformation relative to its parent , while matrix_world will contain the resulting net object-to-world transformation. If no parent is set then matrix_local is equal to matrix_world . Let's check the bottom cube's local matrix value: # Correct, it is indeed -3 in Z relative to its parent >>> bpy . data . objects [ 'Bottom cube' ] . matrix_local Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , - 3.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) As already shown above the parent attribute can be used to inspect and control the parenting relationship: >>> bpy . data . objects [ 'Top cube' ] . parent # None >>> bpy . data . objects [ 'Bottom cube' ] . parent bpy . data . objects [ 'Top cube' ] # Remove parent >>> bpy . data . objects [ 'Bottom cube' ] . parent = None At this point the two cubes are no longer parented and are at Z=2 (\"Bottom cube\") and Z=5 (\"Top cube\") in the scene. But when we restore the parenting relationship from Python something funny happens 1 : # Set parent back to what it was >>> bpy . data . objects [ 'Bottom cube' ] . parent = bpy . data . objects [ 'Top cube' ] The reason for the different position of the cube called \"Bottom cube\" (which is now on top) is that when using the UI to set up a parenting relationship it does more than just setting the parent attribute of the child object. There's also something called the parent-inverse matrix. Let's inspect it and the other matrix transforms we've already seen for the current (unexpected) scene: # Identity matrix, i.e. no transform >>> bpy . data . objects [ 'Bottom cube' ] . matrix_parent_inverse Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 0.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # Hmmm, this places the \"Bottom cube\" 2 in Z *above* its parent at Z=5... >>> bpy . data . objects [ 'Bottom cube' ] . matrix_local Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 2.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # ... so it indeed ends up at Z=7 as we saw (above \"Top cube\") >>> bpy . data . objects [ 'Bottom cube' ] . matrix_world Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 7.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) So what happened here? Apparently the matrix_local matrix changed from its value of Z=-3 as we saw earlier. The answer is that when you set up a parenting relationship using the UI the parent-inverse matrix is set to the inverse of the current parent transformation (as the name suggests) while matrix_local is updated to inverse(parent.matrix_world) @ to_become_child.matrix_world . If we clear the parent value from Python and redo the parenting in the UI we can see this in the resulting transform matrices: >>> bpy . data . objects [ 'Bottom cube' ] . parent = None # <parent \"Bottom cube\" to \"Top cube\" in the UI> # Was identity, is now indeed the inverse of transforming +5 in Z >>> bpy . data . objects [ 'Bottom cube' ] . matrix_parent_inverse Matrix ((( 1.0 , - 0.0 , 0.0 , - 0.0 ), ( - 0.0 , 1.0 , - 0.0 , 0.0 ), ( 0.0 , - 0.0 , 1.0 , - 5.0 ), ( - 0.0 , 0.0 , - 0.0 , 1.0 ))) # Was Z=2, is now 2-5 >>> bpy . data . objects [ 'Bottom cube' ] . matrix_local Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , - 3.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # Was Z=7 >>> bpy . data . objects [ 'Bottom cube' ] . matrix_world Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 2.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) The reason for this behaviour is that when doing parenting in the 3D viewport you usually do not want the object that you are setting as the child to move. So the parenting matrices are adjusted accordingly when the parenting relationship is set up. But when we simply set parent from Python, the matrix_local value is used as is, causing our bottom cube to suddenly move up, as it is used as the transform relative to its parent, while it actually would need a different value to stay in place. There's actually quite a bit more going on with all the different parenting options available from the UI. See this page for more details. The same thing happens when setting the parent in the UI using Object Properties > Relations > Parent \u21a9","title":"The Blender API in detail"},{"location":"advanced/python_scripting/api_summary/#the-blender-api-in-detail","text":"","title":"The Blender API in detail"},{"location":"advanced/python_scripting/api_summary/#introduction","text":"The Blender Python API mostly consists of a thin layer on top of the underlying Blender C/C++ data structures and methods. The underlying C/C++ code is used to automatically generate the Python API during the build process of the Blender executable, which means the API is always up-to-date with respect to the underlying code. The API isn't the only part of Blender that uses Python. Large parts of the user interface, the import/export scripts and all addons are written in Python. It is therefore relatively easy to extend Blender with, say, new UI dialogs or an importer. This is one of the strengths of the Blender Python API. Danger Since the API provides access to Blender internals at a very low level you can screw up the Blender state quite easily, causing unexpected behaviour, data corruption or even crashes. In the worst case you can end up with a file that will no longer load in Blender at all, although that's rare. So when working with Python scripting, save your session to file often, preferably in a number of incremental versions, so you can recover or go a step back when needed. In cases where you suspect Blender's internal state has been corrupted you can save the current state to a temporary file, start a second instance of Blender (keeping the first Blender running!) and then open the temporary file in the second instance to help ensure you start from a known-good state. This prevents you from saving correct Blender state and overwriting your last known-good file.","title":"Introduction"},{"location":"advanced/python_scripting/api_summary/#api-modules","text":"The Blender Python API is comprised of several modules, with bpy being the main one. But there's also useful routines in mathutils , bmesh and a few others. Tip The API documentation on these modules can be easily accessed from within Blender using Help > Python API Reference . By default none of the API modules, not even bpy , is loaded in the environment where a script file runs, so you need to import the ones you need explicitly. The Python Console does import quite a few things by defaults and also sets some useful variables, like C to access bpy.context and D to access bpy.data with less typing: PYTHON INTERACTIVE CONSOLE 3.9.4 ( default , Apr 20 2021 , 15 : 51 : 38 ) [ GCC 10.2.0 ] Builtin Modules : bpy , bpy . data , bpy . ops , bpy . props , bpy . types , bpy . context , bpy . utils , bgl , blf , mathutils Convenience Imports : from mathutils import * ; from math import * Convenience Variables : C = bpy . context , D = bpy . data >>> D . objects . values () [ bpy . data . objects [ 'Camera' ], bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Light' ]]","title":"API modules"},{"location":"advanced/python_scripting/api_summary/#developer-settings","text":"When developing Python scripts in Blender it can be useful to enable a few extra settings: The Python Tooltips under Interface > Display > Python Tooltips . When enabled a tooltip will show the corresponding Python command or a path to the data for a UI element. The Developer Extras under Interface > Display > Developer Extras . When enabled this provides multiple things: The 3D viewport overlay for a mesh in edit mode will now have an extra setting Indices to show the low-level indices of selected vertices/edges/faces. This can be very useful when debugging Python code that works on mesh geometry. The right-click menu for a UI item, such as a button or menu entry, will now also contain an entry called Online Python Reference linking to the relevant Python documentation page. It will enable Operator Search , which will add entries to the F3 search menu for operators. These will be listed after the regular menu entries in the search results. It adds a new menu option Help > Operator Cheat Sheet that will create a new text area called OperatorList.txt , which contains all available operators (see below ) and their default parameters. This list can give you a quick overview of the available operators, with the API documentation providing all the details. As mentioned in the video in the introductory chapter the Info area can be useful if you want to inspect which Python calls Blender performs for certain operations. This certainly will not provide all the details in all cases, but can give some insight. You can either switch to the default Scripting workspace to check the Info area, or use the normal UI area operations to add/change an area to an Info area.","title":"Developer settings"},{"location":"advanced/python_scripting/api_summary/#data-blocks","text":"The different types of data in Blender are stored in data-blocks . For example, there's Mesh, Object, Texture and Shader data-blocks, but there's quite a few more . One of the clever bits in the way Blender is programmed is that data-blocks written to file contain enough information about their content (i.e. metadata) to make them readable by both older and newer versions of Blender than the one they were written with. This metadata system also allows the Python API for accessing those data-blocks to get generated without much manual development. Data-blocks are available through Python, per type, under bpy.data , e.g. bpy.data.objects or bpy.data.meshes . The type of a data-block is the corresponding class under bpy.types , e.g. >>> type ( bpy . data . objects [ 'Cube' ]) < class ' bpy_types . Object '> >>> bpy . types . Object < class ' bpy_types . Object '> Each type of data-block has its own set of attributes and methods, particular to that type. Learning the Blender Python API involves getting to know the details of the data-block types you want to work with and how they interact. Blender keeps track of which data-blocks are no longer being referenced to decide when a data-block does not need to be saved (so-called garbage collection). Usually you don't need to explicitly interact with this system, but it is good to be aware that it is there, see this section below for more details.","title":"Data-blocks"},{"location":"advanced/python_scripting/api_summary/#unique-data-block-names","text":"Per type of data all the data-blocks need to have a unique name . This is enforced automatically by Blender when a data-block is created by appending a number to make the name unique. For example: >>> bpy . data . meshes . new ( 'my object' ) bpy . data . meshes [ 'my object' ] >>> bpy . data . meshes . new ( 'my object' ) bpy . data . meshes [ 'my object.001' ] >>> bpy . data . meshes . new ( 'my object' ) bpy . data . meshes [ 'my object.002' ] This usually isn't an issue, but just something to be aware of when working with referencing objects by name, as the name of a data-block you created might actually be something different than you expect.","title":"Unique data-block names"},{"location":"advanced/python_scripting/api_summary/#objects-and-object-data","text":"When we use the word Object in these pages we mean one of the object types that can be present in a 3D scene, e.g. camera, mesh or light. Such objects are of type bpy.types.Object and all have general properties related to their presence in the 3D scene. For example, their name, 3D transformation, visibility flags, parent, etc. But a Light object needs to specify different properties than, say, a Camera object and these per-type properties are stored as object data . The object data can be accessed through the data attribute of an Object: # Both lights and cameras are Objects >>> type ( bpy . data . objects [ 'Light' ]) < class ' bpy_types . Object '> >>> type ( bpy . data . objects [ 'Camera' ]) < class ' bpy_types . Object '> # But their object data are of a different type >>> type ( bpy . data . objects [ 'Camera' ] . data ) < class ' bpy . types . Camera '> >>> type ( bpy . data . objects [ 'Light' ] . data ) < class ' bpy . types . PointLight '> # And have different attributes, relevant to that type >>> dir ( bpy . data . objects [ 'Camera' ] . data ) [ ... , 'angle' , ... , 'clip_start' , ... , 'dof' , ... ] >>> dir ( bpy . data . objects [ 'Light' ] . data ) [ ... , 'color' , ... , 'distance' , 'energy' , ... , 'falloff_type' , ... ]","title":"Objects and object data"},{"location":"advanced/python_scripting/api_summary/#objects-of-a-specific-type","text":"Sometimes you want to iterate over all objects in a scene, but only perform some operation on a specific type of object. You can use the type attribute for checking an object's type: >>> bpy . data . objects [ 'Camera' ] . type 'CAMERA' >>> bpy . data . objects [ 'Light' ] . type 'LIGHT'","title":"Objects of a specific type"},{"location":"advanced/python_scripting/api_summary/#native-blender-data-structures","text":"When working with the Python API will you frequently use internal Blender types that appear similar to regular Python types, like lists and dictionaries. However, the Blender types are not real native Python types and behave differently in certain aspects. For example, the different collections of scene elements (such as objects or meshes) that are available under bpy.data are of type bpy_prop_collection . This type is a combination of a Python list and a dictionary, sometimes called an ordered dictionary, as it allows indexing by both array position and key: >>> type ( bpy . data . objects ) < class ' bpy_prop_collection '> # Some of its methods match those of native Python data types >>> dir ( bpy . data . objects ) [ '__bool__' , '__contains__' , '__delattr__' , '__delitem__' , '__doc__' , '__doc__' , '__getattribute__' , '__getitem__' , '__iter__' , '__len__' , '__module__' , '__setattr__' , '__setitem__' , '__slots__' , 'bl_rna' , 'find' , 'foreach_get' , 'foreach_set' , 'get' , 'items' , 'keys' , 'new' , 'remove' , 'rna_type' , 'tag' , 'values' ] # Index by position >>> bpy . data . objects [ 0 ] bpy . data . objects [ 'Camera' ] # Index by key >>> bpy . data . objects [ 'Camera' ] bpy . data . objects [ 'Camera' ] # (key, value) pairs >>> bpy . data . objects . items () [( 'Camera' , bpy . data . objects [ 'Camera' ]), ( 'Cube' , bpy . data . objects [ 'Cube' ]), ( 'Light' , bpy . data . objects [ 'Light' ])] Note that the position of an item in the collection, and hence its index, can change during a Blender session.","title":"Native Blender data structures"},{"location":"advanced/python_scripting/api_summary/#inspecting-values","text":"One of the more annoying aspects when working in the Blender Python Console inspecting these kinds of values is that the elements in a bpy_prop_collection (or other Blender types) aren't printed by default, this in contrast to a regular Python dictionary. You need to, for example, cast to a list or call its values() method: # Regular Python dict, prints both keys and values >>> d = dict ( a = 1 , b = 2 , c = 3 ) >>> d { 'a' : 1 , 'b' : 2 , 'c' : 3 } # No items printed >>> bpy . data . objects < bpy_collection [ 3 ], BlendDataObjects > # values() returns a list, so gets printed in detail >>> type ( bpy . data . objects . values ()) < class ' list '> >>> bpy . data . objects . values () [ bpy . data . objects [ 'Camera' ], bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Light' ]] # Difference in list() result: >>> list ( d ) [ 'a' , 'b' , 'c' ] # Returns dict *keys* >>> list ( bpy . data . objects ) [ bpy . data . objects [ 'Camera' ], bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Light' ]] # Returns collection *values* The choice for not printing the values inside a bpy_prop_collection is (most likely) that in many cases the collection will contain large numbers of objects, so printing them all would not be too useful, or might even make the UI non-responsive for a short time.","title":"Inspecting values"},{"location":"advanced/python_scripting/api_summary/#data-organization","text":"In certain cases Blender uses a more elaborate data structure in cases where you might except low-level values, like lists. For example, the set of vertices that make up a mesh are only accessible as a collection of MeshVertex objects: >>> m bpy . data . meshes [ 'Cube' ] >>> type ( m . vertices ) < class ' bpy_prop_collection '> >>> len ( m . vertices ) 8 >>> m . vertices [ 0 ] bpy . data . meshes [ 'Cube' ] . vertices [ 0 ] >>> type ( m . vertices [ 0 ]) < class ' bpy . types . MeshVertex '> >>> dir ( m . vertices [ 0 ]) [ '__doc__' , '__module__' , '__slots__' , 'bevel_weight' , 'bl_rna' , 'co' , 'groups' , 'hide' , 'index' , 'normal' , 'rna_type' , 'select' , 'undeformed_co' ] # Vertex coordinate (object space) >>> m . vertices [ 0 ] . co Vector (( 1.0 , 1.0 , 1.0 )) # Vertex normal >>> m . vertices [ 0 ] . normal Vector (( 0.5773491859436035 , 0.5773491859436035 , 0.5773491859436035 )) The reason for this is that there's several types of data associated with a single vertex, which are all centralized in a MeshVertex object. In short, Blender uses a so-called array-of-structs design. The alternative design choice would have been to have separate arrays for vertex coordinates, vertex normals, etc (which would be a struct-of-arrays design).","title":"Data organization"},{"location":"advanced/python_scripting/api_summary/#vertices-and-matrices","text":"The example above also shows that even a vertex coordinate is not accessed as a low-level Python data type, like a tuple, but by the Vector type (which is in the mathutils module). This has the advantage of providing many useful methods for operating on vector values: >>> v = m . vertices [ 0 ] . normal >>> v Vector (( 0.5773491859436035 , 0.5773491859436035 , 0.5773491859436035 )) >>> v . length 0.999998137353116 # Return a new vector that's orthogonal >>> w = v . orthogonal () >>> w Vector (( 0.5773491859436035 , 0.5773491859436035 , - 1.154698371887207 )) # Dot product (should be zero as v and w are orthogonal) >>> v . dot ( w ) 0.0 # Note: v*w is element-wise product, not dot product! >>> v * w Vector (( 0.3333320915699005 , 0.3333320915699005 , - 0.666664183139801 )) # Cross product between two vectors >>> v . cross ( w ) Vector (( - 0.9999963045120239 , 0.9999963045120239 , 0.0 )) # Swizzling (returning vector elements in a different order) >>> w Vector (( 0.5773491859436035 , 0.5773491859436035 , - 1.154698371887207 )) >>> w . zxy Vector (( - 1.154698371887207 , 0.5773491859436035 , 0.5773491859436035 )) The builtin mathutils module contains many useful data types and methods for working with 3D data, including vectors and matrices, but also different methods for working with transformations (like quaternion) and colors spaces. # Transformation matrix for an object with uniform scale 2 and # translation in Z of 3. These values will match with the Transform UI area >>> o bpy . data . objects [ 'Cube' ] >>> o . matrix_world Matrix ((( 2.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 2.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 2.0 , 3.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # Create a rotation matrix >>> m = Matrix . Rotation ( radians ( 90.0 ), 4 , 'X' ) >>> m Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 7.549790126404332e-08 , - 1.0 , 0.0 ), ( 0.0 , 1.0 , 7.549790126404332e-08 , 0.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) >>> v = Vector (( 1 , 2 , 3 )) # Transform the vector using the matrix. Note the different outcomes # depending on the multiplication order. >>> m @ v Vector (( 1.0 , - 2.999999761581421 , 2.000000238418579 )) >>> v @ m Vector (( 1.0 , 3.000000238418579 , - 1.999999761581421 )) # Also, a 3-vector is assumed to have a fourth element equal to *one* when # multiplying with a matrix: >>> m = Matrix . Translation (( 4 , 5 , 6 )) >>> m Matrix ((( 1.0 , 0.0 , 0.0 , 4.0 ), ( 0.0 , 1.0 , 0.0 , 5.0 ), ( 0.0 , 0.0 , 1.0 , 6.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) >>> m @ Vector (( 1 , 2 , 3 )) Vector (( 5.0 , 7.0 , 9.0 )) >>> m @ Vector (( 1 , 2 , 3 , 0 )) Vector (( 1.0 , 2.0 , 3.0 , 0.0 ))","title":"Vertices and matrices"},{"location":"advanced/python_scripting/api_summary/#selections","text":"In a lot of cases you want to operate on a set of objects. You can access (read only) the current selection with bpy.context.selected_objects : >>> bpy . context . selected_objects [ bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Plane' ]] Changing the current selection can be done in several ways. Control over selection state per object can be controlled with the select_get() and select_set() methods: >>> bpy . context . selected_objects [] >>> bpy . data . objects [ 'Camera' ] . select_get () False >>> bpy . data . objects [ 'Camera' ] . select_set ( True ) >>> bpy . context . selected_objects [ bpy . data . objects [ 'Camera' ]] The full selection set can also be changed: # Select all visible objects >>> bpy . ops . object . select_all ( action = 'SELECT' ) # Deselect all objects >>> bpy . ops . object . select_all ( action = 'DESELECT' ) # Toggle the selection state for each object >>> bpy . ops . object . select_all ( action = 'TOGGLE' ) Note that the default mode for bpy.ops.object.select_all() when not specified is TOGGLE . Also note that the selection methods above operate only on objects that are currently visible objects in the scene (in terms of the outliner eye icon), just like for the selection hotkeys (like A ) in the 3D viewport.","title":"Selections"},{"location":"advanced/python_scripting/api_summary/#often-used-values-and-operations","text":"Here, we list some frequently used parts of the API, for varying types of data.","title":"Often used values and operations"},{"location":"advanced/python_scripting/api_summary/#scene","text":"Current scene: bpy.context.scene (read-only)","title":"Scene"},{"location":"advanced/python_scripting/api_summary/#objects","text":"Active object: bpy.context.active_object (read-only) Selected objects: bpy.context.selected_objects (read-only) Delete selected objects: bpy.ops.object.delete()","title":"Objects"},{"location":"advanced/python_scripting/api_summary/#camera","text":"Active camera object: Scene.camera (this is the camera object , not camera object data ) Type: Camera.type (\"PERSP\", \"ORTHO\", ...) Focal length: Camera.lens (in mm) Clipping distances: Camera.clip_start , Camera.clip_end","title":"Camera"},{"location":"advanced/python_scripting/api_summary/#rendering","text":"Image resolution: Width: Scene.render.settings.resolution_x Height: Scene.render.settings.resolution_y Percentage: Scene.render.settings.resolution_percentage Output file: Scene.render.filepath Image output type: Scene.render.image_settings.file_format (\"PNG\", \"JPEG\", ...) Number of samples per pixel (Cycles): Scene.cycles.samples Render current scene: bpy.ops.render.render() . See parameters how to control the specific type of render (still image versus animation) and whether to save output","title":"Rendering"},{"location":"advanced/python_scripting/api_summary/#animation","text":"Current frame Scene.frame_current Frame range: Scene.frame_start , Scene.frame_end Frame rate: Scene.render.fps","title":"Animation"},{"location":"advanced/python_scripting/api_summary/#file-io","text":"Save the current session to a specific file: bpy.ops.wm.save_as_mainfile() Open Blend file bpy.ops.wm.open_mainfile() Import a file (call depends on file type): bpy.ops.import_scene.obj() (OBJ scene), bpy.ops.import_scene.gltf (glTF scene), bpy.ops.import_mesh.ply (PLY mesh), etc. See here and here Export a file (call depends on file type) follows the same call names, see here and here","title":"File I/O"},{"location":"advanced/python_scripting/api_summary/#object-transformations","text":"The matrix_world attribute of an Object contains the object-to-world transform that places the object in the 3D scene: >>> o = bpy . context . active_object >>> o bpy . data . objects [ 'Cube' ] >>> o . matrix_world Matrix ((( 1.3376139402389526 , 0.0 , 0.0 , 0.3065159320831299 ), ( 0.0 , 1.3376139402389526 , 0.0 , 2.2441697120666504 ), ( 0.0 , 0.0 , 1.3376139402389526 , 1.2577730417251587 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) Comparing this matrix with the values set in the Transform panel, you can see the location is stored in the right-most column of the matrix and the scaling along the diagonal. If there was a rotation set on this object some of these values would not be as recognizable anymore. The location, rotation and scale values can also be inspected and set separately: >>> o . location Vector (( 0.3065159320831299 , 2.2441697120666504 , 1.2577730417251587 )) >>> o . rotation_euler Euler (( 0.0 , 0.0 , 0.0 ), 'XYZ' ) >>> o . scale Vector (( 1.3376139402389526 , 1.3376139402389526 , 1.3376139402389526 )) >>> o . location = ( 1 , 2 , 3 ) # Needs value in radians >>> o . rotation_euler . x = radians ( 45 ) >>> o . scale = ( 2 , 1 , 1 ) >>> o . matrix_world Matrix ((( 2.0 , 0.0 , 0.0 , 1.0 ), ( 0.0 , 0.7071067690849304 , - 0.7071067690849304 , 2.0 ), ( 0.0 , 0.7071067690849304 , 0.7071067690849304 , 3.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) See the section on parenting below for some subtle effects on transformations in cases object parenting is used.","title":"Object transformations"},{"location":"advanced/python_scripting/api_summary/#geometry-coordinates","text":"Mesh geometry in Blender stores vertex coordinates (and other geometric information) in object-space coordinates. But a mesh (or object in general) will usually get transformed to a specific position, scaling and orientation in the scene. As described above the net transform from object-space to world-space coordinates, also called the object-to-world transform, is available through matrix_world . In cases where you need to have access to geometric data in world-space , say vertex coordinates, you need to apply the matrix_world transform manually. For example, given the cube transformed as shown above, with vertex 7 selected (visible bottom-left in the image below): >>> o bpy . data . objects [ 'Cube' ] >>> m = o . data >>> o . matrix_world Matrix ((( 1.3376139402389526 , 0.0 , 0.0 , 0.3065159320831299 ), ( 0.0 , 1.3376139402389526 , 0.0 , 2.2441697120666504 ), ( 0.0 , 0.0 , 1.3376139402389526 , 1.2577730417251587 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # The object-space coordinate of this vertex >>> m . vertices [ 7 ] . co Vector (( - 1.0 , - 1.0 , - 1.0 )) # The world-space coordinate of this vertex, which matches # what the Transform UI shows. Note the Global display mode # select in the UI, if we select Local if will show (-1, -1, -1). >>> o . matrix_world @ m . vertices [ 7 ] . co Vector (( - 1.0310980081558228 , 0.9065557718276978 , - 0.07984089851379395 ))","title":"Geometry coordinates"},{"location":"advanced/python_scripting/api_summary/#api-quirks","text":"Working with the Blender Python API has some peculiarities compared to your average Python scripting. These have to do with the way the API is structured, but also how it interacts with the Blender internals. The API manual contains a lengthy page on some gotchas, but here we list some of the common ones.","title":"API quirks"},{"location":"advanced/python_scripting/api_summary/#object-modes","text":"An object is always in one of several modes. These modes are the same ones you work with in the UI: Object mode, Edit mode, etc. The current mode for an object can be retrieved through the mode property: >>> o = bpy . data . objects [ 'Cube' ] >>> o . mode 'OBJECT' # <enter edit mode with TAB> >>> o . mode 'EDIT' Depending on the current mode of a mesh object certain data might not be up-to-date or even unavailable when accessing it through the Python API, this is especially true when an object is in Edit Mode . This is because the edit mode uses its own copy of the data to let you edit, which is synced with the underlying mesh data when going in and out of edit mode. See here for the relevant section in the Blender API docs. An example continuing with the Cube mesh above: >>> o . mode 'OBJECT' >>> m = o . data >>> m bpy . data . meshes [ 'Cube' ] # Check UV map data >>> len ( m . uv_layers [ 0 ] . data ) 24 # <enter edit mode with TAB> >>> o . mode 'EDIT' # UV map data now empty... >>> len ( m . uv_layers [ 0 ] . data ) 0 In most cases when working on low-level data such as mesh geometry you want the object to be in object mode (or use the bmesh module when you need the object be in edit mode). It's usually a good idea to add a check at the top of your script to verify the current mode is what you expect: o = bpy . context . active_object if o . mode != 'OBJECT' : raise ValueError ( 'Active object needs to be in object mode!' ) There are alternatives for still allowing a mesh to be in edit-mode when accessing its data from a script, see the API docs for details.","title":"Object modes"},{"location":"advanced/python_scripting/api_summary/#interrupting-long-running-scripts","text":"During script development you might get in a situation where your code is stuck in a loop, or takes much longer than you like. Interrupting a running script can usually be done by pressing Ctrl-C in the terminal console window : >>> while True : ... pass ... # Uh oh, execution stuck in a loop and the UI will now have become unresponsive # Pressing Ctrl-C in the terminal console window interrupts script execution, # as it raises a KeyboardInterrupt Traceback ( most recent call last ): File \"<blender_console>\" , line 2 , in < module > KeyboardInterrupt","title":"Interrupting (long-running) scripts"},{"location":"advanced/python_scripting/api_summary/#interaction-with-the-undo-system","text":"When you undo certain operations Blender might re-create certain data, which might cause existing references to the original data to become invalid. This can be especially noticeable when working interactively in the Python Console. For example, with a cube object as active object in the 3D viewport: # The Cube is the active object >>> bpy . context . active_object bpy . data . objects [ 'Cube' ] # Save a reference to it >>> o = bpy . context . active_object # <Grab the object in the 3D viewport and move it somewhere else> # Object reference still valid >>> o bpy . data . objects [ 'Cube' ] # <Undo the object translation in the 3D viewport> # Object reference has now become invalid >>> o < bpy_struct , Object invalid > # Reason: object referenced under name 'Cube' has changed >>> bpy . data . objects [ 'Cube' ] == o False >>> id ( o ) 140543077302976 >>> id ( bpy . data . objects [ 'Cube' ]) 140543077308608 # Will need to reacquire active object, or consistently use bpy.data.objects['Cube'] >>> o = bpy . context . active_object >>> o bpy . data . objects [ 'Cube' ]","title":"Interaction with the Undo system"},{"location":"advanced/python_scripting/api_summary/#operators","text":"A special class of important API routines are the so-called operators . These are usually higher-level operations, such as adding a new cube mesh, deleting the current set of selected objects or running a file importer. As noted above many parts of the Blender UI are set up with Python scripts and in a lot of cases the operations you perform in the UI through menu actions or shortcut keys will simply call the relevant operator from Python to do the actual work. The Info area will show most operators as they get executed, but you can also check what API call is made for a certain UI element (this requires Python Tooltips to be enabled, see above ). For example, adding a plane mesh through the Add menu will call the operator bpy.ops.mesh.primitive_plane_add() , as the tooltip shows: You can simply call the operator directly from Python to add a plane in exactly the same way as with the menu option: >>> bpy . data . objects . values () [] >>> bpy . ops . mesh . primitive_plane_add () { 'FINISHED' } # A plane mesh is now added to the scene >>> bpy . data . objects . values () [ bpy . data . objects [ 'Plane' ]] Many of the operators take parameters, to influence the results. For example, with bpy.ops.mesh.primitive_plane_add() you can set the initial size and location of the plane (see the API docs for all the parameters): >>> bpy . ops . mesh . primitive_plane_add ( size = 3 , location = ( 1 , 2 , 3 )) { 'FINISHED' } Info Note that operator parameters can only be passed using keyword arguments .","title":"Operators"},{"location":"advanced/python_scripting/api_summary/#operator-context","text":"This is all very nice and powerful, but operators have a few inherent properties that can make them tricky to work with. An operator's execution crucially depends on the context in which it is called, where it gets most of the data it needs. As shown above simple parameter values can usually be passed, but values like the object(s) to operate on are retrieved implicitly. For example, to join a set of mesh objects into a single mesh you can call the operator bpy.ops.object.join() . But the current context needs to be correctly set for the operator to work: # We have no objects selected >>> bpy . context . selected_objects [] >>> bpy . ops . object . join () Warning : Active object is not a selected mesh { 'CANCELLED' } # With 3 objects selected >>> bpy . context . selected_objects [ bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Cube.001' ], bpy . data . objects [ 'Cube.002' ]] # Now it works >>> bpy . ops . object . join () { 'FINISHED' } As can be seen above an operator only returns a value indicating the execution status. When calling the operator in the Python Console as above some extra info is printed. But when calling operators from scripts the status return value is all you have to go on, as the extra message isn't printed when the script is executed. And in some cases the reason an operator fails can be quite unclear: >>> bpy . context . selected_objects [ bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Camera' ]] >>> bpy . ops . mesh . intersect_boolean () Traceback ( most recent call last ): File \"<blender_console>\" , line 1 , in < module > File \"/usr/share/blender/2.92/scripts/modules/bpy/ops.py\" , line 132 , in __call__ ret = _op_call ( self . idname_py (), None , kw ) RuntimeError : Operator bpy . ops . mesh . intersect_boolean . poll () failed , context is incorrect This merely shows that the so-called poll function failed. The poll function is used by operators to determine if they can execute in the current context, by checking certain preconditions on things like the selected object(s), the type of data or an object mode. In this case the bpy.ops.mesh.intersect_boolean() operator doesn't perform a boolean intersection on multiple meshes, but only on the faces of a single object in edit mode, but you can't tell from the error message (nor does the documentation make that clear): To actually perform a boolean intersection on two objects from a Python script requires us to do what we would be do in the UI: add a Boolean modifier on one of the objects and set its parameters. We could take advantage of the Python Tooltips to see which operator we need: This would suggest that using bpy.ops.modifier_add(type='BOOLEAN') would be what we need, but then setting the required parameters on the modifier (i.e. the object to subtract) would become tricky. So for a boolean operation, and setting object modifiers in general, there's an easier way: >>> o = bpy . data . objects [ 'Cube' ] # Add a modifier on the object and set its parameters >>> mod = o . modifiers . new ( name = 'boolmod' , type = 'BOOLEAN' ) >>> mod . object = bpy . data . objects [ 'Cube.001' ] >>> mod . operation = 'DIFFERENCE' # At this point the modifier is all set up. We hide # the object we subtract to make the boolean result visible. >>> bpy . data . objects [ 'Cube.001' ] . hide_viewport = True Unfortunately, certain operations can only be performed by calling operators. So there's a good chance that you will need to use them at some point when doing Python scripting. Hopefully this section gives some clues as how to work with them. See this section for more details on all the above subtleties and issues relating to working with operators. The bpy.ops documentation also contains useful information on operators, including how to override an operator's implicit context with values you set yourself.","title":"Operator context"},{"location":"advanced/python_scripting/api_summary/#meshes","text":"One of the more common scene data types to work with from Python are 3D meshes. Meshes in Blender can contain polygons of an arbitrary number of vertices (so-called N-gons), can contain wire edges and support extra layers of data, such as vertex colors and UV coordinates. We go into a fair amount of detail on how to create and access mesh data, in several ways. As usual, the Blender API docs on the Mesh type contain many more details, but we feel the discussion below is a good summary to get you started for many use cases.","title":"Meshes"},{"location":"advanced/python_scripting/api_summary/#creating-a-mesh-high-level","text":"As shown earlier the Mesh.from_pydata(vertices, edges, faces) method allows a simple and high-level way of creating a mesh. This method doesn't offer full control over the created mesh and isn't very fast for large meshes, but it can be good enough in a lot of cases. It takes three lists of values, or actually, any Python iterable that matches the expected form: vertices: a sequence of float triples, e.g. [(1.0, 2.0, 3.0), (4, 5, 6), ...] edges: a sequence of integer pairs (vertex indices), that define edges by. If [] is passed edges are inferred from polygons faces: a sequence of one or more polygons, each defined as a sequence of 3 or more vertex indices. E.g. [(0, 1, 2), (1, 2, 3, 4), ...] Info The choice of how the mesh data is passed might incur an overhead in memory usage and processing time, especially when regular Python data structures, like lists, are used. An alternative would be to pass NumPy arrays. For the examples below we assume that no explicit list of edges is passed. Edges will then be created implicitly based on the polygons specified, which is usually what is preferred. We discuss explicitly specifying edges below . An example of creating a simple mesh: # Create a mesh consisting of 3 polygons using 6 vertices vertices = [ ( 0 , 0 , 0 ), ( 2 , 0 , 0 ), ( 2 , 2 , 0.2 ), ( 0 , 2 , 0.2 ), ( 1 , 3 , 1 ), ( 1 , - 1 , - 1 ), ] polygons = [ ( 0 , 1 , 2 , 3 ), # Quad ( 4 , 3 , 2 ), # Triangle ( 0 , 5 , 1 ) # Triangle ] m = bpy . data . meshes . new ( name = 'my mesh' ) m . from_pydata ( vertices , [], polygons ) At this point we have created a new Mesh Python object, which corresponds to Object Data of type Mesh. Object Data cannot be directly added to a scene, but needs to be referenced by a 3D Object: # Create an object referencing the mesh data o = bpy . data . objects . new ( name = 'my mesh' , object_data = m ) # Add the object to the scene bpy . context . scene . collection . objects . link ( o ) The resulting mesh and outliner entry looks like this:","title":"Creating a mesh (high-level)"},{"location":"advanced/python_scripting/api_summary/#careful-invalid-data","text":"Note that it is possible to set up a mesh with invalid/inconsistent data when setting the underlying arrays manually, as is the case here. This can cause weird behaviour or even crashes. For example: # 3 vertices vertices = [ ( 0 , 0 , 0 ), ( 1 , 1 , 1 ), ( - 1 , 2 , - 1 ) ] # Invalid vertex index 3 used! polygons = [ ( 0 , 1 , 2 , 3 ) ] m = bpy . data . meshes . new ( name = 'my invalid mesh' ) m . from_pydata ( vertices , [], polygons ) o = bpy . data . objects . new ( name = 'my invalid mesh' , object_data = m ) bpy . context . scene . collection . objects . link ( o ) When executing the above code a new mesh is added to the scene, but it will show as a triangle in the 3D viewport, instead of a quad. And even though that doesn't appear to be unreasonable behaviour in this case Blender will crash if we subsequently enter edit mode on the mesh! So the lesson here is to be careful when specifying geometry using these low-level API calls. This actually applies to all parts of the Blender Python API in general. In this case, to make sure a created mesh has valid data we can use the validate() method on a Mesh . This will check the mesh data and remove any invalid values, e.g. by deleting the polygon using non-existent vertex index 3 above. This might not result in a mesh that matches what you want based on the data, but at least you can detect this situation and handle it without Blender crashing. The validate() method has two issues to be aware of: The method returns True in case the mesh does not validate, i.e. when it has issues. More specifically, it returns True when changes were made to the mesh data to remove invalid values. It will only report on the specific issues found when called with validate(verbose=True) and then will only output to the console. But it is still a good idea to always validate a mesh when creating it manually: ... m = bpy . data . meshes . new ( name = 'my invalid mesh' ) m . from_pydata ( vertices , [], polygons ) if m . validate ( verbose = True ): print ( 'Mesh had issues and has been altered! See console output for details' ) In the example of the invalid mesh data above this results in these message being printed in the console output: ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:351 BKE_mesh_validate_arrays: Edge 0: v2 index out of range, 3 ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:351 BKE_mesh_validate_arrays: Edge 3: v2 index out of range, 3 ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:605 BKE_mesh_validate_arrays: Loop 3 has invalid vert reference (3) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:782 BKE_mesh_validate_arrays: Loop 0 is unused. ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:782 BKE_mesh_validate_arrays: Loop 1 is unused. ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:782 BKE_mesh_validate_arrays: Loop 2 is unused. ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:782 BKE_mesh_validate_arrays: Loop 3 is unused. After validate() returns we can see in this case that invalid data was indeed removed: >>> vertices = [ ( 0 , 0 , 0 ), ( 1 , 1 , 1 ), ( - 1 , 2 , - 1 ) ] >>> polygons = [ ( 0 , 1 , 2 , 3 ) ] >>> m = bpy . data . meshes . new ( name = 'my invalid mesh' ) >>> m . from_pydata ( vertices , [], polygons ) >>> len ( m . polygons ) 1 >>> len ( m . edges ) 4 >>> len ( m . vertices ) 3 >>> m . validate () True >>> len ( m . polygons ) 0 >>> len ( m . edges ) 2 >>> len ( m . vertices ) 3","title":"Careful: invalid data"},{"location":"advanced/python_scripting/api_summary/#creating-a-mesh-low-level","text":"A second, and more flexible, way of creating a mesh is using low-level calls for setting the necessary data arrays directly on a Mesh object. This is especially useful in combination with NumPy arrays, as this allows the creation of large meshes with relatively high performance and low memory overhead. Meshes in Blender are stored using 4 arrays, as attributes of the bpy.types.Mesh type: vertices : vertex locations, each specified by 3 floats loops : contains the vertex indices used for defining polygons of a mesh, each polygon as a sequence of indices in the vertices array polygons : defines the start index of each polygon as an index in loops , plus the length of each polygon in number of vertices edges : defines the edges of the mesh, using two vertex indices per edge So to create a mesh at this level we need to set up the necessary values for these arrays. Here, we create the same mesh as in the previous section, using NumPy arrays for storing the data. # Vertices (8): x1 y1 z1 x2 y2 z2 ... vertices = numpy . array ([ 0 , 0 , 0 , 2 , 0 , 0 , 2 , 2 , 0.2 , 0 , 2 , 0.2 , 1 , 3 , 1 , 1 , - 1 , - 1 , 0 , - 2 , - 1 , 2 , - 2 , - 1 ], dtype = numpy . float32 ) # # Polygons, defined in loops # # List of vertex indices of all loops combined vertex_index = numpy . array ([ 0 , 1 , 2 , 3 , # Quad 4 , 3 , 2 , # Triangle 0 , 5 , 1 # Triangle ], dtype = numpy . int32 ) # For each polygon the start of its indices in vertex_index loop_start = numpy . array ([ 0 , 4 , 7 ], dtype = numpy . int32 ) # Length of each polygon in number of vertices loop_total = numpy . array ([ 4 , 3 , 3 ], dtype = numpy . int32 ) We additionally also specify texture coordinates and vertex colors. This is something that is not possible with the high-level from_pydata() API shown above. Note that we need to specify these values per vertex per polygon loop . # Texture coordinates per vertex per polygon loop uv_coordinates = numpy . array ([ 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , # Quad 0.5 , 1 , 0 , 0 , 1 , 0 , # Triangle 0 , 1 , 0.5 , 0 , 1 , 1 # Triangle ], dtype = numpy . float32 ) # Vertex color (RGBA) per vertex per polygon loop vertex_colors = numpy . array ([ 1 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 1 , 1 , ], dtype = numpy . float32 ) Next, we create a new mesh using the above arrays: num_vertices = vertices . shape [ 0 ] // 3 num_vertex_indices = vertex_index . shape [ 0 ] num_loops = loop_start . shape [ 0 ] m = bpy . data . meshes . new ( name = 'my detailed mesh' ) # Vertices m . vertices . add ( num_vertices ) m . vertices . foreach_set ( 'co' , vertices ) # Polygons m . loops . add ( num_vertex_indices ) m . loops . foreach_set ( 'vertex_index' , vertex_index ) m . polygons . add ( num_loops ) m . polygons . foreach_set ( 'loop_start' , loop_start ) m . polygons . foreach_set ( 'loop_total' , loop_total ) # Create UV coordinate layer and set values uv_layer = m . uv_layers . new ( name = 'default' ) uv_layer . data . foreach_set ( 'uv' , uv_coordinates ) # Create vertex color layer and set values vcol_layer = m . vertex_colors . new () vcol_layer . data . foreach_set ( 'color' , vertex_colors ) # Done, update mesh object m . update () # Validate mesh if m . validate ( verbose = True ): print ( 'Mesh data did not validate!' ) # Create an object referencing the mesh data o = bpy . data . objects . new ( name = 'my detailed mesh' , object_data = m ) # Add the object to the scene bpy . context . scene . collection . objects . link ( o ) Info Passing a multi-dimensional NumPy array directly to foreach_set() will not work: >>> vertices = numpy . array ([ ... ( 0 , 0 , 0 ), ( 2 , 0 , 0 ), ( 2 , 2 , 0.2 ), ( 0 , 2 , 0.2 ), ... ( 1 , 3 , 1 ), ( 1 , - 1 , - 1 ), ( 0 , - 2 , - 1 ), ( 2 , - 2 , - 1 ) ... ], 'float32' ) >>> vertices . shape ( 8 , 3 ) >>> m = bpy . data . meshes . new ( name = 'my detailed mesh' ) >>> m . vertices . foreach_set ( 'co' , vertices ) Traceback ( most recent call last ): File \"<blender_console>\" , line 1 , in < module > RuntimeError : internal error setting the array However, passing a flattened array does work: >>> m . vertices . foreach_set ( 'co' , vertices . flatten ()) >>> [ v . co for v in mesh . vertices ] [ Vector (( 0.0 , 0.0 , 0.0 )), Vector (( 2.0 , 0.0 , 0.0 )), Vector (( 2.0 , 2.0 , 0.20000000298023224 )), Vector (( 0.0 , 2.0 , 0.20000000298023224 )), Vector (( 1.0 , 3.0 , 1.0 )), Vector (( 1.0 , - 1.0 , - 1.0 )), Vector (( 0.0 , - 2.0 , - 1.0 )), Vector (( 2.0 , - 2.0 , - 1.0 ))]","title":"Creating a mesh (low-level)"},{"location":"advanced/python_scripting/api_summary/#specifying-edges-when-creating-a-mesh","text":"In most cases we want to create a mesh consisting of only polygons and in that case don't need to specify edges. For certain mesh objects it can be of interest to also be able to specify edges explicitly, or even to create a mesh that consists only of vertices and edges between them. Edges can be used to add line segments that are not part of polygons. We build upon the example mesh we created above by adding a set of 3 edges: # Create a mesh consisting of 3 polygons using 8 vertices, with 3 extra edges # that are not part of the polygons vertices = [ ( 0 , 0 , 0 ), ( 2 , 0 , 0 ), ( 2 , 2 , 0.2 ), ( 0 , 2 , 0.2 ), ( 1 , 3 , 1 ), ( 1 , - 1 , - 1 ), ( 0 , - 2 , - 1 ), ( 2 , - 2 , - 1 ) ] edges = [ ( 5 , 6 ), ( 6 , 7 ), ( 5 , 7 ) ] polygons = [ ( 0 , 1 , 2 , 3 ), # Quad ( 4 , 3 , 2 ), # Triangle ( 0 , 5 , 1 ) # Triangle ] m = bpy . data . meshes . new ( name = 'my mesh with edges' ) m . from_pydata ( vertices , edges , polygons ) o = bpy . data . objects . new ( name = 'my mesh with edges' , object_data = m ) bpy . context . scene . collection . objects . link ( o ) The resulting mesh and outliner entry looks like this: Note that even though we specified only 3 edges explicitly the polygons in the mesh implicitly define 8 more. These are the edges making up those polygons, with shared edges being present only once. In total this results in 11 edges in the mesh: >>> len ( m . edges ) 11 For the second, low-level, method of mesh creation edges are handled slightly different. Edges can be set explicitly by using Mesh.edges : # Vertices (8): x1 y1 z1 x2 y2 z2 ... vertices = numpy . array ([ 0 , 0 , 0 , 2 , 0 , 0 , 2 , 2 , 0.2 , 0 , 2 , 0.2 , 1 , 3 , 1 , 1 , - 1 , - 1 , 0 , - 2 , - 1 , 2 , - 2 , - 1 ], dtype = numpy . float32 ) # Extra edges (3) not defined implicitly by polygons edges = numpy . array ([ 5 , 6 , 6 , 7 , 5 , 7 ], dtype = numpy . int32 ) # # Polygons, defined in loops # # List of vertex indices of all loops combined vertex_index = numpy . array ([ 0 , 1 , 2 , 3 , # Quad 4 , 3 , 2 , # Triangle 0 , 5 , 1 # Triangle ], dtype = numpy . int32 ) # For each polygon the start of its indices in vertex_index loop_start = numpy . array ([ 0 , 4 , 7 ], dtype = numpy . int32 ) # Length of each polygon in number of vertices loop_total = numpy . array ([ 4 , 3 , 3 ], dtype = numpy . int32 ) num_vertices = vertices . shape [ 0 ] // 3 num_edges = edges . shape [ 0 ] // 2 num_vertex_indices = vertex_index . shape [ 0 ] num_loops = loop_start . shape [ 0 ] m = bpy . data . meshes . new ( name = 'detailed mesh with edges' ) # Vertices m . vertices . add ( num_vertices ) m . vertices . foreach_set ( 'co' , vertices ) # Edges m . edges . add ( num_edges ) m . edges . foreach_set ( 'vertices' , edges ) # Polygons m . loops . add ( num_vertex_indices ) m . loops . foreach_set ( 'vertex_index' , vertex_index ) m . polygons . add ( num_loops ) m . polygons . foreach_set ( 'loop_start' , loop_start ) m . polygons . foreach_set ( 'loop_total' , loop_total ) # Done, update mesh object m . update () # Validate mesh if m . validate ( verbose = True ): print ( 'Mesh data did not validate!' ) Here, we only specify the extra edges and not the polygon edges. But when we try to validate the mesh errors will be reported: ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 0 needs missing edge (0, 1) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 0 needs missing edge (1, 2) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 0 needs missing edge (2, 3) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 0 needs missing edge (3, 0) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 1 needs missing edge (4, 3) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 1 needs missing edge (3, 2) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 1 needs missing edge (2, 4) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 2 needs missing edge (0, 5) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 2 needs missing edge (5, 1) ERROR (bke.mesh): ../source/blender/blenkernel/intern/mesh_validate.c:628 BKE_mesh_validate_arrays: Poly 2 needs missing edge (1, 0) So the polygon edges, which we did not specify, are being reported. In this case the validate() method will correct this and add the missing edges. But having errors reported for regular polygon edges makes it harder to detect any other issues with the mesh data. So the Mesh.update() method provides the option calc_edges . By default this option is False , but when set to True all edges in the mesh will be recalculated to be consistent with the available vertex indices, polygons and extra edges set. ... # Done, update mesh object and recalculate edges m . update ( calc_edges = True ) Validation now succeeds: >>> m . validate ( verbose = True ) False","title":"Specifying edges when creating a mesh"},{"location":"advanced/python_scripting/api_summary/#accessing-mesh-data-object-mode","text":"Inspecting or using mesh data is straightforward. Here we use one of the meshes created with the low-level methods above and retrieve some of its data. Note that Blender provides a few values derived from the original arrays, such as loop_indices and vertices per polygon, which can be useful for certain operations. m = bpy . data . meshes [ 'my detailed mesh' ] len ( m . vertices ) => 8 len ( m . polygons ) => 3 # 2 triangles + 1 quad = 2*3 + 1*4 = 10 len ( m . loops ) => 10 # 8 implicit edges (for 2 triangles and 1 quad), shared edges only listed once len ( m . edges ) => 8 m . vertices [ 7 ] . co => Vector (( 2.0 , - 2.0 , - 1.0 )) # Coordinate m . vertices [ 7 ] . normal => Vector (( 0.6 .. , - 0.6 .. , - 0.3 .. )) # Normal m . vertices [ 7 ] . select => True # Selected (edit mode) m . polygons [ 2 ] . index => 2 # Useful in 'for p in m.polygons' m . polygons [ 2 ] . loop_start => 7 # First index in loops array m . polygons [ 2 ] . loop_total => 3 # Number of vertices in loop m . polygons [ 2 ] . loop_indices => [ 7 , 8 , 9 ] # Indices in m.loops m . loops [ 7 ] . vertex_index => 0 m . loops [ 8 ] . vertex_index => 5 m . loops [ 9 ] . vertex_index => 1 m . polygons [ 2 ] . vertices => [ 0 , 5 , 1 ] # Actual vertex indices m . polygons [ 2 ] . select => True # Selected (edit mode) m . polygons [ 2 ] . use_smooth => False # Smooth shading enabled # These are automatically computed m . polygons [ 2 ] . area => 1.4142135381698608 m . polygons [ 2 ] . normal => Vector (( 0.0 , - 0.707 ... , 0.707 ... )) m . polygons [ 2 ] . center => Vector (( 1.0 , - 0.333 ... , - 0.333 ... )) m . edges [ 0 ] . vertices => [ 2 , 3 ] # (bpy_prop_array)","title":"Accessing mesh data (object mode)"},{"location":"advanced/python_scripting/api_summary/#vertex-colors","text":"A mesh can have multiple sets of vertex colors. Each set has a name and for each vertex the associated color (but see below). By default meshes created in Blender do not have a vertex color layer. >>> m bpy . data . meshes [ 'Cube' ] >>> type ( m . vertex_colors ) < class ' bpy_prop_collection '> # Create a new vertex color layer >>> vcol_layer = m . vertex_colors . new ( name = 'My vertex colors' ) >>> vcol_layer bpy . data . meshes [ 'Cube' ] . vertex_colors [ \"My vertex colors\" ] >>> len ( m . vertex_colors ) 1 # Name shown under Object Data -> Vertex Colors >>> vcol_layer . name 'My vertex colors' The vertex colors themselves are accessed through the data member: >>> type ( vcol_layer . data ) < class ' bpy_prop_collection '> >>> len ( vcol_layer . data ) 24 >>> type ( vcol_layer . data [ 0 ] . color ) < class ' bpy_prop_array '> >>> list ( vcol_layer . data [ 0 ] . color ) [ 1.0 , 1.0 , 1.0 , 1.0 ] >>> len ( m . polygons ) 6 >>> len ( m . vertices ) 8 >>> len ( m . loops ) 24 One thing to notice here is that the vertex color array has 24 entries. But the Cube object only has 8 vertices and 6 polygons. The reason for the higher number of vertex colors is that Blender stores separate vertex colors per polygon . So the Cube has 6 polygons, each defined using 4 vertices, hence 6*4=24 vertex colors in total (which is the same number as the length of the loops array). This is more flexible than what most 3D file formats allow, which usually only store one color per vertex. During import Blender will duplicate those colors to set the same color for a vertex in all polygons in which it is used. An example of how to take advantage of the added flexibility is that we can set a random color per cube face by setting each of the 4 vertex colors of a face to the same color: for i in range ( 6 ): r = random () g = random () b = random () for j in range ( 4 ): vcol_layer . data [ 4 * i + j ] . color = ( r , g , b , 1 ) A slightly more Blender-like (and robust) way to write the above code would be to take advantage of the polygon loop indices: for p in m . polygons : r = random () g = random () b = random () for i in p . loop_indices : vcol_layer . data [ i ] . color = ( r , g , b , 1 )","title":"Vertex colors"},{"location":"advanced/python_scripting/api_summary/#uv-coordinates","text":"UV coordinates follow the same setup as vertex colors, but instead store a 2-tuple of floats per vertex per polygon. Note that just like for vertex colors UV coordinates are also specified per vertex per polygon . Meshes created in Blender will already have a UV map called UVMap : >>> m bpy . data . meshes [ 'Cube' ] >>> len ( m . uv_layers ) 1 >>> m . uv_layers [ 0 ] . name 'UVMap' The actual UV values are once again stored under the data member: >>> uv_map = m . uv_layers [ 0 ] >>> uv_map bpy . data . meshes [ 'Cube' ] . uv_layers [ \"UVMap\" ] >>> type ( uv_map . data ) < class ' bpy_prop_collection '> >>> len ( uv_map . data ) 24 >>> type ( uv_map . data [ 0 ]) < class ' bpy . types . MeshUVLoop '> >>> uv_map . data [ 0 ] . uv Vector (( 0.375 , 0.0 )) In general, UV maps are either set through importing or edited within Blender using the UV Editor, although there can be valid reasons for wanting to control them through the Python API.","title":"UV coordinates"},{"location":"advanced/python_scripting/api_summary/#bmesh","text":"There is another method in Blender for creating meshes and accessing their data: the so-called BMesh, which is implemented by the bmesh module and its BMesh class. BMesh is especially interesting when you want to perform more complex geometric operations on an existing mesh, or build up a mesh polygon-by-polygon instead of providing the full mesh in one go as a set of arrays as shown above. Here, we only give a brief overview of BMesh and refer to the API docs for all the details. The differences of BMesh compared to working with the native mesh data structure we showed above: A BMesh holds extra data on mesh connectivity, like the neighbours of a vertex, which can be easily queried for geometric editing. The trade-off is that a BMesh will use more memory to store all this extra data, but that is usually only a limiting factor for very large meshes. It is somewhat slower to create a (large) mesh using a BMesh, as each mesh element (vertex, edge, polygon) takes a Python call to create, plus needs extra calls and Python values to set up. A BMesh cannot be used directly in a scene, it first needs to be converted or copied back to a Mesh (and so mesh data is present twice in memory at some point in time) A large set of high- and low-level geometric operations, such as merging vertices within a given distance, face splitting, edge collapsing or generating a convex hull, is provided in bpy.ops and bmesh.utils . These operations would be tedious and error prone to script manually. Here's a (verbose) example of create a BMesh from scratch that holds a single triangle and edge: import bmesh bm = bmesh . new () # Create 3 vertices v1 = bm . verts . new (( 0 , 0 , 0 )) v2 = bm . verts . new (( 1 , 0 , 1 )) v3 = bm . verts . new (( 0 , 1 , 1 )) v4 = bm . verts . new (( 1 , 1 , 1 )) # Add a triangle bm . faces . new (( v1 , v2 , v3 )) # Add a line edge bm . edges . new (( v3 , v4 )) # Done setting up the BMesh, now copy geometry to a regular Mesh m = bpy . data . meshes . new ( 'mesh' ) bm . to_mesh ( m ) # Release BMesh data, bm will no longer be usable bm . free () # Add regular Mesh as object o = bpy . data . objects . new ( 'mesh' , m ) bpy . context . scene . collection . objects . link ( o ) A BMesh can also be created from an existing Mesh , edited and then copied back to the Mesh : o = bpy . context . active_object m = o . data # Create a new BMesh and copy geometry from the Mesh bm = bmesh . new () bm . from_mesh ( m ) # Edit some geometry bm . verts . ensure_lookup_table () bm . verts [ 4 ] . co . x += 3.14 bm . faces . ensure_lookup_table () bm . faces . remove ( bm . faces [ 0 ]) # Copy back to Mesh bm . to_mesh ( m ) bm . free () If a Mesh is currently in edit mode you can still create a BMesh from it, edit that and the copy the changes back, while keeping the Mesh in edit mode: o = bpy . context . active_object m = o . data assert m . mode == 'EDIT' bm = bmesh . new () # Note the different call, i.e. NOT from_mesh() bm . from_edit_mesh ( m ) # <edit BMesh> # Update edit-mesh of Mesh bm . update_edit_mesh ( m ) bm . free () This can be useful when you're working in edit mode on a mesh and also want to run a script on it that uses BMesh, but don't want to switch in and out of edit-mode to run the script. Note that there are some things to watch out for in synchronizing BMesh state to a Mesh . Some examples of the geometric queries that you can do on a BMesh (see docs for more): bm . verts [ i ] . co # Vertex coordinate as mathutils.Vector bm . verts [ i ] . normal # Vertex normal bm . verts [ i ] . is_boundary # True if vertex is at the mesh boundary bm . verts [ i ] . is_wire # True if vertex is not connected to any faces bm . verts [ i ] . link_edges # Sequence of edges connected to this vertex bm . verts [ i ] . link_faces # Sequence of faces connected to this vertex bm . edges [ i ] . calc_length () # Length of the edge bm . edges [ i ] . is_boundary # True if edge is boundary of a face bm . edges [ i ] . is_wire # True if edge is not connected to any faces bm . edges [ i ] . is_manifold # True if edge is manifold (used in at most 2 faces) v = bm . edges [ i ] . verts [ 0 ] # Get one vertex of this edge bm . edges [ i ] . other_vert ( v ) # Get the other vertex bm . edges [ i ] . link_faces # Sequence of faces connected to this edge bm . faces [ i ] . calc_area () # Face area bm . faces [ i ] . calc_center_median () # Median center bm . faces [ i ] . edges # Sequence of edges defining this face bm . faces [ i ] . verts # Sequence of vertices defining this face bm . faces [ i ] . normal # Face normal","title":"BMesh"},{"location":"advanced/python_scripting/api_summary/#materials","text":"As shown in one of the introductory exercises it is possible to use Python to create a node-based shader. In most cases using the node-based editor in the UI is the preferred option due to its interactivity, but for certain cases it can be interesting to use Python. The general workflow for this is to create the necessary shader nodes, connected them through links as needed and then set the material on the relevant mesh. # Create a new material mat = bpy . data . materials . new ( \"my material\" ) # Enable shader nodes on the material mat . use_nodes = True # Remove the default nodes nodes = mat . node_tree . nodes nodes . clear () # Add a Principled BSDF shader node and set its base color shader = nodes . new ( type = 'ShaderNodeBsdfPrincipled' ) shader . location = 0 , 300 shader . inputs [ 'Base Color' ] . default_value = ( 1 , 0 , 0 , 1 ) # Add a Material Output node node_output = nodes . new ( type = 'ShaderNodeOutputMaterial' ) node_output . location = 400 , 300 # Add a link between the nodes links = nodes . links links . new ( shader . outputs [ 'BSDF' ], node_output . inputs [ 'Surface' ]) # Add material to the mesh's material slots mesh . materials . append ( mat ) A node's inputs and outputs can be referenced by name. This can then be used to set values on inputs, or connect outputs to inputs, as shown. For example, for the Principled BSDF node above: >>> shader . inputs . keys () [ 'Base Color' , 'Subsurface' , 'Subsurface Radius' , 'Subsurface Color' , 'Metallic' , 'Specular' , 'Specular Tint' , 'Roughness' , 'Anisotropic' , 'Anisotropic Rotation' , 'Sheen' , 'Sheen Tint' , 'Clearcoat' , 'Clearcoat Roughness' , 'IOR' , 'Transmission' , 'Transmission Roughness' , 'Emission' , 'Emission Strength' , 'Alpha' , 'Normal' , 'Clearcoat Normal' , 'Tangent' ] >>> shader . outputs . keys () [ 'BSDF' ] The location attributes set above are not strictly needed if you're not going to work on the shader network in the Shader Editor in the UI. But they help to make the node network layout somewhat visually pleasing.","title":"Materials"},{"location":"advanced/python_scripting/api_summary/#material-slots","text":"The last line in the Python code above adds the created material to the mesh's material slots. An object can have multiple materials assigned to it and each assigned material uses a so-called material slot. Each polygon in a mesh can only use a single material, by specifying the material index (i.e. slot) to use for that polygon. This allows different parts of a mesh to use different shaders. By default all faces in a mesh will reference material slot 0. But here's an example of a cube mesh that uses 3 different materials: Inspecting the underlying material data: # Get the mesh, as the material is linked to the mesh by default >>> o = bpy . data . objects [ 'Cube' ] >>> m = o . data # The material slots used >>> list ( m . materials ) [ bpy . data . materials [ 'red' ], bpy . data . materials [ 'black-white checkered' ], bpy . data . materials [ 'voronoi' ]] # Polygon -> slot index >>> m . polygons [ 0 ] . material_index 2 >>> m . polygons [ 1 ] . material_index 0 >>> m . polygons [ 2 ] . material_index 0 >>> m . polygons [ 3 ] . material_index 0 >>> m . polygons [ 4 ] . material_index 1 >>> m . polygons [ 5 ] . material_index 0 Material indices can be set per polygon, or set as an array in one go: # Material slot index for a single polygon m . polygons [ 0 ] . material_index = 0 # Set all polygon material indices face_materials = [ 0 , 1 , 2 , 2 , 1 , 0 ] m . polygons . foreach_set ( 'material_index' , face_materials ) # Force an update of the mesh, needed in this case m . update ()","title":"Material slots"},{"location":"advanced/python_scripting/api_summary/#custom-properties","text":"Sometimes it can useful to be able to control certain values that you use in a script from the UI. The most flexible, but also most complex, approach would be write an add-on . However, in quite a few cases there's a simpler alternative if all you need to control are simple Python values, like an int, float, string or list. From Python you can set custom properties on pretty much any Blender Python data block (see here for more details) and then access those values from the UI: >>> o bpy . data . objects [ 'Cube' ] >>> o [ 'My prop' ] = 123.4 >>> o [ 'My 2nd prop' ] = ( 1 , 1 , 0.5 ) This works, of course, both ways: adding or editing a value from the UI will update the value(s) available through Python. You can then use these values in a script, for example to control a number of objects to create, set a 3D coordinate, etc. See here for more details and examples.","title":"Custom properties"},{"location":"advanced/python_scripting/api_summary/#into-the-deep-end","text":"Here we go deeper into some more exotic topics, but which can be of interest with more advanced Python scripting and complex scene setups.","title":"Into the deep end..."},{"location":"advanced/python_scripting/api_summary/#data-block-users-and-garbage-collection","text":"Blender uses a system based on reference-counting to decide when data-blocks have become unused and can get purged. In the short video below we show some of the details of this scheme: The video shows the Orphan Data outliner mode, but there are several modes that can be used to get detailed insight into the current state of Blender internals: The Blender File mode gives a high-level overview of a file's contents, including some of the more implicit data block types, such as Workspaces. The Data API mode provides an even more detailed view. It is actually a great way to inspect all the gory details of Blender's internal data structures. It will show all data-blocks by type and their attributes. Some attributes can be even be edited in this outliner mode. The Orphan Data mode shows data blocks that do not have any users and which will not be saved (unless they are marked to have a fake user). Some of the data-blocks you see here might not have been created by you, but are used by Blender internally, for example the Brushes. Although the video only focused on materials, the way data-block lifetime is managed using the user counts is general to all types of data-blocks in Blender. But there are subtle differences in whether a data-block is really deleted or just has a link to it removed: Whenever the term \"unlink\" is used it means that a link to that data-block is removed and its user count decreased, but the data-block itself will still be in memory. An example of this is clicking the X next to a mesh's material in the Material Properties. If the UI uses the term \"delete\" it means the data-block is deleted immediately from memory. Any data-blocks linked from the deleted data-block will have their users count decreased. An example of this is deleting a Camera object in the 3D view: the Camera object's data-block is deleted from memory, but the Camera object data data-block (containing the actual camera settings) is still in memory, which you can check in the Orphan Data mode of the outliner. The usage count of data-blocks can also be queried from Python: # Two cube meshes using the same material >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Cube' ], bpy . data . objects [ 'Cube.001' ]] >>> bpy . data . materials [ 'Material' ] . users 2 # Add a new material, set one of the cubes to use it >>> bpy . data . materials [ 'Material' ] . users 1 >>> bpy . data . materials [ 'Material.001' ] . users 1 # <Delete Cube.001 object in the UI> # Hmmm, still has a user? >>> bpy . data . materials [ 'Material.001' ] . users 1 # The reason is we deleted the Cube.001 *object*, but # the Cube.001 *mesh* is still alive (as its usage count # was merely decremented) and it still references the material >>> bpy . data . objects [ 'Cube.001' ] Traceback ( most recent call last ): File \"<blender_console>\" , line 1 , in < module > KeyError : 'bpy_prop_collection[key]: key \"Cube.001\" not found' >>> bpy . data . meshes [ 'Cube.001' ] bpy . data . meshes [ 'Cube.001' ] >>> bpy . data . meshes [ 'Cube.001' ] . users 0 >>> bpy . data . meshes [ 'Cube.001' ] . materials . values () [ bpy . data . materials [ 'Material' ]] The use_fake_user attribute of a data block controls whether a Fake user is set, similar to the checkbox in the UI. Warning In most cases you probably don't want to manually delete data blocks from a file and only use the normal UI operations for that. But it is possible for cases that need it. Truly purging a data block from Python can be done with the relevant remove() method, e.g. >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Cube' ]] >>> o = bpy . context . active_object >>> o bpy . data . objects [ 'Cube' ] >>> m = o . data >>> m bpy . data . meshes [ 'Cube' ] # Remove the Mesh data-block from the file >>> bpy . data . meshes . remove ( m ) >>> bpy . data . meshes . values () [] >>> bpy . data . objects . values () [] Note that in the case of deleting object data (in this case a Mesh) any Objects referencing that object data also get removed ! A second thing to note is the above code does not actually update the current Blender file on disk. That only happens on an explicit save action (e.g. through the File menu or using the relevant operator from Python).","title":"Data-block users and garbage collection"},{"location":"advanced/python_scripting/api_summary/#a-note-on-bpydata-bpydataobjects","text":"We have been using bpy.data.objects in most examples above to access objects in the scene. This is actually not completely clean, as bpy.data.objects holds all objects in the Blender file . Usually, the distinction doesn't matter as you only have one scene, but a Blender file can hold multiple scenes, each with their own set of objects: # A file with two scenes, each with their own set of objects >>> bpy . data . scenes . values () [ bpy . data . scenes [ 'Scene' ], bpy . data . scenes [ 'Scene.001' ]] # Current scene >>> bpy . context . scene bpy . data . scenes [ 'Scene' ] # And its objects >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Bottom cube' ], bpy . data . objects [ 'Top Cube' ]] # <Select different scene> # Different current scene >>> bpy . context . scene bpy . data . scenes [ 'Scene.001' ] # And its objects >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Bottom cube.001' ], bpy . data . objects [ 'Top Cube.001' ]] # All objects in the file >>> bpy . data . objects . values () [ bpy . data . objects [ 'Bottom cube' ], bpy . data . objects [ 'Bottom cube.001' ], bpy . data . objects [ 'Top Cube' ], bpy . data . objects [ 'Top Cube.001' ]] Although objects can also be shared between scenes: # Two scenes >>> bpy . data . scenes . values () [ bpy . data . scenes [ 'Scene' ], bpy . data . scenes [ 'Scene.001' ]] # First scene, cubes are local to scene, torus is shared between scenes >>> bpy . context . scene bpy . data . scenes [ 'Scene' ] >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Torus' ], bpy . data . objects [ 'Bottom cube' ], bpy . data . objects [ 'Top Cube' ]] # Second scene, different cubes, torus is shared >>> bpy . context . scene bpy . data . scenes [ 'Scene.001' ] >>> bpy . context . scene . objects . values () [ bpy . data . objects [ 'Bottom cube.001' ], bpy . data . objects [ 'Top Cube.001' ], bpy . data . objects [ 'Torus' ]] The point here is that bpy.data.objects , and every other attribute under bpy.data , holds values of the complete Blender file . Per-scene values are available through attributes of a Scene object, e.g. bpy.context.scene.objects . For certain use cases this distinction matters.","title":"A note on bpy.data, bpy.data.objects, ..."},{"location":"advanced/python_scripting/api_summary/#parenting","text":"An object's parent can be queried or set simply through its parent attribute, which needs to reference another Object (or None ). But when parenting is involved the use of transformation matrices becomes somewhat more complex. Suppose we have two cubes above each other, the top cube transformed to Z=5 and the bottom cube to Z=2: Using the 3D viewport we'll now parent the bottom cube to the top cube ( LMB click bottom cube, Shift-LMB click top cube, Ctrl-P , select Object ) and inspect the values in Python: >>> bpy . data . objects [ 'Bottom cube' ] . parent bpy . data . objects [ 'Top cube' ] # The bottom cube is still located in the scene at Z=2, # even after parenting, as is expected >>> bpy . data . objects [ 'Bottom cube' ] . matrix_world Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 2.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) If an object has a parent its matrix_local attribute will contain the transformation relative to its parent , while matrix_world will contain the resulting net object-to-world transformation. If no parent is set then matrix_local is equal to matrix_world . Let's check the bottom cube's local matrix value: # Correct, it is indeed -3 in Z relative to its parent >>> bpy . data . objects [ 'Bottom cube' ] . matrix_local Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , - 3.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) As already shown above the parent attribute can be used to inspect and control the parenting relationship: >>> bpy . data . objects [ 'Top cube' ] . parent # None >>> bpy . data . objects [ 'Bottom cube' ] . parent bpy . data . objects [ 'Top cube' ] # Remove parent >>> bpy . data . objects [ 'Bottom cube' ] . parent = None At this point the two cubes are no longer parented and are at Z=2 (\"Bottom cube\") and Z=5 (\"Top cube\") in the scene. But when we restore the parenting relationship from Python something funny happens 1 : # Set parent back to what it was >>> bpy . data . objects [ 'Bottom cube' ] . parent = bpy . data . objects [ 'Top cube' ] The reason for the different position of the cube called \"Bottom cube\" (which is now on top) is that when using the UI to set up a parenting relationship it does more than just setting the parent attribute of the child object. There's also something called the parent-inverse matrix. Let's inspect it and the other matrix transforms we've already seen for the current (unexpected) scene: # Identity matrix, i.e. no transform >>> bpy . data . objects [ 'Bottom cube' ] . matrix_parent_inverse Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 0.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # Hmmm, this places the \"Bottom cube\" 2 in Z *above* its parent at Z=5... >>> bpy . data . objects [ 'Bottom cube' ] . matrix_local Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 2.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # ... so it indeed ends up at Z=7 as we saw (above \"Top cube\") >>> bpy . data . objects [ 'Bottom cube' ] . matrix_world Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 7.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) So what happened here? Apparently the matrix_local matrix changed from its value of Z=-3 as we saw earlier. The answer is that when you set up a parenting relationship using the UI the parent-inverse matrix is set to the inverse of the current parent transformation (as the name suggests) while matrix_local is updated to inverse(parent.matrix_world) @ to_become_child.matrix_world . If we clear the parent value from Python and redo the parenting in the UI we can see this in the resulting transform matrices: >>> bpy . data . objects [ 'Bottom cube' ] . parent = None # <parent \"Bottom cube\" to \"Top cube\" in the UI> # Was identity, is now indeed the inverse of transforming +5 in Z >>> bpy . data . objects [ 'Bottom cube' ] . matrix_parent_inverse Matrix ((( 1.0 , - 0.0 , 0.0 , - 0.0 ), ( - 0.0 , 1.0 , - 0.0 , 0.0 ), ( 0.0 , - 0.0 , 1.0 , - 5.0 ), ( - 0.0 , 0.0 , - 0.0 , 1.0 ))) # Was Z=2, is now 2-5 >>> bpy . data . objects [ 'Bottom cube' ] . matrix_local Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , - 3.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) # Was Z=7 >>> bpy . data . objects [ 'Bottom cube' ] . matrix_world Matrix ((( 1.0 , 0.0 , 0.0 , 0.0 ), ( 0.0 , 1.0 , 0.0 , 0.0 ), ( 0.0 , 0.0 , 1.0 , 2.0 ), ( 0.0 , 0.0 , 0.0 , 1.0 ))) The reason for this behaviour is that when doing parenting in the 3D viewport you usually do not want the object that you are setting as the child to move. So the parenting matrices are adjusted accordingly when the parenting relationship is set up. But when we simply set parent from Python, the matrix_local value is used as is, causing our bottom cube to suddenly move up, as it is used as the transform relative to its parent, while it actually would need a different value to stay in place. There's actually quite a bit more going on with all the different parenting options available from the UI. See this page for more details. The same thing happens when setting the parent in the UI using Object Properties > Relations > Parent \u21a9","title":"Parenting"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_protoplanetary_disk_scientific_model_in_Blender/","text":"Python scripting for Blender - A case study \u00b6 A scientific visualisation with Blender 1. Introduction & goals \u00b6 1.1 What we will visualise: a proto-planetary disk \u00b6 A proto-planetary disk is a disk-like structure around a newly born star. This disk is filled with dust (solid-state particles with a diameter in the order of 1 micrometer) and gas. In the course of time this dust and gas can coallesce into planets. In this case-study we will look at a model (called MCMax) of the dust in such a disk. The model calculates the temperature and density of the dust in the disk, taking the radiation and gravity of the star into account. Using the temperature and density the code can then calculate the light radiation coming from the disk as if observed with a telescope using ray-tracing. The calculations of MCMax is done iteratively and using Monte Carlo techniques. Packages of photons are emitted by the star in random directions and their wavelength sampled from the radiation distribution of the star (by default a blackbody). Using the absorption, scattering and emission properties used for the dust grains in the disk, the scattering, absorption and re-emission of the photons are calculated throughout the disk. This is used to calculate a temperature structure in the disk. This temperature is then used to adapt the starting density structure of the disk after which a new pass is done by tracking a next set of photons and adapting the density subsequenty. This is repeated until convergence is reached. The code uses a two dimentional grid in the radial and theta direction. The disk is assumed to be cylindrically symmetric around the polar axis. The grid cell size is lowered in regions where the density becomes high. 1.2 How will we visualise such a proto-planetary disk \u00b6 We would like to create a 3D model of the disk at constant density and display the temperature as colors on the surface of the model (see sketch in Fig. X). We could use this to make nice renders and animations to show the temperature structure of the disk. For this we need to pre-process the data from the model to get the spatial coordinates of the disk at a constant density. These coordinates then need to be converted into vertices, edges and faces before creating the geometry in Blender. We will then add the temperatures to the faces using vertex coloring by adding the needed shaders to the model. We will start by exploring and pre-processing the data in the next section. Figure X: Sketch of the Blender model we want to make. Indicate cutout surfaces and disk surface. Also indicate spherical and cartesian coordinates. 2. Exploring the example data \u00b6 2.1 How the model data is structured \u00b6 An example output file of modeling code MCMax is shown below. # Format number 5 # NR, NT, NGRAINS, NGRAINS2 100 100 1 1 # Spherical radius grid [cm] (middle of cell) 7479900216981.22 7479900572789.07 [...] # Theta grid [rad, from pole] (middle of cell) 9.233559849414326E-003 2.365344804038962E-002 [...] # Density array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-050 1.001753516582521E-050 [...] # Temperature array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1933.54960366819 1917.22966277529 [...] # Composition array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.00000000000000 1.00000000000000 [...] # Gas density array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-048 1.001753516582521E-048 [...] # Density0 array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-050 1.001753516582521E-050 [...] The file is structured in a way the scientist thought best at the time using the tools at hand. For us it is important to notice the NR and NT , which stands for number of radial and theta points respectively (NGRAINS is related to the number of different type of dust grains in the disk and we will not work with this for now). Further, the output file then lists the radius points and after that the theta points. Subsequently temperature and density values are listed by iterating over the radius and then the theta indices. The units of all the values in the MCMax output are: R[cm], Theta[radians], Density[gr/cm^3], Temperature[K]. We can make a function to read in the output file of the MCMax code like this: def readInMCMaxFile ( filepath ): denstemp_file = open ( filepath , \"r\" ) # Header information on first 3 lines header = [ denstemp_file . readline () for i in range ( 3 )] # One line contains grid information Nrad , Ntheta , nGrains , nGrains2 = [ int ( i ) for i in denstemp_file . readline () . split ()] denstemp_file . readline () # Non-data line # Read in radius grid R = [ float ( denstemp_file . readline ()) / AU for i in range ( Nrad )] denstemp_file . readline () # Non-data line # Read in theta grid Th = [ float ( denstemp_file . readline ()) / ( 2 * math . pi ) * 360 for i in range ( Ntheta )] denstemp_file . readline () # Non-data line # Read in density rho = [ [ float ( denstemp_file . readline ()) * scale_density for j in range ( Ntheta )] for i in range ( Nrad )] denstemp_file . readline () # Non-data line # Read in temperature temp = [ [ float ( denstemp_file . readline ()) for j in range ( Ntheta )] for i in range ( Nrad )] denstemp_file . readline () # Non-data line denstemp_file . close () return Nrad , Ntheta , R , Th , rho , temp Here we convert theta from radians to degrees and the radius from cm to astronomical units (AU, one times the distance between the Sun and Earth). We now have the data in lists and we are ready to process them to values we can use in Blender. 2.2 Pre-processing the data \u00b6 Converting the coordinates \u00b6 The data from the MCMax code is in spherical coordinates, while the system in Blender works with cartesian coordinates. The theta in the output is defined as the angle with the z-axis. We define phi as the angle from the x-axis. In this way we can convert from spherical coordinates to cartesian: def getCartCoord ( R , th , ph ): x = math . sin ( ph / 360 * 2 * math . pi ) * math . sin ( th / 360 * 2 * math . pi ) * R y = math . cos ( ph / 360 * 2 * math . pi ) * math . sin ( th / 360 * 2 * math . pi ) * R z = math . cos ( th / 360 * 2 * math . pi ) * R return x , y , z Figure X: Definition of coordinates. Exercise 2.1: Inspecting the data Make a plot from the datsa .... Making isosurfaces for density and other parameters \u00b6 Creating vertices and edges \u00b6 Making faces using fill_holes() \u00b6 Creating vertex colors for the temperature data \u00b6","title":"<span style=\"color:#00abe9\">Python scripting for Blender - A case study</span>"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_protoplanetary_disk_scientific_model_in_Blender/#python-scripting-for-blender-a-case-study","text":"A scientific visualisation with Blender","title":"Python scripting for Blender - A case study"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_protoplanetary_disk_scientific_model_in_Blender/#1-introduction-goals","text":"","title":"1. Introduction &amp; goals"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_protoplanetary_disk_scientific_model_in_Blender/#11-what-we-will-visualise-a-proto-planetary-disk","text":"A proto-planetary disk is a disk-like structure around a newly born star. This disk is filled with dust (solid-state particles with a diameter in the order of 1 micrometer) and gas. In the course of time this dust and gas can coallesce into planets. In this case-study we will look at a model (called MCMax) of the dust in such a disk. The model calculates the temperature and density of the dust in the disk, taking the radiation and gravity of the star into account. Using the temperature and density the code can then calculate the light radiation coming from the disk as if observed with a telescope using ray-tracing. The calculations of MCMax is done iteratively and using Monte Carlo techniques. Packages of photons are emitted by the star in random directions and their wavelength sampled from the radiation distribution of the star (by default a blackbody). Using the absorption, scattering and emission properties used for the dust grains in the disk, the scattering, absorption and re-emission of the photons are calculated throughout the disk. This is used to calculate a temperature structure in the disk. This temperature is then used to adapt the starting density structure of the disk after which a new pass is done by tracking a next set of photons and adapting the density subsequenty. This is repeated until convergence is reached. The code uses a two dimentional grid in the radial and theta direction. The disk is assumed to be cylindrically symmetric around the polar axis. The grid cell size is lowered in regions where the density becomes high.","title":"1.1 What we will visualise: a proto-planetary disk"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_protoplanetary_disk_scientific_model_in_Blender/#12-how-will-we-visualise-such-a-proto-planetary-disk","text":"We would like to create a 3D model of the disk at constant density and display the temperature as colors on the surface of the model (see sketch in Fig. X). We could use this to make nice renders and animations to show the temperature structure of the disk. For this we need to pre-process the data from the model to get the spatial coordinates of the disk at a constant density. These coordinates then need to be converted into vertices, edges and faces before creating the geometry in Blender. We will then add the temperatures to the faces using vertex coloring by adding the needed shaders to the model. We will start by exploring and pre-processing the data in the next section. Figure X: Sketch of the Blender model we want to make. Indicate cutout surfaces and disk surface. Also indicate spherical and cartesian coordinates.","title":"1.2 How will we visualise such a proto-planetary disk"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_protoplanetary_disk_scientific_model_in_Blender/#2-exploring-the-example-data","text":"","title":"2. Exploring the example data"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_protoplanetary_disk_scientific_model_in_Blender/#21-how-the-model-data-is-structured","text":"An example output file of modeling code MCMax is shown below. # Format number 5 # NR, NT, NGRAINS, NGRAINS2 100 100 1 1 # Spherical radius grid [cm] (middle of cell) 7479900216981.22 7479900572789.07 [...] # Theta grid [rad, from pole] (middle of cell) 9.233559849414326E-003 2.365344804038962E-002 [...] # Density array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-050 1.001753516582521E-050 [...] # Temperature array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1933.54960366819 1917.22966277529 [...] # Composition array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.00000000000000 1.00000000000000 [...] # Gas density array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-048 1.001753516582521E-048 [...] # Density0 array (for ir=0,nr-1 do for it=0,nt-1 do ...) 1.001753516582521E-050 1.001753516582521E-050 [...] The file is structured in a way the scientist thought best at the time using the tools at hand. For us it is important to notice the NR and NT , which stands for number of radial and theta points respectively (NGRAINS is related to the number of different type of dust grains in the disk and we will not work with this for now). Further, the output file then lists the radius points and after that the theta points. Subsequently temperature and density values are listed by iterating over the radius and then the theta indices. The units of all the values in the MCMax output are: R[cm], Theta[radians], Density[gr/cm^3], Temperature[K]. We can make a function to read in the output file of the MCMax code like this: def readInMCMaxFile ( filepath ): denstemp_file = open ( filepath , \"r\" ) # Header information on first 3 lines header = [ denstemp_file . readline () for i in range ( 3 )] # One line contains grid information Nrad , Ntheta , nGrains , nGrains2 = [ int ( i ) for i in denstemp_file . readline () . split ()] denstemp_file . readline () # Non-data line # Read in radius grid R = [ float ( denstemp_file . readline ()) / AU for i in range ( Nrad )] denstemp_file . readline () # Non-data line # Read in theta grid Th = [ float ( denstemp_file . readline ()) / ( 2 * math . pi ) * 360 for i in range ( Ntheta )] denstemp_file . readline () # Non-data line # Read in density rho = [ [ float ( denstemp_file . readline ()) * scale_density for j in range ( Ntheta )] for i in range ( Nrad )] denstemp_file . readline () # Non-data line # Read in temperature temp = [ [ float ( denstemp_file . readline ()) for j in range ( Ntheta )] for i in range ( Nrad )] denstemp_file . readline () # Non-data line denstemp_file . close () return Nrad , Ntheta , R , Th , rho , temp Here we convert theta from radians to degrees and the radius from cm to astronomical units (AU, one times the distance between the Sun and Earth). We now have the data in lists and we are ready to process them to values we can use in Blender.","title":"2.1 How the model data is structured"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_protoplanetary_disk_scientific_model_in_Blender/#22-pre-processing-the-data","text":"","title":"2.2 Pre-processing the data"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_something_more/","text":"Python scripting for Blender - extra topics \u00b6 1. The Blender context and python scripting \u00b6 1.1 Access the Blender context \u00b6 User context: Based on what is selected by the user bpy.context.object 2 Using and making operators \u00b6 Adding an operator through classes: import bpy class SimpleOperator ( bpy . types . Operator ): bl_idname = \"object.simple_operator\" # USED IN bpy.ops.object bl_label = \"Tool Name\" # USED BY OPERATOR SEARCH (spacebar, F3) def execute ( self , context ): print ( \"Hello World\" ) return { 'FINISHED' } bpy . utils . register_class ( SimpleOperator ) Misc Blender items \u00b6 Adding a background \u00b6 Go to World Properties Click on the circle next to 'Color' and choose 'Image Texture' Change 'Repeat' into 'Clip' Click on the circle next to 'Vector' - 'default' and set it to 'Window' See also: https://henryegloff.com/how-to-render-a-background-image-in-blender-2-8-using-the-document-world-settings/","title":"<span style=\"color:#00abe9\">Python scripting for Blender - extra topics</span>"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_something_more/#python-scripting-for-blender-extra-topics","text":"","title":"Python scripting for Blender - extra topics"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_something_more/#1-the-blender-context-and-python-scripting","text":"","title":"1. The Blender context and python scripting"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_something_more/#11-access-the-blender-context","text":"User context: Based on what is selected by the user bpy.context.object","title":"1.1 Access the Blender context"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_something_more/#2-using-and-making-operators","text":"Adding an operator through classes: import bpy class SimpleOperator ( bpy . types . Operator ): bl_idname = \"object.simple_operator\" # USED IN bpy.ops.object bl_label = \"Tool Name\" # USED BY OPERATOR SEARCH (spacebar, F3) def execute ( self , context ): print ( \"Hello World\" ) return { 'FINISHED' } bpy . utils . register_class ( SimpleOperator )","title":"2 Using and making operators"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_something_more/#misc-blender-items","text":"","title":"Misc Blender items"},{"location":"advanced/python_scripting/extra_and_answers/Tutorial_something_more/#adding-a-background","text":"Go to World Properties Click on the circle next to 'Color' and choose 'Image Texture' Change 'Repeat' into 'Clip' Click on the circle next to 'Vector' - 'default' and set it to 'Window' See also: https://henryegloff.com/how-to-render-a-background-image-in-blender-2-8-using-the-document-world-settings/","title":"Adding a background"},{"location":"advanced/python_scripting/extra_and_answers/answers/","text":"Exercise 1: starting Blender from the console \u00b6 Find the Blender executable on the machine you are working on. Open Blender through the console. Delete the cube in the default project of Blender, what output is shown in the console? Exercise 2: running a script and rendering from the console \u00b6 Write an external script that removes the Cube object that is part of the default scene [^1] Then, from the command line and without opening the Blender GUI execute this script and render the first frame. Let it output a PNG image file in the directory of the blender file. Was the cube indeed removed from the rendered image? Extra question: is the cube removed from the blender file? Answer: import bpy bpy . data . objects [ 'Cube' ] . select_set ( True ) bpy . ops . object . delete () # OR: bpy.data.objects.remove(bpy.data.objects[\"Cube\"]) blender -b test.blend -o .//render_ -F PNG -P test.py -f 1 Exercise 3: a filled disc from scratch \u00b6 In the text above we created a triangle, now as an exercise let's create a spherical disk. First create a ring of vertices, then create edges and a face. Answer: import bpy import math X = [ math . sin ( a ) for a in [ p / 360 * 2 * math . pi for p in range ( 360 )]] Y = [ math . cos ( a ) for a in [ p / 360 * 2 * math . pi for p in range ( 360 )]] verts = [( x , y , 0 ) for x , y in zip ( X , Y )] edges = [( iv , iv + 1 ) for iv in range ( len ( verts ) - 1 )] faces = [ range ( len ( verts ))] mesh = bpy . data . meshes . new ( \"disk_mesh\" ) ob = bpy . data . objects . new ( \"disk\" , mesh ) mesh . from_pydata ( verts , edges , faces ) bpy . context . collection . objects . link ( ob ) Exercise 4: making triangles and make a vertex color layer \u00b6 Let's take the triangle we made in section 4.1, but let's add another triangle to it, attached to the first. The code would look like this: import bpy # Create a new mesh ob_name = \"triangle\" mesh = bpy . data . meshes . new ( ob_name + \"_mesh\" ) # Create a new object with the mesh ob = bpy . data . objects . new ( ob_name , mesh ) # Define some geometry verts = [ ( 0 , 0 , 0 ), ( 0 , 2 , 0 ), ( 0 , 1 , 2 ) , ( 0 , 3 , 2 ) ] edges = [ ( 0 , 1 ), ( 1 , 2 ), ( 2 , 0 ), ( 1 , 3 ), ( 3 , 2 ) ] # These are indices pointing to elements in the list verts faces = [ ( 0 , 1 , 2 ), ( 1 , 3 , 2 ) ] # These are indices pointing to elements in the list verts # Add it to the mesh mesh . from_pydata ( verts , edges , faces ) # Link the object to the first collection bpy . data . collections [ 0 ] . objects . link ( ob ) Now make a vertex color layer for your triangles. Then inspect how many entries are in color_layer = mesh.vertex_colors['vert_colors'] . Why are they the same or different from the total number of vertices in the mesh? Answer: vertex_colors_name = \"vert_colors\" mesh . vertex_colors . new ( name = vertex_colors_name ) color_layer = mesh . vertex_colors [ vertex_colors_name ] >>> len ( color_layer . data ) 6 Exercise 5: coloring your triangles \u00b6 Let's take the two connected triangles of exercise 4. We will color them in two different ways, using vertex coloring and Python scripting. a) Make the first triangle (face (0,1,2)) green and the second (face (1,3,2)) red. b) Now color vertex (0,0,0) and (0,3,2) red and (0,2,0) and (0,1,2) green to make it look like in Fig.4. Answer: if exercise == \"a\" : vert_colors = [ [ 0 , 1 , 0 , 1 ], [ 0 , 1 , 0 , 1 ], [ 0 , 1 , 0 , 1 ], [ 1 , 0 , 0 , 1 ], [ 1 , 0 , 0 , 1 ] , [ 1 , 0 , 0 , 1 ] ] else : vert_colors = [ [ 1 , 0 , 0 , 1 ], [ 0 , 1 , 0 , 1 ], [ 0 , 1 , 0 , 1 ], [ 1 , 0 , 0 , 1 ] ] for poly in mesh . polygons : for vert_i_poly , vert_i_mesh in enumerate ( poly . vertices ): #loop_indices: vert_i_loop = poly . loop_indices [ vert_i_poly ] if exercise == \"a\" : color_layer . data [ vert_i_loop ] . color = vert_colors [ vert_i_loop ] #rgb else : color_layer . data [ vert_i_loop ] . color = vert_colors [ vert_i_mesh ] print ( poly , vert_i_poly , vert_i_mesh )","title":"Answers"},{"location":"advanced/python_scripting/extra_and_answers/answers/#exercise-1-starting-blender-from-the-console","text":"Find the Blender executable on the machine you are working on. Open Blender through the console. Delete the cube in the default project of Blender, what output is shown in the console?","title":"Exercise 1: starting Blender from the console"},{"location":"advanced/python_scripting/extra_and_answers/answers/#exercise-2-running-a-script-and-rendering-from-the-console","text":"Write an external script that removes the Cube object that is part of the default scene [^1] Then, from the command line and without opening the Blender GUI execute this script and render the first frame. Let it output a PNG image file in the directory of the blender file. Was the cube indeed removed from the rendered image? Extra question: is the cube removed from the blender file? Answer: import bpy bpy . data . objects [ 'Cube' ] . select_set ( True ) bpy . ops . object . delete () # OR: bpy.data.objects.remove(bpy.data.objects[\"Cube\"]) blender -b test.blend -o .//render_ -F PNG -P test.py -f 1","title":"Exercise 2: running a script and rendering from the console"},{"location":"advanced/python_scripting/extra_and_answers/answers/#exercise-3-a-filled-disc-from-scratch","text":"In the text above we created a triangle, now as an exercise let's create a spherical disk. First create a ring of vertices, then create edges and a face. Answer: import bpy import math X = [ math . sin ( a ) for a in [ p / 360 * 2 * math . pi for p in range ( 360 )]] Y = [ math . cos ( a ) for a in [ p / 360 * 2 * math . pi for p in range ( 360 )]] verts = [( x , y , 0 ) for x , y in zip ( X , Y )] edges = [( iv , iv + 1 ) for iv in range ( len ( verts ) - 1 )] faces = [ range ( len ( verts ))] mesh = bpy . data . meshes . new ( \"disk_mesh\" ) ob = bpy . data . objects . new ( \"disk\" , mesh ) mesh . from_pydata ( verts , edges , faces ) bpy . context . collection . objects . link ( ob )","title":"Exercise 3: a filled disc from scratch"},{"location":"advanced/python_scripting/extra_and_answers/answers/#exercise-4-making-triangles-and-make-a-vertex-color-layer","text":"Let's take the triangle we made in section 4.1, but let's add another triangle to it, attached to the first. The code would look like this: import bpy # Create a new mesh ob_name = \"triangle\" mesh = bpy . data . meshes . new ( ob_name + \"_mesh\" ) # Create a new object with the mesh ob = bpy . data . objects . new ( ob_name , mesh ) # Define some geometry verts = [ ( 0 , 0 , 0 ), ( 0 , 2 , 0 ), ( 0 , 1 , 2 ) , ( 0 , 3 , 2 ) ] edges = [ ( 0 , 1 ), ( 1 , 2 ), ( 2 , 0 ), ( 1 , 3 ), ( 3 , 2 ) ] # These are indices pointing to elements in the list verts faces = [ ( 0 , 1 , 2 ), ( 1 , 3 , 2 ) ] # These are indices pointing to elements in the list verts # Add it to the mesh mesh . from_pydata ( verts , edges , faces ) # Link the object to the first collection bpy . data . collections [ 0 ] . objects . link ( ob ) Now make a vertex color layer for your triangles. Then inspect how many entries are in color_layer = mesh.vertex_colors['vert_colors'] . Why are they the same or different from the total number of vertices in the mesh? Answer: vertex_colors_name = \"vert_colors\" mesh . vertex_colors . new ( name = vertex_colors_name ) color_layer = mesh . vertex_colors [ vertex_colors_name ] >>> len ( color_layer . data ) 6","title":"Exercise 4: making triangles and make a vertex color layer"},{"location":"advanced/python_scripting/extra_and_answers/answers/#exercise-5-coloring-your-triangles","text":"Let's take the two connected triangles of exercise 4. We will color them in two different ways, using vertex coloring and Python scripting. a) Make the first triangle (face (0,1,2)) green and the second (face (1,3,2)) red. b) Now color vertex (0,0,0) and (0,3,2) red and (0,2,0) and (0,1,2) green to make it look like in Fig.4. Answer: if exercise == \"a\" : vert_colors = [ [ 0 , 1 , 0 , 1 ], [ 0 , 1 , 0 , 1 ], [ 0 , 1 , 0 , 1 ], [ 1 , 0 , 0 , 1 ], [ 1 , 0 , 0 , 1 ] , [ 1 , 0 , 0 , 1 ] ] else : vert_colors = [ [ 1 , 0 , 0 , 1 ], [ 0 , 1 , 0 , 1 ], [ 0 , 1 , 0 , 1 ], [ 1 , 0 , 0 , 1 ] ] for poly in mesh . polygons : for vert_i_poly , vert_i_mesh in enumerate ( poly . vertices ): #loop_indices: vert_i_loop = poly . loop_indices [ vert_i_poly ] if exercise == \"a\" : color_layer . data [ vert_i_loop ] . color = vert_colors [ vert_i_loop ] #rgb else : color_layer . data [ vert_i_loop ] . color = vert_colors [ vert_i_mesh ] print ( poly , vert_i_poly , vert_i_mesh )","title":"Exercise 5: coloring your triangles"},{"location":"basics/animation/everything/","text":"Animating everything \u00b6 Here, we'll show how generic and powerful the Blender animation system is.","title":"Animating everything"},{"location":"basics/animation/everything/#animating-everything","text":"Here, we'll show how generic and powerful the Blender animation system is.","title":"Animating everything"},{"location":"basics/animation/example_flipbook_animation/","text":"\ud83d\udcbb Flipbook animation \u00b6 Here are the steps needed to import a set of animated meshes and make them play as an animation within Blender. The approach we use here is to have a single mesh object on which we change the associated mesh data each frame. So even though all timesteps are loaded only one of them is visible at a time. Here we take advantage of the Blender scene organization, where each object (a mesh object in this case) refers to object data (one of the meshes in the animation). We use a small Python script, called a frame handler , to respond to a change of the current frame time. Warning The method below will import all meshes in the animation into the current scene. This uses quite a bit of memory (around 1GB in our tests). Info The data for this example is part of our advanced course and you can find the data on https://edu.nl/hrvbe under data/animation . The animated_ply_imports.blend scene file contains two Python scripts, in the Text Editor called 1. import ply files and 2. register anim handler . The dambreak.tar.gz file contains a set of animated meshes in binary PLY format and so is quite large when extracted. Extract dambreak.tar.gz in the same directory as animated_ply_imports.blend . This will create a directory dambreak which contains the PLY files. Load animated_ply_imports.blend As noted above, this blend file not only contains a 3D scene, but also two Python scripts we use to set up the flipbook animation. The first step is to load all the timesteps in the dataset using one of the scripts. This might take a bit of time, depending on the speed of your system. By default, only the first 100 steps are loaded. You can increase the number of files to the full 300 if you like by updating the variable N in both the import script and the animation handler script. Execute the script that imports the time step meshes from the PLY files. To do this step make sure the script called 1. import ply files is selected in the text editor panel. Then press the play button to the right of it, which will execute the script (an alternative is press Alt-P in the editor). The cursor will change to an animated circle, indicating the import is running. In case you get the idea something is wrong check the console output in the terminal where you started Blender. After all PLY files are loaded execute the script that installs the frame change handler. This script is called 2. register anim handler . Make sure the text editor is switched to this script, then press the play button. Verify that the flipbook animation works with Space and/or moving the time slider in the Timeline with Shift-RMB . You should see the fluid simulation evolve with each frame. You can also check the object data associated with the Fluid sim object in the Outliner to see that it changes. The playback speed will depend on your system's performance, but also on the framerate setting chosen. Change the Frame Rate value (in the Output properties tab at the right side of the screen, icon ) to different values to see how your system handles it. Is 60 fps feasible? The Fluid sim object is still transformable as any normal object. Experiment with this, to see how it influences the flipbook animation. If you like, you can add a Camera to the scene and make it follow the wave of fluid in a nice way and then render this into animation.","title":"\ud83d\udcbb Flipbook animation"},{"location":"basics/animation/example_flipbook_animation/#flipbook-animation","text":"Here are the steps needed to import a set of animated meshes and make them play as an animation within Blender. The approach we use here is to have a single mesh object on which we change the associated mesh data each frame. So even though all timesteps are loaded only one of them is visible at a time. Here we take advantage of the Blender scene organization, where each object (a mesh object in this case) refers to object data (one of the meshes in the animation). We use a small Python script, called a frame handler , to respond to a change of the current frame time. Warning The method below will import all meshes in the animation into the current scene. This uses quite a bit of memory (around 1GB in our tests). Info The data for this example is part of our advanced course and you can find the data on https://edu.nl/hrvbe under data/animation . The animated_ply_imports.blend scene file contains two Python scripts, in the Text Editor called 1. import ply files and 2. register anim handler . The dambreak.tar.gz file contains a set of animated meshes in binary PLY format and so is quite large when extracted. Extract dambreak.tar.gz in the same directory as animated_ply_imports.blend . This will create a directory dambreak which contains the PLY files. Load animated_ply_imports.blend As noted above, this blend file not only contains a 3D scene, but also two Python scripts we use to set up the flipbook animation. The first step is to load all the timesteps in the dataset using one of the scripts. This might take a bit of time, depending on the speed of your system. By default, only the first 100 steps are loaded. You can increase the number of files to the full 300 if you like by updating the variable N in both the import script and the animation handler script. Execute the script that imports the time step meshes from the PLY files. To do this step make sure the script called 1. import ply files is selected in the text editor panel. Then press the play button to the right of it, which will execute the script (an alternative is press Alt-P in the editor). The cursor will change to an animated circle, indicating the import is running. In case you get the idea something is wrong check the console output in the terminal where you started Blender. After all PLY files are loaded execute the script that installs the frame change handler. This script is called 2. register anim handler . Make sure the text editor is switched to this script, then press the play button. Verify that the flipbook animation works with Space and/or moving the time slider in the Timeline with Shift-RMB . You should see the fluid simulation evolve with each frame. You can also check the object data associated with the Fluid sim object in the Outliner to see that it changes. The playback speed will depend on your system's performance, but also on the framerate setting chosen. Change the Frame Rate value (in the Output properties tab at the right side of the screen, icon ) to different values to see how your system handles it. Is 60 fps feasible? The Fluid sim object is still transformable as any normal object. Experiment with this, to see how it influences the flipbook animation. If you like, you can add a Camera to the scene and make it follow the wave of fluid in a nice way and then render this into animation.","title":"\ud83d\udcbb Flipbook animation"},{"location":"basics/animation/exercise_manual_camera_orbit/","text":"\ud83d\udcbb Orbiting an object manually \u00b6 Info The steps in this exercise were partly shown in the presentation as well, but that was mostly to illustrate keyframe animation. Here, you can redo those steps in detail and experiment with them. To orbit an object the camera needs a circular path around the object's location. Load orbit.blend The scene contains a single monkey (centered at the origin) and a camera. Note that the animation has a length of 100 frames, starting at frame 0. As a first way of doing an orbit we're going to add keyframes for the camera position, as it rotates around the monkey, using the 3D cursor pivot mode. Set the Pivot Point mode to 3D cursor (bring up the Pivot Point pie menu with period . , select 3D Cursor ). Make sure the 3D cursor is located in the origin by resetting its position with Shift-C . This will also change the view to fit the scene extents. In general, you can check the current position of the 3D cursor in the sidebar ( N to toggle) on the View tab under 3D Cursor Select the camera. Verify that as you rotate it around the Z axis the camera indeed orbits the 3D cursor, and therefore also orbits around the monkey head. Add 4 keyframes at intervals of 25 frames and 90 degrees rotation around Z to complete a 360 degree rotation of the camera around the object over the full animation of 100 frames Play the animation with Spacebar . Is the camera orbit usable? Why not? Also check the camera view during playback. Check the graphs in the Graph Editor. See if you can improve the camera orbit, either by changing the graphs, inserting more keyframes, or both. One way to influence the shape of the curves is to edit the handles attached to each control point, or to change the keyframe interpolation for a control point with T . Tip If you have only a single object in front of the camera around which you want to orbit, an alternative approach is to simply rotate the object itself while keeping the camera in a fixed position. However, this might not always be feasible or preferable.","title":"\ud83d\udcbb Orbiting an object manually"},{"location":"basics/animation/exercise_manual_camera_orbit/#orbiting-an-object-manually","text":"Info The steps in this exercise were partly shown in the presentation as well, but that was mostly to illustrate keyframe animation. Here, you can redo those steps in detail and experiment with them. To orbit an object the camera needs a circular path around the object's location. Load orbit.blend The scene contains a single monkey (centered at the origin) and a camera. Note that the animation has a length of 100 frames, starting at frame 0. As a first way of doing an orbit we're going to add keyframes for the camera position, as it rotates around the monkey, using the 3D cursor pivot mode. Set the Pivot Point mode to 3D cursor (bring up the Pivot Point pie menu with period . , select 3D Cursor ). Make sure the 3D cursor is located in the origin by resetting its position with Shift-C . This will also change the view to fit the scene extents. In general, you can check the current position of the 3D cursor in the sidebar ( N to toggle) on the View tab under 3D Cursor Select the camera. Verify that as you rotate it around the Z axis the camera indeed orbits the 3D cursor, and therefore also orbits around the monkey head. Add 4 keyframes at intervals of 25 frames and 90 degrees rotation around Z to complete a 360 degree rotation of the camera around the object over the full animation of 100 frames Play the animation with Spacebar . Is the camera orbit usable? Why not? Also check the camera view during playback. Check the graphs in the Graph Editor. See if you can improve the camera orbit, either by changing the graphs, inserting more keyframes, or both. One way to influence the shape of the curves is to edit the handles attached to each control point, or to change the keyframe interpolation for a control point with T . Tip If you have only a single object in front of the camera around which you want to orbit, an alternative approach is to simply rotate the object itself while keeping the camera in a fixed position. However, this might not always be feasible or preferable.","title":"\ud83d\udcbb Orbiting an object manually"},{"location":"basics/animation/exercise_parented_camera_orbit/","text":"\ud83d\udcbb Camera orbiting using parenting \u00b6 We will try another way of doing a camera orbit. This method involves parenting the camera to an empty . Parenting is creating a hierarchical relation between two objects. An empty is a special 3D object with no geometry, but which can be placed and oriented in the scene as usual. It is shown as a 3D cross-hairs in the 3D view. It is often used when doing parenting. Load orbit.blend . If you happened to have saved the file in the previous assignment with some keyframes set on the camera you can delete these by selecting the Camera. Then go into the Timeline editor at the bottom and select all keyframes (diamond markers) with A , press X , choose Delete Keyframes . Reset the 3D cursor to the origin with Shift-C Add an Empty to the scene: Shift-A > Empty > Arrows Select only the camera, then add the Empty to the selection by clicking Shift-LMB with the cursor over the empty (or using Ctrl-LMB in the outliner). The camera should now have a dark orange selection outline, while the empty should have a light orange outline, as the latter is the active object. Press Ctrl-P and pick Object to add a parent-child relationship A black dotted line from the camera to the empty should now be visible in the scene. This means the camera is now parented to the empty. Any transformation you apply to the empty will get applied to the camera as well. Info Bad Parenting : if you made a mistake in the parenting of step 6 then you can clear an object's parent by selecting that object, pressing Alt-P and picking Clear Parent . Verify in the outliner that the Camera object is now indeed a child of the Empty (you might have to use the little white triangles to open the necessary tree entries) Make the empty the single selected object. Enter Z rotation mode by pressing R followed by Z . Note that as you move the mouse both the empty and camera are transformed. Exit the rotation mode with Esc , leaving the Z rotation of the empty set to zero. Add key frames at the beginning and end of the animation to have the empty rotate 360 degrees around Z over the animation period Check the camera orbit, including how it looks in the camera view. Is this orbit better? You might have noticed that, even though we now have a nice circular rotation of the camera around the object, the rotation speed actually isn't constant. If you select the empty and look at the Graph Editor you can see that the graph line representing the Z rotation value isn't straight, but looks like an S. This is due to the default interpolation mode that is used between key frames. To make the rotation speed constant make sure the empty is selected. Then in the Graph Editor select all curve points with A and press V to set the handle type, pick Vector . The curves should now have become straight lines. Check the animation to see the rotation speed has become constant. Depending on how exactly you set up the animation you might notice a hickup at the moment that the animation wraps around from frame 99 to frame 0. This happens in case you set the same visible rotation of the empty for frame 0 and 99 (e.g. 0 degrees for frame 0 and 360 degrees for frame 99). You can fix this by changing the animation length to 99 frames by setting End to 98 in the Output properties panel (the value is directly below Frame Start ). Now, the animation should wrap around smoothly.","title":"\ud83d\udcbb Camera orbiting using parenting"},{"location":"basics/animation/exercise_parented_camera_orbit/#camera-orbiting-using-parenting","text":"We will try another way of doing a camera orbit. This method involves parenting the camera to an empty . Parenting is creating a hierarchical relation between two objects. An empty is a special 3D object with no geometry, but which can be placed and oriented in the scene as usual. It is shown as a 3D cross-hairs in the 3D view. It is often used when doing parenting. Load orbit.blend . If you happened to have saved the file in the previous assignment with some keyframes set on the camera you can delete these by selecting the Camera. Then go into the Timeline editor at the bottom and select all keyframes (diamond markers) with A , press X , choose Delete Keyframes . Reset the 3D cursor to the origin with Shift-C Add an Empty to the scene: Shift-A > Empty > Arrows Select only the camera, then add the Empty to the selection by clicking Shift-LMB with the cursor over the empty (or using Ctrl-LMB in the outliner). The camera should now have a dark orange selection outline, while the empty should have a light orange outline, as the latter is the active object. Press Ctrl-P and pick Object to add a parent-child relationship A black dotted line from the camera to the empty should now be visible in the scene. This means the camera is now parented to the empty. Any transformation you apply to the empty will get applied to the camera as well. Info Bad Parenting : if you made a mistake in the parenting of step 6 then you can clear an object's parent by selecting that object, pressing Alt-P and picking Clear Parent . Verify in the outliner that the Camera object is now indeed a child of the Empty (you might have to use the little white triangles to open the necessary tree entries) Make the empty the single selected object. Enter Z rotation mode by pressing R followed by Z . Note that as you move the mouse both the empty and camera are transformed. Exit the rotation mode with Esc , leaving the Z rotation of the empty set to zero. Add key frames at the beginning and end of the animation to have the empty rotate 360 degrees around Z over the animation period Check the camera orbit, including how it looks in the camera view. Is this orbit better? You might have noticed that, even though we now have a nice circular rotation of the camera around the object, the rotation speed actually isn't constant. If you select the empty and look at the Graph Editor you can see that the graph line representing the Z rotation value isn't straight, but looks like an S. This is due to the default interpolation mode that is used between key frames. To make the rotation speed constant make sure the empty is selected. Then in the Graph Editor select all curve points with A and press V to set the handle type, pick Vector . The curves should now have become straight lines. Check the animation to see the rotation speed has become constant. Depending on how exactly you set up the animation you might notice a hickup at the moment that the animation wraps around from frame 99 to frame 0. This happens in case you set the same visible rotation of the empty for frame 0 and 99 (e.g. 0 degrees for frame 0 and 360 degrees for frame 99). You can fix this by changing the animation length to 99 frames by setting End to 98 in the Output properties panel (the value is directly below Frame Start ). Now, the animation should wrap around smoothly.","title":"\ud83d\udcbb Camera orbiting using parenting"},{"location":"basics/animation/exercise_track_to/","text":"\ud83d\udcbb Track To constraint \u00b6 Load track_to.blend This scene contains two moving cubes and a single camera. We would like to keep the camera pointed at one of the cubes as it moves across the scene. We could animate the camera orientation ourselves, but there is an easier way using a constraint. A constraint operates on an object and can influence things like orientation or scale amount based on another object's properties. We will be using a Track To constraint here, which keeps one object pointing at another object. Select the camera Switch the Properties panel to the Object Constraints tab using the icon In the Add Object Constraint menu pick Track To under Tracking The Track To constraint will keep the object, in this case our camera, oriented at another object all the time. The other object is called the Target object (in this case one of the cubes). In the constraint settings under Target (the top one!) pick Cube If you had the 3D View set to view through the active camera (the view will be named Camera Perspective ) one of the cubes should now be nicely centered in the view. Check that when playing the animation the cube indeed stays centered in the camera view. Orient the 3D view so you can see the camera's orientation in relation to the scene, specifically the targeted cube. There is a blue dotted line indicating the constraint between the camera and the cube. To understand how the Track To constraint works in this case we need to understand the basic orientation of a Blender camera. Add a new Camera ( Shift-A > Camera ) Select it and clear its rotation with Alt-R . Zoom in on the new camera so you can see along which axis it is looking. Also note which axis is the Up direction of the camera (i.e. pointing towards the top of the view as seen by this camera). Select the original camera we wanted to animate and which has the Track To constraint. Change the 3D view so you can see the whole scene, including the selected camera. Change the Track Axis value of the Track To constraint to different values. Also experiment with different values for the Up setting. Compare these settings against what you concluded from step 10.","title":"\ud83d\udcbb Track To constraint"},{"location":"basics/animation/exercise_track_to/#track-to-constraint","text":"Load track_to.blend This scene contains two moving cubes and a single camera. We would like to keep the camera pointed at one of the cubes as it moves across the scene. We could animate the camera orientation ourselves, but there is an easier way using a constraint. A constraint operates on an object and can influence things like orientation or scale amount based on another object's properties. We will be using a Track To constraint here, which keeps one object pointing at another object. Select the camera Switch the Properties panel to the Object Constraints tab using the icon In the Add Object Constraint menu pick Track To under Tracking The Track To constraint will keep the object, in this case our camera, oriented at another object all the time. The other object is called the Target object (in this case one of the cubes). In the constraint settings under Target (the top one!) pick Cube If you had the 3D View set to view through the active camera (the view will be named Camera Perspective ) one of the cubes should now be nicely centered in the view. Check that when playing the animation the cube indeed stays centered in the camera view. Orient the 3D view so you can see the camera's orientation in relation to the scene, specifically the targeted cube. There is a blue dotted line indicating the constraint between the camera and the cube. To understand how the Track To constraint works in this case we need to understand the basic orientation of a Blender camera. Add a new Camera ( Shift-A > Camera ) Select it and clear its rotation with Alt-R . Zoom in on the new camera so you can see along which axis it is looking. Also note which axis is the Up direction of the camera (i.e. pointing towards the top of the view as seen by this camera). Select the original camera we wanted to animate and which has the Track To constraint. Change the 3D view so you can see the whole scene, including the selected camera. Change the Track Axis value of the Track To constraint to different values. Also experiment with different values for the Up setting. Compare these settings against what you concluded from step 10.","title":"\ud83d\udcbb Track To constraint"},{"location":"basics/animation/introduction/","text":"Introduction \u00b6 Animation is a very broad topic and we will only cover a very small part of what is possible in Blender. We'll begin with an introduction into animation and then focus on basic keyframe animation. Summary of basic UI interaction and shortcut keys \u00b6 All (3D View, Timeline, Graph Editor) \u00b6 Shift-Left for moving time to the first frame in the animation, Shift-Right for the last frame Left key for 1 frame back, Right for 1 forward Up key for 1 keyframe forward, Down for 1 back Spacebar for toggling animation playback 3D view \u00b6 I in the 3D view for inserting/updating a keyframe for the current frame (pick the type) Alt-I in the 3D view for deleting the keyframe data for the current frame Timeline \u00b6 Changing current frame (either click or drag): LMB on the row of frame numbers at the top OR Shift-RMB within the full area Change zoom with mouse Wheel , zoom extent with Home LMB click or LMB + drag for selecting keyframes (the yellow diamonds) The usual shortcuts for editing keyframes, e.g. A for selecting all keyframes, X for deleting all selected keyframes, G for grabbing and moving, etc Graph editor \u00b6 Change current frame with Shift-RMB Change zoom with Ctrl-MMB drag, or mouse Wheel Translate with Shift-MMB (same as in 3D view) Zoom graph extent with Home (same as in 3D view) The usual shortcuts for editing curve control points, e.g. A for selecting all, X for deleting all selected points, G for grabbing and moving, etc Tip If one or more curves in the graph editor don't seem to be editable (and they show as dotted lines) then you might have accidentally disabled editing. To fix: with the mouse over the graph editor select all curves with A and press TAB to toggle editability. Further reading \u00b6 This section in the Blender manual contains many more details on keyframing, particularly with respect to the curves in the Graph Editor. The proper definitions of the colors of keyframed values is described here","title":"Introduction"},{"location":"basics/animation/introduction/#introduction","text":"Animation is a very broad topic and we will only cover a very small part of what is possible in Blender. We'll begin with an introduction into animation and then focus on basic keyframe animation.","title":"Introduction"},{"location":"basics/animation/introduction/#summary-of-basic-ui-interaction-and-shortcut-keys","text":"","title":"Summary of basic UI interaction and shortcut keys"},{"location":"basics/animation/introduction/#all-3d-view-timeline-graph-editor","text":"Shift-Left for moving time to the first frame in the animation, Shift-Right for the last frame Left key for 1 frame back, Right for 1 forward Up key for 1 keyframe forward, Down for 1 back Spacebar for toggling animation playback","title":"All (3D View, Timeline, Graph Editor)"},{"location":"basics/animation/introduction/#3d-view","text":"I in the 3D view for inserting/updating a keyframe for the current frame (pick the type) Alt-I in the 3D view for deleting the keyframe data for the current frame","title":"3D view"},{"location":"basics/animation/introduction/#timeline","text":"Changing current frame (either click or drag): LMB on the row of frame numbers at the top OR Shift-RMB within the full area Change zoom with mouse Wheel , zoom extent with Home LMB click or LMB + drag for selecting keyframes (the yellow diamonds) The usual shortcuts for editing keyframes, e.g. A for selecting all keyframes, X for deleting all selected keyframes, G for grabbing and moving, etc","title":"Timeline"},{"location":"basics/animation/introduction/#graph-editor","text":"Change current frame with Shift-RMB Change zoom with Ctrl-MMB drag, or mouse Wheel Translate with Shift-MMB (same as in 3D view) Zoom graph extent with Home (same as in 3D view) The usual shortcuts for editing curve control points, e.g. A for selecting all, X for deleting all selected points, G for grabbing and moving, etc Tip If one or more curves in the graph editor don't seem to be editable (and they show as dotted lines) then you might have accidentally disabled editing. To fix: with the mouse over the graph editor select all curves with A and press TAB to toggle editability.","title":"Graph editor"},{"location":"basics/animation/introduction/#further-reading","text":"This section in the Blender manual contains many more details on keyframing, particularly with respect to the curves in the Graph Editor. The proper definitions of the colors of keyframed values is described here","title":"Further reading"},{"location":"basics/animation/tradeoffs_settings_output/","text":"Trade-offs, settings and output \u00b6 Here, we look into trade-offs that you can make in terms of chosen frame rate, animation length, quality, etc. Secondly, we will look in detail into the different settings available for an animation, including the type of output (images or video file) and strategies to handle long render times. We also describe how to do command-line rendering. Easy command-line rendering \u00b6 If you have set up the animation and its settings (e.g. frame rate, start/end frame, output name, etc) as you like in the Blender file then rendering from the command-line usually doesn't involve anything more than running this command: blender -b file.blend -a The -b option makes sure Blender renders in the background without opening a window. You only need to add extra options if you want to override values set in the Blender file.","title":"Trade-offs, settings and output"},{"location":"basics/animation/tradeoffs_settings_output/#trade-offs-settings-and-output","text":"Here, we look into trade-offs that you can make in terms of chosen frame rate, animation length, quality, etc. Secondly, we will look in detail into the different settings available for an animation, including the type of output (images or video file) and strategies to handle long render times. We also describe how to do command-line rendering.","title":"Trade-offs, settings and output"},{"location":"basics/animation/tradeoffs_settings_output/#easy-command-line-rendering","text":"If you have set up the animation and its settings (e.g. frame rate, start/end frame, output name, etc) as you like in the Blender file then rendering from the command-line usually doesn't involve anything more than running this command: blender -b file.blend -a The -b option makes sure Blender renders in the background without opening a window. You only need to add extra options if you want to override values set in the Blender file.","title":"Easy command-line rendering"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/","text":"\ud83d\udcbb Interaction, selections, outliner \u00b6 Here it's time for a first exercise! Follow the steps given below, which will let you work with Blender yourself and get to know the different methods of 3D scene interaction. Tip Summary of 3D view navigation: MMB = rotate view Scrollwheel or Ctrl+MMB = zoom view Shift+MMB = translate view Home = zoom out to show all objects See the cheat sheet to refresh your memory w.r.t. other view interaction and shortcut keys and mouse actions. Viewpoints \u00b6 Load motorbike.blend In one of the two 3D views (your choice) manipulate the view to the following viewpoints: Alongside the motorbike, amongst the streamlines, looking in the direction of travel. From the rider's point of view, just in front of the helmet, looking ahead. An up-close point of view clearly showing the two streamlines that cross near the rider's helmet on his/her right side, one going under the arm, the other going over it. There is a single streamline that goes between the two rods of the steering column. Does that streamline terminate on the bike or does it continue past the bike? Try to get really close with the view so you can see where the streamline goes. Individual selection \u00b6 Select all objects using the A key. As you've seen earlier this will introduce orange outlines surrounding selected objects. Check the outliner, specifically the color of the object names, to see how the current selection is represented. In the 3D view deselect only the motorbike using Shift-LMB with the mouse cursor at the appropriate position Again check the outliner status, do you notice a difference in the name for the motorbike object? Add the motorbike back to the selection by using Shift-LMB over the bike in the 3D view. Check the orange outline color of the motorbike (or the corresponding entry in the outliner) to verify that it is now the active object. It should be the only object with a light orange color. Use Shift-LMB with the mouse over the \"floor and walls\" object. What changed in the selection? Specifically, what is now the active object? Once more use Shift-LMB on the \"floor ans walls\" object. What changed this time in the selection status of the object? Box selection \u00b6 Clear the selection with Alt-A (or double click the A key). Use box select ( LMB drag) to select all objects in the scene. Clear the selection with the Alt-A key. Now try to select ONLY the motorbike using box select. Check the outliner to make sure you're selecting just one object. You can also check the status line at the bottom of the Blender window, specifically the part that reads Objects: #/# , meaning selected / total. Outliner selection \u00b6 Make sure no objects are currently selected. Test with following actions in the outliner to get a good idea of what actions it supports and how this influences the visual state of the items in the outliner tree: Left-clicking on an item (possibly holding the Shift or Ctrl key) Using the keys A and Alt-A (note how these are similar in functionality to what they do in the 3D view, but in the context of the outliner items) Right-clicking on an item and choosing Select or Deselect How does the blue highlight of a line in the outliner relate to the selection status of an object in the 3D view?","title":"\ud83d\udcbb Interaction, selections, outliner"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#interaction-selections-outliner","text":"Here it's time for a first exercise! Follow the steps given below, which will let you work with Blender yourself and get to know the different methods of 3D scene interaction. Tip Summary of 3D view navigation: MMB = rotate view Scrollwheel or Ctrl+MMB = zoom view Shift+MMB = translate view Home = zoom out to show all objects See the cheat sheet to refresh your memory w.r.t. other view interaction and shortcut keys and mouse actions.","title":"\ud83d\udcbb Interaction, selections, outliner"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#viewpoints","text":"Load motorbike.blend In one of the two 3D views (your choice) manipulate the view to the following viewpoints: Alongside the motorbike, amongst the streamlines, looking in the direction of travel. From the rider's point of view, just in front of the helmet, looking ahead. An up-close point of view clearly showing the two streamlines that cross near the rider's helmet on his/her right side, one going under the arm, the other going over it. There is a single streamline that goes between the two rods of the steering column. Does that streamline terminate on the bike or does it continue past the bike? Try to get really close with the view so you can see where the streamline goes.","title":"Viewpoints"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#individual-selection","text":"Select all objects using the A key. As you've seen earlier this will introduce orange outlines surrounding selected objects. Check the outliner, specifically the color of the object names, to see how the current selection is represented. In the 3D view deselect only the motorbike using Shift-LMB with the mouse cursor at the appropriate position Again check the outliner status, do you notice a difference in the name for the motorbike object? Add the motorbike back to the selection by using Shift-LMB over the bike in the 3D view. Check the orange outline color of the motorbike (or the corresponding entry in the outliner) to verify that it is now the active object. It should be the only object with a light orange color. Use Shift-LMB with the mouse over the \"floor and walls\" object. What changed in the selection? Specifically, what is now the active object? Once more use Shift-LMB on the \"floor ans walls\" object. What changed this time in the selection status of the object?","title":"Individual selection"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#box-selection","text":"Clear the selection with Alt-A (or double click the A key). Use box select ( LMB drag) to select all objects in the scene. Clear the selection with the Alt-A key. Now try to select ONLY the motorbike using box select. Check the outliner to make sure you're selecting just one object. You can also check the status line at the bottom of the Blender window, specifically the part that reads Objects: #/# , meaning selected / total.","title":"Box selection"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#outliner-selection","text":"Make sure no objects are currently selected. Test with following actions in the outliner to get a good idea of what actions it supports and how this influences the visual state of the items in the outliner tree: Left-clicking on an item (possibly holding the Shift or Ctrl key) Using the keys A and Alt-A (note how these are similar in functionality to what they do in the 3D view, but in the context of the outliner items) Right-clicking on an item and choosing Select or Deselect How does the blue highlight of a line in the outliner relate to the selection status of an object in the 3D view?","title":"Outliner selection"},{"location":"basics/blender_fundamentals/2_assignment_transformations/","text":"\ud83d\udcbb Transformations \u00b6 Hint You can clear an object's translation to all zero with Alt-G You can clear an object's rotation to all zero with Alt-R You can clear an object's scale to all zero with Alt-S You can undo a transformation with Ctrl-Z (or reload the file to reset completely) See section Object Actions of the cheat sheet for more shortcut keys Basic transformations \u00b6 Load axes.blend The Axes object in the scene is a 3D object just like any other. Note that the axes object shows the local axes of the object. Try translating, rotating and scaling the axes object with the different methods shown: The transform widgets (accessible from the toolbox on the upper-left) Using the G , R or S keys Entering values in the properties region in the upper-right of the view, under Transform Activate one of the transform modes (e.g. G for translation) and experiment with limiting a transformation to an axis with X , Y or Z keys, Activate one of the transform modes (e.g. G for translation) and experiment with limiting a transformation to a plane with Shift-X , Shift-Y or Shift-Z . Reload the axes.blend file to get back the original scene. Rotate the axes 30 degrees around (global) X. Now rotate the axes 45 degrees around the local Z axis. Pivot point modes \u00b6 Load transformations.blend Select the cone, monkey, torus and sphere Set pivot mode to Median Point (using the Pivot Point pie menu, which opens with the . key, i.e. period), if it isn't already. Press S to start scaling, then move the mouse to scale the objects apart Notice that as you scale up the objects increase in size and move apart, but only the torus' center point (the orange dot) moves below the plane. Why? Cancel the scale operation with Esc or a RMB click Enable the Only Locations option in the Pivot Point pie menu. When this is enabled it will cause any transformation to be applied to the locations of the objects (shown as orange circles), instead of to the objects themselves. Repeat the scaling of the four objects. Do you notice how the objects now transform differently? Change the pivot mode to Individual Origins and disable the Only Locations option. Do the scaling again, notice the difference. Enable the Only Locations setting. When you try to rotate the objects around Z nothing happens. Why not? Change the pivot mode to Median Point , leave Only Locations enabled. Rotate the objects around the Z axis. Now disable the Only Locations option and rotate the objects once again around the Z axis. Do you notice the subtle difference in transformation? Experiment some more with different selections of objects and the different Pivot Point modes, until you feel you get the hang of it. Rubik's cube \u00b6 Hint You can add a cube object with Shift-A > Mesh > Cube You can duplicate selected objects with Shift-D . This will also activate grab mode after the duplication. Start with an empty scene ( File > New > General ) Model a Rubik's cube: 3x3x3 Cube objects (minus the center cube) on a rectangular grid. Try to get the spacing between the Cube objects the same in all directions. Now select one face of the Rubik's cube (i.e. 3x3 cubes) and rotate it 30 degrees just like the real thing. Bonus: 2001 - A Space Odyssey \u00b6 Start with an empty scene ( File > New > General ) Remember the scene from Space Odyssey 2001, with our primate ancestors looking up at the monolith? Recreate that scene :) Add 4 or more monkey heads, surrounding a thin narrow box for the monolith Make the monkeys look up at the monolith If you want to go crazy add bodies to the monkeys using some scaled spheres Add a sun object + corresponding light somewhere in the sky.","title":"\ud83d\udcbb Transformations"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#transformations","text":"Hint You can clear an object's translation to all zero with Alt-G You can clear an object's rotation to all zero with Alt-R You can clear an object's scale to all zero with Alt-S You can undo a transformation with Ctrl-Z (or reload the file to reset completely) See section Object Actions of the cheat sheet for more shortcut keys","title":"\ud83d\udcbb Transformations"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#basic-transformations","text":"Load axes.blend The Axes object in the scene is a 3D object just like any other. Note that the axes object shows the local axes of the object. Try translating, rotating and scaling the axes object with the different methods shown: The transform widgets (accessible from the toolbox on the upper-left) Using the G , R or S keys Entering values in the properties region in the upper-right of the view, under Transform Activate one of the transform modes (e.g. G for translation) and experiment with limiting a transformation to an axis with X , Y or Z keys, Activate one of the transform modes (e.g. G for translation) and experiment with limiting a transformation to a plane with Shift-X , Shift-Y or Shift-Z . Reload the axes.blend file to get back the original scene. Rotate the axes 30 degrees around (global) X. Now rotate the axes 45 degrees around the local Z axis.","title":"Basic transformations"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#pivot-point-modes","text":"Load transformations.blend Select the cone, monkey, torus and sphere Set pivot mode to Median Point (using the Pivot Point pie menu, which opens with the . key, i.e. period), if it isn't already. Press S to start scaling, then move the mouse to scale the objects apart Notice that as you scale up the objects increase in size and move apart, but only the torus' center point (the orange dot) moves below the plane. Why? Cancel the scale operation with Esc or a RMB click Enable the Only Locations option in the Pivot Point pie menu. When this is enabled it will cause any transformation to be applied to the locations of the objects (shown as orange circles), instead of to the objects themselves. Repeat the scaling of the four objects. Do you notice how the objects now transform differently? Change the pivot mode to Individual Origins and disable the Only Locations option. Do the scaling again, notice the difference. Enable the Only Locations setting. When you try to rotate the objects around Z nothing happens. Why not? Change the pivot mode to Median Point , leave Only Locations enabled. Rotate the objects around the Z axis. Now disable the Only Locations option and rotate the objects once again around the Z axis. Do you notice the subtle difference in transformation? Experiment some more with different selections of objects and the different Pivot Point modes, until you feel you get the hang of it.","title":"Pivot point modes"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#rubiks-cube","text":"Hint You can add a cube object with Shift-A > Mesh > Cube You can duplicate selected objects with Shift-D . This will also activate grab mode after the duplication. Start with an empty scene ( File > New > General ) Model a Rubik's cube: 3x3x3 Cube objects (minus the center cube) on a rectangular grid. Try to get the spacing between the Cube objects the same in all directions. Now select one face of the Rubik's cube (i.e. 3x3 cubes) and rotate it 30 degrees just like the real thing.","title":"Rubik's cube"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#bonus-2001-a-space-odyssey","text":"Start with an empty scene ( File > New > General ) Remember the scene from Space Odyssey 2001, with our primate ancestors looking up at the monolith? Recreate that scene :) Add 4 or more monkey heads, surrounding a thin narrow box for the monolith Make the monkeys look up at the monolith If you want to go crazy add bodies to the monkeys using some scaled spheres Add a sun object + corresponding light somewhere in the sky.","title":"Bonus: 2001 - A Space Odyssey"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/","text":"\ud83d\udcbb Cameras and views \u00b6 Open cameras.blend This scene contains a bunny object, a sun light and two cameras: \"Close-up\" near the bunny's head and \"Overview\" further away. Select the Overview camera object, by either left-clicking on it in the 3D view or in the Outliner. Make this camera the active camera with either the outliner (click on the green camera icon right of the name), View > Cameras > Set Active Object as Camera or use Ctrl-Numpad0 . Notice that the 3D view changes to the camera's viewpoint. Rotate the 3D view with MMB to exit the camera view. You are now back in the normal 3D view interaction. Select the Close-up camera Switch to camera view by bringing up the View pie menu with ` (backtick, usually below the ~ ), then pick View Camera . What camera view are you now seeing, Close-up or Overview? So one thing to remember is that selecting a camera does not make it the \"active camera\" (even though it can be the active object , confusingly). Change the active camera to Close-up Rotate away from the camera view to the normal 3D view For switching back to the active camera view there's two more methods apart from the pie menu, try them: Using the View menu at the top of the 3D view area: View > Cameras > Active Camera Press Numpad0 Experiment with the different camera controls until you find the ones you're comfortable with Rotate away from the camera view to a 3D view that shows both cameras. In the Scene properties tab on the right-hand side of the window (and not the similar icon in the top bar left of Scene ) there's a drop-down menu Camera which lists the active camera. Change the active camera using that selection box. Apart from the name listed under the Scene properties do you notice how you can identify the active camera in the 3D view? Hint: it's subtle and unrelated to the yellow/orange color used for highlighting selected objects. Camera transformation \u00b6 Make sure the Overview camera object is the only selected object Make the Overview camera the active camera and then switch to its view In the camera view use regular object transformations to point the camera at the rabbit's tail. To refresh, in camera view with only the camera selected : Press G to translate, then move the mouse to change the view While still in move mode press MMB (or Z twice) to enter \"truck\" mode: this moves the camera forward/backward along the view axis. Pressing X twice will allow moving the camera sideways. Press R to rotate around the view axis In rotate mode press MMB to \"look around\" LMB to confirm, Esc to cancel Another useful feature is when you like the current viewpoint in the 3D view and want to match the active camera to this viewpoint. For this you can use Ctrl-Alt-Numpad0 (or with View > Align View > Align Active Camera To View in the header of the 3D view) Quad view \u00b6 Switch the 3D View to the so-called Quad View with Ctrl-Alt-Q . You now have orthogonal 2D views along the three axes (Top, Front and Right Orthographic), plus a 3D view (Camera Perspective). Note: the three axis views can only be translated and zoomed, not rotated Change the upper-right quad to a camera view, if it isn't already Press N to show the sidebar on the right On the View tab, under View Lock there's a Lock option called Camera to View . Enable that option. You should now see a dotted red outline around the orange camera rectangle in the Camera Perspective view. Hide the sidebar again ( N ), leaving the Lock option enabled Change the view in the Camera Perspective view using the regular 3D view mouse interaction ( MMB to rotate, Shift-MMB to translate, Ctrl-MMB to move forward/backward). Observe what happens to the active camera in the other quadrants when you alter the view. Use the sidebar again to disable the Lock Camera to View option Fly mode \u00b6 Add a camera to the scene ( Shift-A > Camera ). It will be placed at the position of the 3D cursor (the small red-white striped circle and axes). Change the upper-right view to this camera Activate fly mode with Shift-` (backtick). Use the ASDWXQE keys to move this camera close to the two bunny ears and look between them. You can change the fly speed with the mouse Wheel . In fly mode you can confirm the current view with LMB or press Enter . Press Esc to cancel and go back to the original view.","title":"\ud83d\udcbb Cameras and views"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/#cameras-and-views","text":"Open cameras.blend This scene contains a bunny object, a sun light and two cameras: \"Close-up\" near the bunny's head and \"Overview\" further away. Select the Overview camera object, by either left-clicking on it in the 3D view or in the Outliner. Make this camera the active camera with either the outliner (click on the green camera icon right of the name), View > Cameras > Set Active Object as Camera or use Ctrl-Numpad0 . Notice that the 3D view changes to the camera's viewpoint. Rotate the 3D view with MMB to exit the camera view. You are now back in the normal 3D view interaction. Select the Close-up camera Switch to camera view by bringing up the View pie menu with ` (backtick, usually below the ~ ), then pick View Camera . What camera view are you now seeing, Close-up or Overview? So one thing to remember is that selecting a camera does not make it the \"active camera\" (even though it can be the active object , confusingly). Change the active camera to Close-up Rotate away from the camera view to the normal 3D view For switching back to the active camera view there's two more methods apart from the pie menu, try them: Using the View menu at the top of the 3D view area: View > Cameras > Active Camera Press Numpad0 Experiment with the different camera controls until you find the ones you're comfortable with Rotate away from the camera view to a 3D view that shows both cameras. In the Scene properties tab on the right-hand side of the window (and not the similar icon in the top bar left of Scene ) there's a drop-down menu Camera which lists the active camera. Change the active camera using that selection box. Apart from the name listed under the Scene properties do you notice how you can identify the active camera in the 3D view? Hint: it's subtle and unrelated to the yellow/orange color used for highlighting selected objects.","title":"\ud83d\udcbb Cameras and views"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/#camera-transformation","text":"Make sure the Overview camera object is the only selected object Make the Overview camera the active camera and then switch to its view In the camera view use regular object transformations to point the camera at the rabbit's tail. To refresh, in camera view with only the camera selected : Press G to translate, then move the mouse to change the view While still in move mode press MMB (or Z twice) to enter \"truck\" mode: this moves the camera forward/backward along the view axis. Pressing X twice will allow moving the camera sideways. Press R to rotate around the view axis In rotate mode press MMB to \"look around\" LMB to confirm, Esc to cancel Another useful feature is when you like the current viewpoint in the 3D view and want to match the active camera to this viewpoint. For this you can use Ctrl-Alt-Numpad0 (or with View > Align View > Align Active Camera To View in the header of the 3D view)","title":"Camera transformation"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/#quad-view","text":"Switch the 3D View to the so-called Quad View with Ctrl-Alt-Q . You now have orthogonal 2D views along the three axes (Top, Front and Right Orthographic), plus a 3D view (Camera Perspective). Note: the three axis views can only be translated and zoomed, not rotated Change the upper-right quad to a camera view, if it isn't already Press N to show the sidebar on the right On the View tab, under View Lock there's a Lock option called Camera to View . Enable that option. You should now see a dotted red outline around the orange camera rectangle in the Camera Perspective view. Hide the sidebar again ( N ), leaving the Lock option enabled Change the view in the Camera Perspective view using the regular 3D view mouse interaction ( MMB to rotate, Shift-MMB to translate, Ctrl-MMB to move forward/backward). Observe what happens to the active camera in the other quadrants when you alter the view. Use the sidebar again to disable the Lock Camera to View option","title":"Quad view"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/#fly-mode","text":"Add a camera to the scene ( Shift-A > Camera ). It will be placed at the position of the 3D cursor (the small red-white striped circle and axes). Change the upper-right view to this camera Activate fly mode with Shift-` (backtick). Use the ASDWXQE keys to move this camera close to the two bunny ears and look between them. You can change the fly speed with the mouse Wheel . In fly mode you can confirm the current view with LMB or press Enter . Press Esc to cancel and go back to the original view.","title":"Fly mode"},{"location":"basics/blender_fundamentals/4_assignment_collections/","text":"\ud83d\udcbb Collections \u00b6 Open collections.blend This file contains four objects in three collections. Check the outliner: the plane is in all collections, the other three objects area each in only one collection. You can change which collections are visible using the 1 , 2 , ... keys. Switch among the three collections using the numeric keys, showing only the objects in one specific collection. Check the outliner, specifically the eye icons next to each collection and object, for how it updates when switching collections. Try toggling visibility of individual collections use Shift-1 , etc. Show all collections using the numeric keys Visibility control can also be done in the outliner by clicking on the eye icons right of the collection and object entries. Toggle the eye icon of Collection 1 by left-clicking on it. Verify that this does the same as Shift-1 in the 3D view. Make all collections visible again Moving objects between collections can be done in two ways: Select the cube object, move it to collection 3 using M and picking Collection 3 . Another way is drag the object in the outliner. Drag the Cube object back to Collection 2 . Adding a collection can also be useful: Create a new collection using RMB on the Scene Collection in the outliner, pick New Collection . Another method is to select the object to place in the new collection and use the M key in the 3D view and pick + New Collection . As we noted initially the plane object is present in multiple collections. You can place an object to multiple collection using the outliner by linking the object to the collection: Link the Suzanne object (the monkey head) to the newly created Collection 4 in the outliner by clicking with LMB on the Suzanne entry and holding the mouse , then drag the entry over to Collection 4 , press Ctrl and finally release the mouse and Ctrl key. Add an object to the scene, say a torus mesh. In which collection is it placed? Can you figure how to see to which collection new objects are added? To try figure out how to change this active collection. Hint: use a specific action in the outliner. Also check how the numeric keys 1 , 2 , ... in the 3D view affect the active collection (and what the difference is with using the outliner). Bonus: 14: Can you explain what a Shift-LMB click on the eye icon next to a collection does?","title":"\ud83d\udcbb Collections"},{"location":"basics/blender_fundamentals/4_assignment_collections/#collections","text":"Open collections.blend This file contains four objects in three collections. Check the outliner: the plane is in all collections, the other three objects area each in only one collection. You can change which collections are visible using the 1 , 2 , ... keys. Switch among the three collections using the numeric keys, showing only the objects in one specific collection. Check the outliner, specifically the eye icons next to each collection and object, for how it updates when switching collections. Try toggling visibility of individual collections use Shift-1 , etc. Show all collections using the numeric keys Visibility control can also be done in the outliner by clicking on the eye icons right of the collection and object entries. Toggle the eye icon of Collection 1 by left-clicking on it. Verify that this does the same as Shift-1 in the 3D view. Make all collections visible again Moving objects between collections can be done in two ways: Select the cube object, move it to collection 3 using M and picking Collection 3 . Another way is drag the object in the outliner. Drag the Cube object back to Collection 2 . Adding a collection can also be useful: Create a new collection using RMB on the Scene Collection in the outliner, pick New Collection . Another method is to select the object to place in the new collection and use the M key in the 3D view and pick + New Collection . As we noted initially the plane object is present in multiple collections. You can place an object to multiple collection using the outliner by linking the object to the collection: Link the Suzanne object (the monkey head) to the newly created Collection 4 in the outliner by clicking with LMB on the Suzanne entry and holding the mouse , then drag the entry over to Collection 4 , press Ctrl and finally release the mouse and Ctrl key. Add an object to the scene, say a torus mesh. In which collection is it placed? Can you figure how to see to which collection new objects are added? To try figure out how to change this active collection. Hint: use a specific action in the outliner. Also check how the numeric keys 1 , 2 , ... in the 3D view affect the active collection (and what the difference is with using the outliner). Bonus: 14: Can you explain what a Shift-LMB click on the eye icon next to a collection does?","title":"\ud83d\udcbb Collections"},{"location":"basics/blender_fundamentals/avoiding_data_loss/","text":"\u26a0\ufe0f Avoiding data loss \u00b6 There are some things to be aware of when working with Blender that might behave a little different from other programs, or general expectations, and that can cause you to loose work. The file overwrite prompt is very subtle \u00b6 Suppose we have saved our work to a file scene.blend . We then make some more changes in Blender to create a second version of our scene and save this as scene2.blend . Finally, we make a third version and intend to save this as scene3.blend , but we forget to change the file name in the save dialog and it stays at the current scene2.blend . The Blender way of warning you that you are about to overwrite an existing file is really subtle: Notice the red color behind the file name? That's the signal that the file name you entered is the same as an existing file in the current directory. If we change the file name to something that doesn't exist yet the color becomes gray again: The File > Save As workflow (and similar for related file dialogs) is a somewhat double-edged sword: If you're aware of the above signal and want to quickly overwrite the current file you can simply press Enter once in the dialog, and the file will be saved with no \"Overwriting are you sure?\" prompt is shown. So in this respect the UI stays out of your way and avoids an extra confirmation dialog. But if you miss the red prompt or are unaware of its meaning then it's easy to accidentally overwrite existing work. Easy file versions \u00b6 A nice way to save to successive versions of a file is using the + button right of the file name, as shown in the pictures above. Using the + (and - ) you can easily change the version number at the end of a file name, e.g. scene2.blend to scene3.blend . The red verwrite indicator will also update depending on the existence of the chosen file name. Retrieving lost work \u00b6 There are several layers of defense in case something goes unexpectedly wrong when saving files, or in case Blender crashes. It depends on the situation you're trying to recover from which one of these options provides the best results, if applicable. Please check what each of these features does before deciding how to recover, to make sure you don't accidentally make things worse by using one of the recover options within Blender in the wrong way. .blend1 files? \u00b6 You might notice that when you overwrite an existing file, say file.blend , another file called file.blend1 will now have appeared next to it in the same directory. This is Blender's method for automatically keeping around the previous version of the file you overwrote: it first moves the existing file.blend to file.blend1 , and only then saves the new file.blend . So if you accidentally overwrite a file you can still get to the previous version (i.e. the .blend1 file), as long as you haven't overwritten more than once . More than 1 previous version You can actually have multiple previous versions kept around if you like. The preference setting for this is Save & Load > Save Versions , which defaults to 1. If you would increase it then files with extensions .blend2, .blend3 and so on would be kept around. Auto save \u00b6 By default, Blender will automatically save your current scene to a file in a temporary directory every few minutes (2 minutes by default). The settings that control this are Save & Load > Auto Save and Save & Load > Auto Save > Timer (Minutes) . This auto-save file is stored in your system's temporary directory , and uses the process ID of Blender in the file name, as well as the string _autosave . Here is an example from a Linux system, where /tmp is used and Blender's process ID is 66597: melis@juggle 22:13:/tmp$ ps aux | grep blender melis 66597 1.2 5.7 1838680 463920 ? Sl 21:54 0:14 blender melis@juggle 22:13:/tmp$ ls 66597* 66597_autosave.blend See this section of the Blender manual on recovering a session from an auto-save file from the File manual (you can also copy or load the file manually, of course, there is nothing special about it). Edit mode data not saved If you happend to be in edit (or sculpt) mode at the time Blender does an auto-save, then the current updated state of the mesh will not get saved. This is a limitation of the auto-save feature. Last session \u00b6 Whenever Blender quits normally (i.e. not a crash) it will save the current scene to a file called quit.blend in your system's temporary directory. You can load this file quickly with the File > Recover > Last Session option. Note that there currently is no option to disable this Save-on-Quit feature. So for large scenes this will incur a (usually short) delay when exiting. Blender crash \u00b6 In case Blender crashes it usually does not manage to save the current scene to a recover file. So in this case you are hopefully able to recover any lost work using the data available saved through the features described above. Unused elements in the scene are not saved \u00b6 Suppose you have a 3D scene and have created a material A that you use on some object. You then create a material B and assign it to that object, causing material A to now be unused in the scene. If you save your scene to file at this point material A will not get saved to the file , as it is not referenced by anything in the scene. This automatic \"garbage collection\" feature of Blender is somewhat controversial, and it is definitely good to be aware of this behaviour. For most scene elements used in the Basics part of this course garbage-collection-on-save does not really interfere, except for the case of Materials (as described in the example above). For materials, and other scene elements, you can see if they are unused by checking for a 0 next to their name when they appear in a list: The quick fix in case you have a material that is currently not used in the scene, but that you definitely want to have saved to file, is to use the \"Fake User\" option by clicking the shield icon ( be sure to enable this option for the right material! ): Note that you can use the same Fake User option for some other types of scene elements as well. You can verify the intended material now has a fake user by checking for an F next to its name: We have a more detailed discussion of the garbage collection system in the Python API module of the advanced course, as the behaviour relates to the data block system that Blender uses internally, which is out of scope for this Basics course. However, it's good to be aware of the above behaviour.","title":"\u26a0\ufe0f Avoiding data loss"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#avoiding-data-loss","text":"There are some things to be aware of when working with Blender that might behave a little different from other programs, or general expectations, and that can cause you to loose work.","title":"\u26a0\ufe0f Avoiding data loss"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#the-file-overwrite-prompt-is-very-subtle","text":"Suppose we have saved our work to a file scene.blend . We then make some more changes in Blender to create a second version of our scene and save this as scene2.blend . Finally, we make a third version and intend to save this as scene3.blend , but we forget to change the file name in the save dialog and it stays at the current scene2.blend . The Blender way of warning you that you are about to overwrite an existing file is really subtle: Notice the red color behind the file name? That's the signal that the file name you entered is the same as an existing file in the current directory. If we change the file name to something that doesn't exist yet the color becomes gray again: The File > Save As workflow (and similar for related file dialogs) is a somewhat double-edged sword: If you're aware of the above signal and want to quickly overwrite the current file you can simply press Enter once in the dialog, and the file will be saved with no \"Overwriting are you sure?\" prompt is shown. So in this respect the UI stays out of your way and avoids an extra confirmation dialog. But if you miss the red prompt or are unaware of its meaning then it's easy to accidentally overwrite existing work.","title":"The file overwrite prompt is very subtle"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#easy-file-versions","text":"A nice way to save to successive versions of a file is using the + button right of the file name, as shown in the pictures above. Using the + (and - ) you can easily change the version number at the end of a file name, e.g. scene2.blend to scene3.blend . The red verwrite indicator will also update depending on the existence of the chosen file name.","title":"Easy file versions"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#retrieving-lost-work","text":"There are several layers of defense in case something goes unexpectedly wrong when saving files, or in case Blender crashes. It depends on the situation you're trying to recover from which one of these options provides the best results, if applicable. Please check what each of these features does before deciding how to recover, to make sure you don't accidentally make things worse by using one of the recover options within Blender in the wrong way.","title":"Retrieving lost work"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#blend1-files","text":"You might notice that when you overwrite an existing file, say file.blend , another file called file.blend1 will now have appeared next to it in the same directory. This is Blender's method for automatically keeping around the previous version of the file you overwrote: it first moves the existing file.blend to file.blend1 , and only then saves the new file.blend . So if you accidentally overwrite a file you can still get to the previous version (i.e. the .blend1 file), as long as you haven't overwritten more than once . More than 1 previous version You can actually have multiple previous versions kept around if you like. The preference setting for this is Save & Load > Save Versions , which defaults to 1. If you would increase it then files with extensions .blend2, .blend3 and so on would be kept around.","title":".blend1 files?"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#auto-save","text":"By default, Blender will automatically save your current scene to a file in a temporary directory every few minutes (2 minutes by default). The settings that control this are Save & Load > Auto Save and Save & Load > Auto Save > Timer (Minutes) . This auto-save file is stored in your system's temporary directory , and uses the process ID of Blender in the file name, as well as the string _autosave . Here is an example from a Linux system, where /tmp is used and Blender's process ID is 66597: melis@juggle 22:13:/tmp$ ps aux | grep blender melis 66597 1.2 5.7 1838680 463920 ? Sl 21:54 0:14 blender melis@juggle 22:13:/tmp$ ls 66597* 66597_autosave.blend See this section of the Blender manual on recovering a session from an auto-save file from the File manual (you can also copy or load the file manually, of course, there is nothing special about it). Edit mode data not saved If you happend to be in edit (or sculpt) mode at the time Blender does an auto-save, then the current updated state of the mesh will not get saved. This is a limitation of the auto-save feature.","title":"Auto save"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#last-session","text":"Whenever Blender quits normally (i.e. not a crash) it will save the current scene to a file called quit.blend in your system's temporary directory. You can load this file quickly with the File > Recover > Last Session option. Note that there currently is no option to disable this Save-on-Quit feature. So for large scenes this will incur a (usually short) delay when exiting.","title":"Last session"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#blender-crash","text":"In case Blender crashes it usually does not manage to save the current scene to a recover file. So in this case you are hopefully able to recover any lost work using the data available saved through the features described above.","title":"Blender crash"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#unused-elements-in-the-scene-are-not-saved","text":"Suppose you have a 3D scene and have created a material A that you use on some object. You then create a material B and assign it to that object, causing material A to now be unused in the scene. If you save your scene to file at this point material A will not get saved to the file , as it is not referenced by anything in the scene. This automatic \"garbage collection\" feature of Blender is somewhat controversial, and it is definitely good to be aware of this behaviour. For most scene elements used in the Basics part of this course garbage-collection-on-save does not really interfere, except for the case of Materials (as described in the example above). For materials, and other scene elements, you can see if they are unused by checking for a 0 next to their name when they appear in a list: The quick fix in case you have a material that is currently not used in the scene, but that you definitely want to have saved to file, is to use the \"Fake User\" option by clicking the shield icon ( be sure to enable this option for the right material! ): Note that you can use the same Fake User option for some other types of scene elements as well. You can verify the intended material now has a fake user by checking for an F next to its name: We have a more detailed discussion of the garbage collection system in the Python API module of the advanced course, as the behaviour relates to the data block system that Blender uses internally, which is out of scope for this Basics course. However, it's good to be aware of the above behaviour.","title":"Unused elements in the scene are not saved"},{"location":"basics/blender_fundamentals/cameras_and_views/","text":"Cameras and views \u00b6 This section shows cameras and how to work with them. In the exercise after this section you get to try a lot of the operations shown, so following the video along isn't strictly needed. But if you do want to then the file used is data/blender_basics/cameras.blend .","title":"Cameras and views"},{"location":"basics/blender_fundamentals/cameras_and_views/#cameras-and-views","text":"This section shows cameras and how to work with them. In the exercise after this section you get to try a lot of the operations shown, so following the video along isn't strictly needed. But if you do want to then the file used is data/blender_basics/cameras.blend .","title":"Cameras and views"},{"location":"basics/blender_fundamentals/first_steps/","text":"First steps in the user interface \u00b6 Hint A lot of new concepts and UI elements will be introduced in the upcoming videos. It probably works best to watch video(s) limited to a certain topic, try the operations shown and corresponding exercise(s) in Blender yourself, before moving on to the next topic. Starting Blender \u00b6 In general Blender isn't different to start than any other GUI application. However, warning and error messages will be printed to the console window. It depends on the operating system you're working on how the console window is available: (All operating systems) If you start Blender from a terminal window, e.g. xterm or Windows Command window, then Blender output will be printed in that window (Windows only) If Blender was started from the Start menu, or using a desktop icon, then you can toggle the associated console window using the Window > Toggle System Console option See this section in the Blender manual for more details on starting Blender from the command line and details specific for each operating system. User interface fundamentals \u00b6 We will go over fundamentals of the user interface in terms of interaction and areas, specifically the 3D view and Outliner. We also touch on a number of often-performed operations, such as rendering an image and changing the set of selected objects. We also look a bit closer into keyboard shortcuts and menus. It's probably best to follow along in Blender on your own system while viewing the video. The files used in the video can be found under data/blender_basics . Slow 3D interaction If the interaction in the 3D view isn't smooth (as seen in the video) on your PC/laptop something might be wrong in the setup of your system. Please contact us if this appears to be the case. Accidental 'Edit mode' If the 3D view (or some of the other areas) suddenly appear to behave strangely, or you now see your mesh with all kinds of dots or lines then you might have accidentally entered the so-called \"Edit Mode\" or any of the other modes available ( Tab and Ctrl-Tab are used for this). Check the menu in the upper left of the 3D view, which should read Object Mode : In this course we will use only Object Mode (and briefly use Vertex Paint mode in one of the exercises). You can use the drop-down menu shown above (or the Ctrl-Tab menu in the 3D view) and pick Object Mode to get back to the correct mode. Accidental workspace switch Another thing that might happen is that you accidentally click one of the tabs at the top of the screen, which then completely changes the layout of your user interface. These tabs are used to switch between workspaces, where each workspace allows a different layout to focus on a certain task (e.g. 3D modeling, versus shader editing, versus animation). The default workspace is Layout and you might have to switch back to that one: Some user interface tips \u00b6 To bring up the relevant section of the official Blender manual for (almost) any user interface element, e.g. button, setting or menu, right-click on that element and click Online Manual . This will start a web browser showing the relevant manual page. You can hover with the mouse over pretty much any UI element to get a tooltip with a short description, including shortcut key(s) if available. The keyboard and mouse shortcuts for object selection, editing, view interaction, etc work mostly the same in all Blender editors. So G to grab, X to delete, LMB to select, Shift-MMB to translate, Wheel to zoom, etc. The mouse controls the current area in focus and any keyboard actions are applied in the active area first. You can maximize a user interface area by using Ctrl+Spacebar having the mouse in the area you want to maximize. This can sometimes be useful to temporarily get a larger area to work with. You can use the same shortcut to toggle the area back to its original size, or use the Back to Previous button at the top of the screen. Changes to default preference settings \u00b6 Here we suggest some preferences settings to change from their default value. Info It's not required to change these defaults, but we find they help us in working with Blender, and so might be useful for you as well Under Edit > Preferences , in the Interface tab: Under Display disable Splash Screen . This will save you a click to get rid of the splash screen each time you start Blender. If you ever want to look at the splash again you can use the Blender logo icon in the top-level of the window and use Splash Screen . Under Editors > Temporary Editors set Render In to Image Editor . This will cause the rendered image to be displayed as a replacement of the 3D view, instead of in a separate window. After rendering press Escape to get back the 3D view that was replaced by the rendered output. Under Editors > Status Bar enable Scene Statistics , System Memory and Video Memory . This will show extra scene statistics in the status bar. Another way to do this is to right-click on the status bar and enable the same options. In case you find that Blender's user interface elements, such as buttons or menu text, are too small you can scale up the UI with a single setting under Display > Resolution Scale . If you change the value you can see the changes in the UI immediately. Orbit around selection Another option which you might consider enabling is Orbit Around Selection . By default this is turned off and in that mode any rotation of the 3D viewport will be around the center of the view, which might cause selected objects to go out of view. When the option is turned on viewport rotation will be around the selected object(s), always keeping them in view. You can find this option on the Navigation tab under Orbit & Pan . Enabling GPU-based rendering in Cycles \u00b6 In general GPU-based rendering with Cycles is a lot faster than rendering on a multi-core CPU. However, only by making a comparison on your particular system can you really find out if GPU rendering is a good option for you (for example, you might not have a very powerful GPU in your laptop). Apart from performance there are some other aspects to consider with GPU rendering: When doing a GPU render your desktop environment might become less responsive, although this has become less of a problem with recent Blender versions A GPU usually has less memory available, which might cause problems with really large scenes In case you want to try enabling GPU rendering go to the Preferences window ( Edit > Preferences ) and then the System tab. The settings available under Cycles Render Device are somewhat dependent on the hardware in your system but should look a little like this: GPU rendering in Blender has slightly different support depending on whether you're on Windows, Linux or macOS. The most up-to-date official reference for the information below is this page from the Blender manual, but below we summarize the options you can encounter. Windows, Linux \u00b6 By default None will be active, meaning no GPU acceleration is used for rendering and it all happens on the CPU. In general, on a PC/Laptop with an NVIDIA GPU the CUDA option is to be preferred, although OptiX might work well as an alternative (but will only be available on newer GPUs). On Windows systems with an AMD GPU the option HIP might be available and is then definitely worth a try. macOS \u00b6 For macOS systems only the Metal option will be available, apart from the default None . In Blender 3.1 the GPU rendering support in Cycles is based on the Metal API, which is not supported on all macOS systems (also depending on the system version). Only for the following two configurations GPU rendering support currently available: Apple M1 computers running macOS 12.2 or newer Apple computers with AMD graphics cards running macOS 12.3 or newer Warning The Blender 3.1 release notes warn that the GPU rendering implementation is in an early state. Performance optimizations and support for Intel GPUs are under development. Info This section is about GPU rendering in Cycles, which is different from GPU acceleration for the Blender user interface and EEVEE (see below) rendering. So even though your macOS system might not provide GPU rendering in Cycles, it might still work fine for Blender usage with CPU-based rendering. A thing called EEVEE? \u00b6 When consulting other Blender materials, specifically on rendering, you may see references to EEVEE. This is another render engine available in Blender, which is different from the Cycles engine we will be using in this course. Even though EEVEE is meant for fast and highly interactive rendering work, even more so than the Cycles preview render we showed so far, we do not use EEVEE in this course. The reasons for this are: We personally find Cycles to be more intuitive to work with and explain, as it is built around the path tracing algorithm, which is easy to understand while providing a very versatile set of rendering and lighting features. EEVEE's rendering setup is somewhat more complex, as it uses a combination of different techniques. Cycles can render both on CPU and GPU, whereas EEVEE can only render on a GPU (more specifically, it needs OpenGL) Cycles is more feature-complete, whereas EEVEE has some limitations compared to Cycles, although that situation improves with each Blender release EEVEE doesn't (easily) support headless rendering, i.e. starting a Blender render from the command-line without showing the user interface, which is especially relevant when rendering long animations on an HPC system Although Cycles and EEVEE are getting closer in features with every Blender release they are still not fully equivalent. This would mean having to dedicate extra course material on these differences If you do like more information on EEVEE then please check this section in the Blender manual.","title":"First steps in the user interface"},{"location":"basics/blender_fundamentals/first_steps/#first-steps-in-the-user-interface","text":"Hint A lot of new concepts and UI elements will be introduced in the upcoming videos. It probably works best to watch video(s) limited to a certain topic, try the operations shown and corresponding exercise(s) in Blender yourself, before moving on to the next topic.","title":"First steps in the user interface"},{"location":"basics/blender_fundamentals/first_steps/#starting-blender","text":"In general Blender isn't different to start than any other GUI application. However, warning and error messages will be printed to the console window. It depends on the operating system you're working on how the console window is available: (All operating systems) If you start Blender from a terminal window, e.g. xterm or Windows Command window, then Blender output will be printed in that window (Windows only) If Blender was started from the Start menu, or using a desktop icon, then you can toggle the associated console window using the Window > Toggle System Console option See this section in the Blender manual for more details on starting Blender from the command line and details specific for each operating system.","title":"Starting Blender"},{"location":"basics/blender_fundamentals/first_steps/#user-interface-fundamentals","text":"We will go over fundamentals of the user interface in terms of interaction and areas, specifically the 3D view and Outliner. We also touch on a number of often-performed operations, such as rendering an image and changing the set of selected objects. We also look a bit closer into keyboard shortcuts and menus. It's probably best to follow along in Blender on your own system while viewing the video. The files used in the video can be found under data/blender_basics . Slow 3D interaction If the interaction in the 3D view isn't smooth (as seen in the video) on your PC/laptop something might be wrong in the setup of your system. Please contact us if this appears to be the case. Accidental 'Edit mode' If the 3D view (or some of the other areas) suddenly appear to behave strangely, or you now see your mesh with all kinds of dots or lines then you might have accidentally entered the so-called \"Edit Mode\" or any of the other modes available ( Tab and Ctrl-Tab are used for this). Check the menu in the upper left of the 3D view, which should read Object Mode : In this course we will use only Object Mode (and briefly use Vertex Paint mode in one of the exercises). You can use the drop-down menu shown above (or the Ctrl-Tab menu in the 3D view) and pick Object Mode to get back to the correct mode. Accidental workspace switch Another thing that might happen is that you accidentally click one of the tabs at the top of the screen, which then completely changes the layout of your user interface. These tabs are used to switch between workspaces, where each workspace allows a different layout to focus on a certain task (e.g. 3D modeling, versus shader editing, versus animation). The default workspace is Layout and you might have to switch back to that one:","title":"User interface fundamentals"},{"location":"basics/blender_fundamentals/first_steps/#some-user-interface-tips","text":"To bring up the relevant section of the official Blender manual for (almost) any user interface element, e.g. button, setting or menu, right-click on that element and click Online Manual . This will start a web browser showing the relevant manual page. You can hover with the mouse over pretty much any UI element to get a tooltip with a short description, including shortcut key(s) if available. The keyboard and mouse shortcuts for object selection, editing, view interaction, etc work mostly the same in all Blender editors. So G to grab, X to delete, LMB to select, Shift-MMB to translate, Wheel to zoom, etc. The mouse controls the current area in focus and any keyboard actions are applied in the active area first. You can maximize a user interface area by using Ctrl+Spacebar having the mouse in the area you want to maximize. This can sometimes be useful to temporarily get a larger area to work with. You can use the same shortcut to toggle the area back to its original size, or use the Back to Previous button at the top of the screen.","title":"Some user interface tips"},{"location":"basics/blender_fundamentals/first_steps/#changes-to-default-preference-settings","text":"Here we suggest some preferences settings to change from their default value. Info It's not required to change these defaults, but we find they help us in working with Blender, and so might be useful for you as well Under Edit > Preferences , in the Interface tab: Under Display disable Splash Screen . This will save you a click to get rid of the splash screen each time you start Blender. If you ever want to look at the splash again you can use the Blender logo icon in the top-level of the window and use Splash Screen . Under Editors > Temporary Editors set Render In to Image Editor . This will cause the rendered image to be displayed as a replacement of the 3D view, instead of in a separate window. After rendering press Escape to get back the 3D view that was replaced by the rendered output. Under Editors > Status Bar enable Scene Statistics , System Memory and Video Memory . This will show extra scene statistics in the status bar. Another way to do this is to right-click on the status bar and enable the same options. In case you find that Blender's user interface elements, such as buttons or menu text, are too small you can scale up the UI with a single setting under Display > Resolution Scale . If you change the value you can see the changes in the UI immediately. Orbit around selection Another option which you might consider enabling is Orbit Around Selection . By default this is turned off and in that mode any rotation of the 3D viewport will be around the center of the view, which might cause selected objects to go out of view. When the option is turned on viewport rotation will be around the selected object(s), always keeping them in view. You can find this option on the Navigation tab under Orbit & Pan .","title":"Changes to default preference settings"},{"location":"basics/blender_fundamentals/first_steps/#enabling-gpu-based-rendering-in-cycles","text":"In general GPU-based rendering with Cycles is a lot faster than rendering on a multi-core CPU. However, only by making a comparison on your particular system can you really find out if GPU rendering is a good option for you (for example, you might not have a very powerful GPU in your laptop). Apart from performance there are some other aspects to consider with GPU rendering: When doing a GPU render your desktop environment might become less responsive, although this has become less of a problem with recent Blender versions A GPU usually has less memory available, which might cause problems with really large scenes In case you want to try enabling GPU rendering go to the Preferences window ( Edit > Preferences ) and then the System tab. The settings available under Cycles Render Device are somewhat dependent on the hardware in your system but should look a little like this: GPU rendering in Blender has slightly different support depending on whether you're on Windows, Linux or macOS. The most up-to-date official reference for the information below is this page from the Blender manual, but below we summarize the options you can encounter.","title":"Enabling GPU-based rendering in Cycles"},{"location":"basics/blender_fundamentals/first_steps/#windows-linux","text":"By default None will be active, meaning no GPU acceleration is used for rendering and it all happens on the CPU. In general, on a PC/Laptop with an NVIDIA GPU the CUDA option is to be preferred, although OptiX might work well as an alternative (but will only be available on newer GPUs). On Windows systems with an AMD GPU the option HIP might be available and is then definitely worth a try.","title":"Windows, Linux"},{"location":"basics/blender_fundamentals/first_steps/#macos","text":"For macOS systems only the Metal option will be available, apart from the default None . In Blender 3.1 the GPU rendering support in Cycles is based on the Metal API, which is not supported on all macOS systems (also depending on the system version). Only for the following two configurations GPU rendering support currently available: Apple M1 computers running macOS 12.2 or newer Apple computers with AMD graphics cards running macOS 12.3 or newer Warning The Blender 3.1 release notes warn that the GPU rendering implementation is in an early state. Performance optimizations and support for Intel GPUs are under development. Info This section is about GPU rendering in Cycles, which is different from GPU acceleration for the Blender user interface and EEVEE (see below) rendering. So even though your macOS system might not provide GPU rendering in Cycles, it might still work fine for Blender usage with CPU-based rendering.","title":"macOS"},{"location":"basics/blender_fundamentals/first_steps/#a-thing-called-eevee","text":"When consulting other Blender materials, specifically on rendering, you may see references to EEVEE. This is another render engine available in Blender, which is different from the Cycles engine we will be using in this course. Even though EEVEE is meant for fast and highly interactive rendering work, even more so than the Cycles preview render we showed so far, we do not use EEVEE in this course. The reasons for this are: We personally find Cycles to be more intuitive to work with and explain, as it is built around the path tracing algorithm, which is easy to understand while providing a very versatile set of rendering and lighting features. EEVEE's rendering setup is somewhat more complex, as it uses a combination of different techniques. Cycles can render both on CPU and GPU, whereas EEVEE can only render on a GPU (more specifically, it needs OpenGL) Cycles is more feature-complete, whereas EEVEE has some limitations compared to Cycles, although that situation improves with each Blender release EEVEE doesn't (easily) support headless rendering, i.e. starting a Blender render from the command-line without showing the user interface, which is especially relevant when rendering long animations on an HPC system Although Cycles and EEVEE are getting closer in features with every Blender release they are still not fully equivalent. This would mean having to dedicate extra course material on these differences If you do like more information on EEVEE then please check this section in the Blender manual.","title":"A thing called EEVEE?"},{"location":"basics/blender_fundamentals/introduction/","text":"Introduction \u00b6 This first part of the course is meant to introduce you to Blender, its user interface and basic features. We'll start with a brief look into some of the background of Blender and challenges in learning it.","title":"Introduction"},{"location":"basics/blender_fundamentals/introduction/#introduction","text":"This first part of the course is meant to introduce you to Blender, its user interface and basic features. We'll start with a brief look into some of the background of Blender and challenges in learning it.","title":"Introduction"},{"location":"basics/blender_fundamentals/objects_3d_cursor_undo/","text":"Objects, 3D cursor, Undo \u00b6 A short section on how to add, duplicate or delete objects. What the 3D cursor is and what role it plays, plus the undo system.","title":"Objects, 3D cursor, Undo"},{"location":"basics/blender_fundamentals/objects_3d_cursor_undo/#objects-3d-cursor-undo","text":"A short section on how to add, duplicate or delete objects. What the 3D cursor is and what role it plays, plus the undo system.","title":"Objects, 3D cursor, Undo"},{"location":"basics/blender_fundamentals/scene_hierarchy/","text":"Scene hierarchy \u00b6 We briefly look a the way a scene is organized and how this interacts with the properties panel. The above actually isn't the full story, as we only briefly mention collections. In the official Blender manual you can find more detail on collections here , in case you want to know more.","title":"Scene hierarchy"},{"location":"basics/blender_fundamentals/scene_hierarchy/#scene-hierarchy","text":"We briefly look a the way a scene is organized and how this interacts with the properties panel. The above actually isn't the full story, as we only briefly mention collections. In the official Blender manual you can find more detail on collections here , in case you want to know more.","title":"Scene hierarchy"},{"location":"basics/blender_fundamentals/transformations/","text":"Transformations \u00b6 This might be a bit more of a technical subject and deals with the way 3D objects can be transformed in a scene. The transformations exercise will allow you to try most of these operations yourself. But if you wanto to follow along with the video then the file used is data/blender_basics/three_objects.blend . Summary of shortcut keys \u00b6 G to enter translation mode (\"grab\") S to enter scale mode R to enter rotation mode LMB or Enter to confirm the current transformation, Escape to cancel while still one of the transformation modes While in a transformation mode press X , Y or Z to constrain the transformation to the X, Y or Z axis, respectively. While in a transformation mode press Shift+X , Shift+Y or Shift+Z to constrain the transformation to the plane perpendicular to the X, Y or Z axis, respectively.","title":"Transformations"},{"location":"basics/blender_fundamentals/transformations/#transformations","text":"This might be a bit more of a technical subject and deals with the way 3D objects can be transformed in a scene. The transformations exercise will allow you to try most of these operations yourself. But if you wanto to follow along with the video then the file used is data/blender_basics/three_objects.blend .","title":"Transformations"},{"location":"basics/blender_fundamentals/transformations/#summary-of-shortcut-keys","text":"G to enter translation mode (\"grab\") S to enter scale mode R to enter rotation mode LMB or Enter to confirm the current transformation, Escape to cancel while still one of the transformation modes While in a transformation mode press X , Y or Z to constrain the transformation to the X, Y or Z axis, respectively. While in a transformation mode press Shift+X , Shift+Y or Shift+Z to constrain the transformation to the plane perpendicular to the X, Y or Z axis, respectively.","title":"Summary of shortcut keys"},{"location":"basics/blender_fundamentals/ui/","text":"User interface configuration \u00b6 A short section on how the Blender user interface system works and how to configure it to your liking. This is useful to know as the current UI layout is saved in a Blender file, so files you get from some other source might look very different.","title":"User interface configuration"},{"location":"basics/blender_fundamentals/ui/#user-interface-configuration","text":"A short section on how the Blender user interface system works and how to configure it to your liking. This is useful to know as the current UI layout is saved in a Blender file, so files you get from some other source might look very different.","title":"User interface configuration"},{"location":"basics/importing_data/exercise_vertex_colors/","text":"\ud83d\udcbb Vertex colors \u00b6 Check if you have a menu option to import X3D format. For this, go to File -> Import and check if there is an entry X3D Extensible 3D (.x3d/.wrl) . If you do NOT have the X3D import option then perform the following steps to enable the X3D add-on (otherwise continue to step 3): Open the preferences window with Edit -> Preferences Switch to the Add-ons tab In the search field (with the little spyglass) enter \"X3D\", the list should get reduced to just one entry Enable the checkbox left of \"Import-Export: Web3D X3D/VRML2 format\" Close the preferences window (it saves the settings automatically) Under File -> Import there should now be a new entry X3D Extensible 3D (.x3d/.wrl) Importing data always adds to the current scene. So start with an empty scene, i.e. delete all objects. Make sure Blender is set to use Cycles as the renderer. For this, switch to the Render tab in the properties area. Check the Render Engine drop-down, it should be set to Cycles . Import file glyphs.x3d using File > Import > X3D Extensible 3D . In the importer settings (on the right side of the window when selecting the file to import) use Forward: Y Forward , Up: Z Up . This X3D file holds a scene exported from ParaView. Inspect the objects in the scene to get some idea of what it contains. Delete all the lights in the scene to clear everything up a bit. Add a single sun light in return. This 3D model has so-called \"vertex colors\". This means that each vertex of the geometry has an associated RGB color, which is a common way to show data values in a visualization. There are a few ways to inspect if, and what, vertex colors a model has. First, there is the so-called Vertex Paint mode. In this mode vertex colors are shown when they are available and can even be edited. To enable Vertex Paint mode: Make the 3D arrows in the scene the single selected object Open the Mode pie menu with Ctrl-TAB and switch to Vertex Paint . An alternative is to use the menu showing Object Mode in the upper-left of the 3D view header. The 3D View should now show the arrow geometry colored by its vertex colors. The colors shown are velocity values from a computational flow simulation, using the well-known rainbow color scale (blue \u2192 green \u2192 yellow \u2192 orange \u2192 red for a low-to-high value range) Two things have changed in the interface: 1) the cursor is now a circle, 2) the tool shelf on the left now shows color operations (paint brush = Draw, drop = Blur, ...) As this is Vertex Paint mode you can actually alter the vertex colors. This works quite similar to a normal paint program, like Photoshop or the GIMP, but in 3D. Although it may not make much sense to change colors that are based on simulation output (like these CFD results) it can still be interesting to clean up or highlight vertex-colored geometry. Experiment with vertex painting: move the cursor over part of the arrow geometry, press and hold LMB and move the mouse. See what happens. Switch to the Active Tool and Workspace settings tab in the properties area You can change the color you're painting with with the color wheel. Also check out the radius and strength settings. Change back to Object Mode using the Ctrl-TAB mode menu when you're done playing around. Note that the arrows no longer show the vertex colors. The second way to use vertex colors is to apply them during rendering. If you've screwed up the vertex colors really badly in the previous steps you might want to reimport the model... Make the 3D arrows in the scene the single selected object Switch to the Object Data tab in the properties Check that there is an entry \"Col\" in the list under Vertex Colors . A model can have multiple sets of vertex colors, but this file has only one set called \"Col\" Now we will set up a material using the vertex colors stored in the \"Col\" layer. Go to the Material tab Select the material called \"Material\" in the drop-down list left of the New button. This set the (grey) material \"Material\" on the arrows geometry. Press F12 (or use interactive render) to get a rendered view of the current scene. You'll notice that all the geometry is grey/white, i.e. no vertex colors are used. We'll now alter the material to use vertex colors. In the settings of the material there is a field called \"Base Color\" with a white area right of it. This setting controls the color of the geometry. Click the button right of the color area (it has a small white circle in it) Pick Attribute from the left-most column labeled Input . This specifies that the material color should be based on an attribute value. Base Color is now set to Attribute | Color . Directly below the entry there is a Name field. Enter \"Col\" here. This specifies that the attribute to use is called \"Col\" (i.e. our vertex colors) Now render the scene again The rendered image should now be showing the arrow geometry colored by vertex colors","title":"\ud83d\udcbb Vertex colors"},{"location":"basics/importing_data/exercise_vertex_colors/#vertex-colors","text":"Check if you have a menu option to import X3D format. For this, go to File -> Import and check if there is an entry X3D Extensible 3D (.x3d/.wrl) . If you do NOT have the X3D import option then perform the following steps to enable the X3D add-on (otherwise continue to step 3): Open the preferences window with Edit -> Preferences Switch to the Add-ons tab In the search field (with the little spyglass) enter \"X3D\", the list should get reduced to just one entry Enable the checkbox left of \"Import-Export: Web3D X3D/VRML2 format\" Close the preferences window (it saves the settings automatically) Under File -> Import there should now be a new entry X3D Extensible 3D (.x3d/.wrl) Importing data always adds to the current scene. So start with an empty scene, i.e. delete all objects. Make sure Blender is set to use Cycles as the renderer. For this, switch to the Render tab in the properties area. Check the Render Engine drop-down, it should be set to Cycles . Import file glyphs.x3d using File > Import > X3D Extensible 3D . In the importer settings (on the right side of the window when selecting the file to import) use Forward: Y Forward , Up: Z Up . This X3D file holds a scene exported from ParaView. Inspect the objects in the scene to get some idea of what it contains. Delete all the lights in the scene to clear everything up a bit. Add a single sun light in return. This 3D model has so-called \"vertex colors\". This means that each vertex of the geometry has an associated RGB color, which is a common way to show data values in a visualization. There are a few ways to inspect if, and what, vertex colors a model has. First, there is the so-called Vertex Paint mode. In this mode vertex colors are shown when they are available and can even be edited. To enable Vertex Paint mode: Make the 3D arrows in the scene the single selected object Open the Mode pie menu with Ctrl-TAB and switch to Vertex Paint . An alternative is to use the menu showing Object Mode in the upper-left of the 3D view header. The 3D View should now show the arrow geometry colored by its vertex colors. The colors shown are velocity values from a computational flow simulation, using the well-known rainbow color scale (blue \u2192 green \u2192 yellow \u2192 orange \u2192 red for a low-to-high value range) Two things have changed in the interface: 1) the cursor is now a circle, 2) the tool shelf on the left now shows color operations (paint brush = Draw, drop = Blur, ...) As this is Vertex Paint mode you can actually alter the vertex colors. This works quite similar to a normal paint program, like Photoshop or the GIMP, but in 3D. Although it may not make much sense to change colors that are based on simulation output (like these CFD results) it can still be interesting to clean up or highlight vertex-colored geometry. Experiment with vertex painting: move the cursor over part of the arrow geometry, press and hold LMB and move the mouse. See what happens. Switch to the Active Tool and Workspace settings tab in the properties area You can change the color you're painting with with the color wheel. Also check out the radius and strength settings. Change back to Object Mode using the Ctrl-TAB mode menu when you're done playing around. Note that the arrows no longer show the vertex colors. The second way to use vertex colors is to apply them during rendering. If you've screwed up the vertex colors really badly in the previous steps you might want to reimport the model... Make the 3D arrows in the scene the single selected object Switch to the Object Data tab in the properties Check that there is an entry \"Col\" in the list under Vertex Colors . A model can have multiple sets of vertex colors, but this file has only one set called \"Col\" Now we will set up a material using the vertex colors stored in the \"Col\" layer. Go to the Material tab Select the material called \"Material\" in the drop-down list left of the New button. This set the (grey) material \"Material\" on the arrows geometry. Press F12 (or use interactive render) to get a rendered view of the current scene. You'll notice that all the geometry is grey/white, i.e. no vertex colors are used. We'll now alter the material to use vertex colors. In the settings of the material there is a field called \"Base Color\" with a white area right of it. This setting controls the color of the geometry. Click the button right of the color area (it has a small white circle in it) Pick Attribute from the left-most column labeled Input . This specifies that the material color should be based on an attribute value. Base Color is now set to Attribute | Color . Directly below the entry there is a Name field. Enter \"Col\" here. This specifies that the attribute to use is called \"Col\" (i.e. our vertex colors) Now render the scene again The rendered image should now be showing the arrow geometry colored by vertex colors","title":"\ud83d\udcbb Vertex colors"},{"location":"basics/importing_data/exercise_your_data/","text":"\ud83d\udcbb Your own data \u00b6 Info If you do not have data that you want to import in Blender then you can skip this part. Think about your own data What is the goal for importing the data? What visual representation(s) of the data do you aim for? What scene object types do you need for this? What approach would you use to get it into Blender? Challenges? Problems? Try to import your own data, or a representative subset, using your chosen approach.","title":"\ud83d\udcbb Your own data"},{"location":"basics/importing_data/exercise_your_data/#your-own-data","text":"Info If you do not have data that you want to import in Blender then you can skip this part. Think about your own data What is the goal for importing the data? What visual representation(s) of the data do you aim for? What scene object types do you need for this? What approach would you use to get it into Blender? Challenges? Problems? Try to import your own data, or a representative subset, using your chosen approach.","title":"\ud83d\udcbb Your own data"},{"location":"basics/importing_data/introduction/","text":"Introduction \u00b6 This chapter will present a lot of information on getting data into Blender through importing. It will describe the overall approach, available file formats and their relative strengths/weaknesses and look closer into handling specific types of data, specifically point data and volumetric data. Most of this chapter consists of the video presentation below, which covers quite a few subjects. After you are done viewing the video there is a first exercise on vertex colors, which uses data we provide. While the second exercise is more of a guideline for when you want to import your own data. As mentioned in the presentation the PDF slides for this chapter contain some more reference material on getting data from ParaView , VisIt and VTK .","title":"Introduction"},{"location":"basics/importing_data/introduction/#introduction","text":"This chapter will present a lot of information on getting data into Blender through importing. It will describe the overall approach, available file formats and their relative strengths/weaknesses and look closer into handling specific types of data, specifically point data and volumetric data. Most of this chapter consists of the video presentation below, which covers quite a few subjects. After you are done viewing the video there is a first exercise on vertex colors, which uses data we provide. While the second exercise is more of a guideline for when you want to import your own data. As mentioned in the presentation the PDF slides for this chapter contain some more reference material on getting data from ParaView , VisIt and VTK .","title":"Introduction"},{"location":"basics/rendering_lighting_materials/composition/","text":"Composition \u00b6 Below you'll find supplementary video on image composition. It is supplementary in a way that you won't need it to do the exercises but it might help you with your future Blender renders. This video will give you some practical guidelines that could give your final renders the extra edge it needs to stand out:","title":"Composition"},{"location":"basics/rendering_lighting_materials/composition/#composition","text":"Below you'll find supplementary video on image composition. It is supplementary in a way that you won't need it to do the exercises but it might help you with your future Blender renders. This video will give you some practical guidelines that could give your final renders the extra edge it needs to stand out:","title":"Composition"},{"location":"basics/rendering_lighting_materials/introduction/","text":"Introduction \u00b6 This part of the course is all about the aesthetics, the last part of the pipeline. You know now of the basics and by now you are able to import some scientific data within Blender, the final thing that is left is how the final image will look like. What does the surface of your 3D-model will look like, what tangible texture and colors it has, how it will be illuminated and finally how it will be composed. All these things go hand-in-hand and these things need to be in balance to create an aesthetically pleasing image. Before you start with the exercises the following video will give you the theoretical and practical background to make these exercises. In this video there are some Blender walk-throughs, if you want to follow along you can use the walk-through files in the ./walkthroughs/06-rendering-lighting-and-materials directory.","title":"Introduction"},{"location":"basics/rendering_lighting_materials/introduction/#introduction","text":"This part of the course is all about the aesthetics, the last part of the pipeline. You know now of the basics and by now you are able to import some scientific data within Blender, the final thing that is left is how the final image will look like. What does the surface of your 3D-model will look like, what tangible texture and colors it has, how it will be illuminated and finally how it will be composed. All these things go hand-in-hand and these things need to be in balance to create an aesthetically pleasing image. Before you start with the exercises the following video will give you the theoretical and practical background to make these exercises. In this video there are some Blender walk-throughs, if you want to follow along you can use the walk-through files in the ./walkthroughs/06-rendering-lighting-and-materials directory.","title":"Introduction"},{"location":"basics/rendering_lighting_materials/rlm_assignment/","text":"\ud83d\udcbb Rendering, lighting and materials \u00b6 Open the rlm_assignment.blend file and you'll see several objects in the scene: a ground plane, a plateau, Suzanne (the monkey head) and 3 knots. The goal of this assignment is to place some lights, set the camera parameters to your liking, add materials to the objects and render the final image. We'll do this in steps. Tip To view your result with realistic lighting and materials use the Shading pie menu, which opens with the Z key: Option Rendered shows realistic lighting and materials, with slower interaction Option Solid shows simple colors and lighting, with faster interaction Lighting - Creating light sources \u00b6 To see what we are doing in Rendered shading ( Z - Rendered ) we first need to add the lighting. Add one or two sun lights by either using the 3D view menu in the header ( Add > Light > Sun ) or use Shift-A > Light > Sun in the 3D view Try to position and rotate the lights so that they light the objects under a bit of an angle ( G and R keys). Before we change the appearance of the lights we need to switch to Rendered using the Shading pie menu ( Z > Rendered ) Now adjust the Color and Strength settings under the Object Data properties tab in the properties panel , perhaps try to give one the lights a warm yellowish sun-like color and the other a more less strong blueish and cold color. In the same properties panel tab, try to adjust the Angle (or Size for the other light types) of the sun light and see how it affects the shadows. Small angles (or sizes) create hard shadows, which are ideal to see minor details and large angles (or sizes) create soft shadows, which is more suited to see the overall form of the objects and are less straining on the eye. Now in the same properties editor tab, try out some different lamp types ( Point , Sun , ...) to experiment with the different lighting effects they produce. Bonus : If setting up the lamps is too cumbersome, you can go to the World tab in the properties editor and click the little globe drop-down menu button at the top and select the HDRIWorldLighting . This will enable predefined environment lighting using a 360 image of somebody's living room. Do make sure that you de-activate ( in the Outliner ) or remove the lamps to see the full effect. Camera - Setting the starting point of the light paths \u00b6 With the lighting setup, we can now see what each of the camera settings does. Or from the light ray paths perspective: configure the starting point of the light rays. First you need to be in the camera view to be able to see the changes of the camera settings by selecting with the View pie menu ( `-button ) the View Camera option or through the 3D view menu in the header ( View > Viewpoint > Camera ). Try changing the camera's focal length. For this, select the camera Camera and go the Lens settings in the Object Data properties tab in the properties panel . There you can find the Focal Length setting, try for example values 18, 50 and 100 and see what effect this has. Notice that when you set the Focal Length to a lower value you might see clipping (the scene is cut off from a certain distance). This can be changed by setting the Clip Start in the same Lens section to a lower value, e.g. 0.01. Finally set the focal length to the desired value. Next we are going to bring the focus to a chosen object in the scene with the depth of field settings. For this, select the camera, scroll down in the Object Data properties tab in the properties panel to the Depth of Field settings. Check the check-box before Depth of Field to activate the depth of field. Now try to set the Focus on Object value to the Suzanne object and test different values for the Aperture > F-Stop setting. When you are done, disable depth of field again. This makes the material editing easier. If the lighting gives the desired effect looking through the configured camera then you can give the objects the look you want with materials in the next section. Materials - How will the light paths bounce? \u00b6 To design how the light is reflected or refracted off the objects you are going to give each object a different material. For each object (including ground plane and plateau): Select the object and go to the Material tab in the properties editor. In the Material tab click the New button. Then under the Surface section set the Surface parameter to either Diffuse BSDF , Glossy BSDF or Principled BSDF . Try to play with the material settings Roughness and Color (the latter is called Base Color for the Principled BSDF) Bonus : When you feel that the roughness and the color alone didn't give you the look that you want with the Principled BSDF then also try and have a look at the other, in the slides, mentioned settings, Metallic , Transmission , IOR and Subsurface . Rendering - Creating your final image \u00b6 Lights, camera, (materials,) set aaaaaaand action!... Now you will set the desired render settings to generate the final image! Go to the properties editor and set the following settings: - Render properties tab Set Device to GPU Compute . If your device doesn't have a (powerful) GPU set it to CPU . Sampling section: set Render to 128 Light Paths section: set Clamping > Indirect Light to 1.0 Output tab Dimensions section: set Resolution to 1920x1080, 100%. If everything is set, press F12 . Now the Image editor will replace the 3D view and your image will slowly be rendered in small rectangular image parts called \"tiles\". Finally when the image looks the way you want don't forget to save it! In the Image editor go to the Image menu and click on Save As... and choose a location and save the image.","title":"\ud83d\udcbb Rendering, lighting and materials"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#rendering-lighting-and-materials","text":"Open the rlm_assignment.blend file and you'll see several objects in the scene: a ground plane, a plateau, Suzanne (the monkey head) and 3 knots. The goal of this assignment is to place some lights, set the camera parameters to your liking, add materials to the objects and render the final image. We'll do this in steps. Tip To view your result with realistic lighting and materials use the Shading pie menu, which opens with the Z key: Option Rendered shows realistic lighting and materials, with slower interaction Option Solid shows simple colors and lighting, with faster interaction","title":"\ud83d\udcbb Rendering, lighting and materials"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#lighting-creating-light-sources","text":"To see what we are doing in Rendered shading ( Z - Rendered ) we first need to add the lighting. Add one or two sun lights by either using the 3D view menu in the header ( Add > Light > Sun ) or use Shift-A > Light > Sun in the 3D view Try to position and rotate the lights so that they light the objects under a bit of an angle ( G and R keys). Before we change the appearance of the lights we need to switch to Rendered using the Shading pie menu ( Z > Rendered ) Now adjust the Color and Strength settings under the Object Data properties tab in the properties panel , perhaps try to give one the lights a warm yellowish sun-like color and the other a more less strong blueish and cold color. In the same properties panel tab, try to adjust the Angle (or Size for the other light types) of the sun light and see how it affects the shadows. Small angles (or sizes) create hard shadows, which are ideal to see minor details and large angles (or sizes) create soft shadows, which is more suited to see the overall form of the objects and are less straining on the eye. Now in the same properties editor tab, try out some different lamp types ( Point , Sun , ...) to experiment with the different lighting effects they produce. Bonus : If setting up the lamps is too cumbersome, you can go to the World tab in the properties editor and click the little globe drop-down menu button at the top and select the HDRIWorldLighting . This will enable predefined environment lighting using a 360 image of somebody's living room. Do make sure that you de-activate ( in the Outliner ) or remove the lamps to see the full effect.","title":"Lighting - Creating light sources"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#camera-setting-the-starting-point-of-the-light-paths","text":"With the lighting setup, we can now see what each of the camera settings does. Or from the light ray paths perspective: configure the starting point of the light rays. First you need to be in the camera view to be able to see the changes of the camera settings by selecting with the View pie menu ( `-button ) the View Camera option or through the 3D view menu in the header ( View > Viewpoint > Camera ). Try changing the camera's focal length. For this, select the camera Camera and go the Lens settings in the Object Data properties tab in the properties panel . There you can find the Focal Length setting, try for example values 18, 50 and 100 and see what effect this has. Notice that when you set the Focal Length to a lower value you might see clipping (the scene is cut off from a certain distance). This can be changed by setting the Clip Start in the same Lens section to a lower value, e.g. 0.01. Finally set the focal length to the desired value. Next we are going to bring the focus to a chosen object in the scene with the depth of field settings. For this, select the camera, scroll down in the Object Data properties tab in the properties panel to the Depth of Field settings. Check the check-box before Depth of Field to activate the depth of field. Now try to set the Focus on Object value to the Suzanne object and test different values for the Aperture > F-Stop setting. When you are done, disable depth of field again. This makes the material editing easier. If the lighting gives the desired effect looking through the configured camera then you can give the objects the look you want with materials in the next section.","title":"Camera - Setting the starting point of the light paths"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#materials-how-will-the-light-paths-bounce","text":"To design how the light is reflected or refracted off the objects you are going to give each object a different material. For each object (including ground plane and plateau): Select the object and go to the Material tab in the properties editor. In the Material tab click the New button. Then under the Surface section set the Surface parameter to either Diffuse BSDF , Glossy BSDF or Principled BSDF . Try to play with the material settings Roughness and Color (the latter is called Base Color for the Principled BSDF) Bonus : When you feel that the roughness and the color alone didn't give you the look that you want with the Principled BSDF then also try and have a look at the other, in the slides, mentioned settings, Metallic , Transmission , IOR and Subsurface .","title":"Materials - How will the light paths bounce?"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#rendering-creating-your-final-image","text":"Lights, camera, (materials,) set aaaaaaand action!... Now you will set the desired render settings to generate the final image! Go to the properties editor and set the following settings: - Render properties tab Set Device to GPU Compute . If your device doesn't have a (powerful) GPU set it to CPU . Sampling section: set Render to 128 Light Paths section: set Clamping > Indirect Light to 1.0 Output tab Dimensions section: set Resolution to 1920x1080, 100%. If everything is set, press F12 . Now the Image editor will replace the 3D view and your image will slowly be rendered in small rectangular image parts called \"tiles\". Finally when the image looks the way you want don't forget to save it! In the Image editor go to the Image menu and click on Save As... and choose a location and save the image.","title":"Rendering - Creating your final image"},{"location":"basics/simple_mesh_editing/introduction/","text":"Introduction \u00b6 This chapter will introduce the basic mesh editing tools available within Blender. The basic mesh editing will be performed with the so called modifiers, these modifiers make it relatively easy to do large mesh editing operations that can greatly impact the visual representation of your 3D-models. Below you'll find a video that will give you a theoretical introduction followed by a practical walk-through in Blender. If you want to follow along with the walk-through you can find the Blend files in the walk-through directory ./walkthroughs/04-simple-mesh-editing . After you watched the video about simple mesh-editing you are ready for the exercises!","title":"Introduction"},{"location":"basics/simple_mesh_editing/introduction/#introduction","text":"This chapter will introduce the basic mesh editing tools available within Blender. The basic mesh editing will be performed with the so called modifiers, these modifiers make it relatively easy to do large mesh editing operations that can greatly impact the visual representation of your 3D-models. Below you'll find a video that will give you a theoretical introduction followed by a practical walk-through in Blender. If you want to follow along with the walk-through you can find the Blend files in the walk-through directory ./walkthroughs/04-simple-mesh-editing . After you watched the video about simple mesh-editing you are ready for the exercises!","title":"Introduction"},{"location":"basics/simple_mesh_editing/sme_assignment/","text":"\ud83d\udcbb Simple mesh editing \u00b6 In this exercise you will use some mesh modifiers on an iso-surface of a CT scan of a fish and try to see if you can reveal the insides. Once you opened the exercise blend file sme_assignment.blend you'll see the fish iso-surface above a plane. Decimate - Reducing the triangles \u00b6 The fish 3D model, for your convenience, has been divided into two parts: the fishskin and fishbones. Combined, this model has a large amount of triangles (155k for the fishskin and 573k for the fishbones). On lower end devices this can slow everything down to a crawl. In order to be able to add modifiers or edit the meshes with reasonable interactivity you first need to decimate the meshes. The decimation is for reducing the number of triangles, by merging adjacent triangles together into one, iteratively. Select the fishskin by clicking on the fishskin with LMB . Once selected go the Modifiers tab in the properties editor. Click Add Modifier and add the Decimate modifier (it's in the Generate column). Keep the decimation type set to Collapse , set the Ratio to 0.5 and press Enter . The mesh processing will take a couple of seconds but will immediately reduce the number of triangles to ~77k, which is visible in the modifier under Face Count . You can even reduce it to a lower number but it might affect the appearance and shape of the model negatively by creating hard edges on the surface. Once you are satisfied with the results press Apply , under the dropdown-menu arrow to the right of Decimate , or by pressing Ctrl-A , while focused on the Decimate modifier, to make the changes permanent. Again, this can take a few seconds. Now that the fishskin triangles have been reduced, select it and press H to hide it or click the icon in the Outliner . This simultaneously hides the fishskin and reveal the fishbones. Preform the same steps the fishbones and try to reduce the triangle count significantly without affecting the appearance of the model. Now unhide the fishskin again for the next assignment by clicking the icon. Smooth - Ironing the creases \u00b6 The geometry of the fishskin and the fishbones both look a bit rough because of the iso-surface extraction algorithm. If that is not desired, the rough edges can be smoothed out with the Smooth Modifier . Select the fishskin model by clicking on the fishskin with LMB . Go to the Modifiers tab in the properties editor. Click Add Modifier and add the Smooth modifier (it's in the Deform column). Keep the Factor at 0.5 but increase the Repeat to 5 . Watch out with using the slider, every change re-triggers the modifier and when you accidentally slide to a high number it will take a while to calculate. Unfortunately you will notices that the Smooth modifier creates tears along the skin model, this conveniently revealed that the underlying mesh triangles are not fully connected and are present in connected patches. These patches stems from the creation of this model where the calculation of the geometry was done in multiple processors and each patch was created by a separate process. This can be fixed in the Edit-mode but that will be covered in the advanced course. The Factor is good as it is, but changing the value shows what kind of drastic effect it has. Once you are satisfied with the smoothness of the fishskin press Apply and try to do the same with the fishbones. Boolean - Slicing the geometry \u00b6 If you wanna both show the inside of the fish with the context of the outside you can use slice through the fishskin model and reveal the insides of the fish by using a Boolean Modifier . Before you add the Boolean Modifier you first need to reveal the fishskin mesh object again by clicking the icon in the Outliner . Select the fishskin mesh object and go to the Modifiers tab in the properties editor to add a Boolean modifier (it's in the Generate column). Now that the Boolean modifier is added we still miss another 3D mesh object to perform the Boolean operation with. You are now gonna prepare the other mesh object. Move the mouse into the 3D view and add a new UV sphere with Shift-A > Mesh > UV Sphere Scale and translate ( S and G keys) the UV sphere so that it overlaps a part of the fish which you want to clip away. The UV sphere is now shown as a solid surface, which is not desirable when you want to use it for clipping because you want to see through it. You can change the representation of an object in the 3D view using the Object properties under Viewport Display : set Display As to Wire . Also when you want to look at the results in Rendered mode you need to make the sphere invisible using the Ray Visibility settings under Visibility : disable all check-boxes (Camera, Diffuse, Glossy, Transmission, Volume Scatter and Shadow) Now that you prepared the mesh object to preform the Boolean operation with, you can continue setting up the Boolean modifier. Select the fishskin mesh object and go to the Modifiers tab in the properties editor to reveal the already added Boolean modifier. Now under Object , select the Sphere mesh object. Before we want to start moving the clipping Sphere around we want to change the Solver to Fast . This is more simpler and better performing solver and in our case, with the underlying broken patched mesh, also a better option since this solver is able to handle this type of geometry. Now if you select the Sphere object and translate and scale it over the fishskin mesh object you can clip away any desired part as the Boolean modifier updates in real time. As you might have noticed, this Boolean modifier does have some problems with this current mesh and placement of the clipping sphere must be precise. This off course is not always the cause but it should be kept in mind when working with the Boolean modifier. Finally you can view your results with Cycles with Rendered shading ( Z > Rendered ) for better lighting and materials. Or you can give the camera a better position and make a nice final render.","title":"\ud83d\udcbb Simple mesh editing"},{"location":"basics/simple_mesh_editing/sme_assignment/#simple-mesh-editing","text":"In this exercise you will use some mesh modifiers on an iso-surface of a CT scan of a fish and try to see if you can reveal the insides. Once you opened the exercise blend file sme_assignment.blend you'll see the fish iso-surface above a plane.","title":"\ud83d\udcbb Simple mesh editing"},{"location":"basics/simple_mesh_editing/sme_assignment/#decimate-reducing-the-triangles","text":"The fish 3D model, for your convenience, has been divided into two parts: the fishskin and fishbones. Combined, this model has a large amount of triangles (155k for the fishskin and 573k for the fishbones). On lower end devices this can slow everything down to a crawl. In order to be able to add modifiers or edit the meshes with reasonable interactivity you first need to decimate the meshes. The decimation is for reducing the number of triangles, by merging adjacent triangles together into one, iteratively. Select the fishskin by clicking on the fishskin with LMB . Once selected go the Modifiers tab in the properties editor. Click Add Modifier and add the Decimate modifier (it's in the Generate column). Keep the decimation type set to Collapse , set the Ratio to 0.5 and press Enter . The mesh processing will take a couple of seconds but will immediately reduce the number of triangles to ~77k, which is visible in the modifier under Face Count . You can even reduce it to a lower number but it might affect the appearance and shape of the model negatively by creating hard edges on the surface. Once you are satisfied with the results press Apply , under the dropdown-menu arrow to the right of Decimate , or by pressing Ctrl-A , while focused on the Decimate modifier, to make the changes permanent. Again, this can take a few seconds. Now that the fishskin triangles have been reduced, select it and press H to hide it or click the icon in the Outliner . This simultaneously hides the fishskin and reveal the fishbones. Preform the same steps the fishbones and try to reduce the triangle count significantly without affecting the appearance of the model. Now unhide the fishskin again for the next assignment by clicking the icon.","title":"Decimate - Reducing the triangles"},{"location":"basics/simple_mesh_editing/sme_assignment/#smooth-ironing-the-creases","text":"The geometry of the fishskin and the fishbones both look a bit rough because of the iso-surface extraction algorithm. If that is not desired, the rough edges can be smoothed out with the Smooth Modifier . Select the fishskin model by clicking on the fishskin with LMB . Go to the Modifiers tab in the properties editor. Click Add Modifier and add the Smooth modifier (it's in the Deform column). Keep the Factor at 0.5 but increase the Repeat to 5 . Watch out with using the slider, every change re-triggers the modifier and when you accidentally slide to a high number it will take a while to calculate. Unfortunately you will notices that the Smooth modifier creates tears along the skin model, this conveniently revealed that the underlying mesh triangles are not fully connected and are present in connected patches. These patches stems from the creation of this model where the calculation of the geometry was done in multiple processors and each patch was created by a separate process. This can be fixed in the Edit-mode but that will be covered in the advanced course. The Factor is good as it is, but changing the value shows what kind of drastic effect it has. Once you are satisfied with the smoothness of the fishskin press Apply and try to do the same with the fishbones.","title":"Smooth - Ironing the creases"},{"location":"basics/simple_mesh_editing/sme_assignment/#boolean-slicing-the-geometry","text":"If you wanna both show the inside of the fish with the context of the outside you can use slice through the fishskin model and reveal the insides of the fish by using a Boolean Modifier . Before you add the Boolean Modifier you first need to reveal the fishskin mesh object again by clicking the icon in the Outliner . Select the fishskin mesh object and go to the Modifiers tab in the properties editor to add a Boolean modifier (it's in the Generate column). Now that the Boolean modifier is added we still miss another 3D mesh object to perform the Boolean operation with. You are now gonna prepare the other mesh object. Move the mouse into the 3D view and add a new UV sphere with Shift-A > Mesh > UV Sphere Scale and translate ( S and G keys) the UV sphere so that it overlaps a part of the fish which you want to clip away. The UV sphere is now shown as a solid surface, which is not desirable when you want to use it for clipping because you want to see through it. You can change the representation of an object in the 3D view using the Object properties under Viewport Display : set Display As to Wire . Also when you want to look at the results in Rendered mode you need to make the sphere invisible using the Ray Visibility settings under Visibility : disable all check-boxes (Camera, Diffuse, Glossy, Transmission, Volume Scatter and Shadow) Now that you prepared the mesh object to preform the Boolean operation with, you can continue setting up the Boolean modifier. Select the fishskin mesh object and go to the Modifiers tab in the properties editor to reveal the already added Boolean modifier. Now under Object , select the Sphere mesh object. Before we want to start moving the clipping Sphere around we want to change the Solver to Fast . This is more simpler and better performing solver and in our case, with the underlying broken patched mesh, also a better option since this solver is able to handle this type of geometry. Now if you select the Sphere object and translate and scale it over the fishskin mesh object you can clip away any desired part as the Boolean modifier updates in real time. As you might have noticed, this Boolean modifier does have some problems with this current mesh and placement of the clipping sphere must be precise. This off course is not always the cause but it should be kept in mind when working with the Boolean modifier. Finally you can view your results with Cycles with Rendered shading ( Z > Rendered ) for better lighting and materials. Or you can give the camera a better position and make a nice final render.","title":"Boolean - Slicing the geometry"},{"location":"overview/about/","text":"About us \u00b6 We are members of the High-Performance Computing & Visualization (HPCV) group at SURF , and are based in Amsterdam. SURF is a cooperative association of Dutch educational and research institutions in which the members combine their strengths to acquire or develop digital services, and to encourage knowledge sharing through continuous innovation. Within the HPCV group we support users of the Dutch National compute infrastructure with visualization expertise and software development, on topics such as data visualization, remote visualization, 3D modeling and rendering and use of eXtended Reality (XR) for research and education. Part of our jobs is to provide courses on topics related to visualization in HPC. This Blender course was created for the PRACE Training Center and first provided (in-person) in 2018, and has since been repeated at least once a year. Paul Melis \u00b6 Paul Melis has an MSc in Computer Science from the University of Twente in The Netherlands and worked on topics in scientific visualization and VR at the University of Groningen and University of Amsterdam before joining SURFsara in 2009 (which has since become part of SURF). At SURF he is involved in several activities related to visualization, including realizing visualization projects for end-users, teaching courses and providing user support for visualization tasks on our HPC systems. As part of the SURF innovation portfolio he leads a project on the use of extended reality (XR) for research and education. He likes to use Blender for all things 3D, but also works with ParaView, and sometimes develops a bit of code in Python or C++. Casper van Leeuwen \u00b6 Casper has a MSc in Computer Science from Delft University of Technology where he graduated on the topic of medical visualization. He has been at SURFsara since 2014. He mainly works on web-based 2D/3D visualization, including Jupyter Notebooks and loves to work on Blender projects when the goal is to make something look aesthetic! Besides that he also knows his way around Unity and Unreal Engine. Ben de Vries \u00b6 Ben de Vries has a PhD in Astrophysics from KU Leuven. He joined SURF in 2019. He focuses on 2D/3D visualization projects using Blender, Unity and general 3D programming.","title":"About us"},{"location":"overview/about/#about-us","text":"We are members of the High-Performance Computing & Visualization (HPCV) group at SURF , and are based in Amsterdam. SURF is a cooperative association of Dutch educational and research institutions in which the members combine their strengths to acquire or develop digital services, and to encourage knowledge sharing through continuous innovation. Within the HPCV group we support users of the Dutch National compute infrastructure with visualization expertise and software development, on topics such as data visualization, remote visualization, 3D modeling and rendering and use of eXtended Reality (XR) for research and education. Part of our jobs is to provide courses on topics related to visualization in HPC. This Blender course was created for the PRACE Training Center and first provided (in-person) in 2018, and has since been repeated at least once a year.","title":"About us"},{"location":"overview/about/#paul-melis","text":"Paul Melis has an MSc in Computer Science from the University of Twente in The Netherlands and worked on topics in scientific visualization and VR at the University of Groningen and University of Amsterdam before joining SURFsara in 2009 (which has since become part of SURF). At SURF he is involved in several activities related to visualization, including realizing visualization projects for end-users, teaching courses and providing user support for visualization tasks on our HPC systems. As part of the SURF innovation portfolio he leads a project on the use of extended reality (XR) for research and education. He likes to use Blender for all things 3D, but also works with ParaView, and sometimes develops a bit of code in Python or C++.","title":"Paul Melis"},{"location":"overview/about/#casper-van-leeuwen","text":"Casper has a MSc in Computer Science from Delft University of Technology where he graduated on the topic of medical visualization. He has been at SURFsara since 2014. He mainly works on web-based 2D/3D visualization, including Jupyter Notebooks and loves to work on Blender projects when the goal is to make something look aesthetic! Besides that he also knows his way around Unity and Unreal Engine.","title":"Casper van Leeuwen"},{"location":"overview/about/#ben-de-vries","text":"Ben de Vries has a PhD in Astrophysics from KU Leuven. He joined SURF in 2019. He focuses on 2D/3D visualization projects using Blender, Unity and general 3D programming.","title":"Ben de Vries"},{"location":"overview/conventions/","text":"Text conventions \u00b6 The conventions on these pages follow those used in the official Blender documentation as much as possible: Keyboard and mouse actions, menu names, literal text to enter, etc are shown in monospaced bold, e.g. X or Shift-MMB LMB = left mouse button, MMB = middle mouse button, RMB = right mouse button, Wheel = scrolling the mouse wheel Menu actions are shown as View > Cameras > Set Active Object as Camera , for View menu, Cameras submenu, \"Set Active Object as Camera\" option. Exercises \u00b6 We highlight exercise sections by prefixing their titles with a \ud83d\udcbb symbol.","title":"Text conventions"},{"location":"overview/conventions/#text-conventions","text":"The conventions on these pages follow those used in the official Blender documentation as much as possible: Keyboard and mouse actions, menu names, literal text to enter, etc are shown in monospaced bold, e.g. X or Shift-MMB LMB = left mouse button, MMB = middle mouse button, RMB = right mouse button, Wheel = scrolling the mouse wheel Menu actions are shown as View > Cameras > Set Active Object as Camera , for View menu, Cameras submenu, \"Set Active Object as Camera\" option.","title":"Text conventions"},{"location":"overview/conventions/#exercises","text":"We highlight exercise sections by prefixing their titles with a \ud83d\udcbb symbol.","title":"Exercises"},{"location":"overview/introduction/","text":"Introduction \u00b6 This Blender course consists of two parts, that are each taught separately over the course of a number of weeks: In the Basics part we assume no prior knowledge of Blender. We will introduce Blender from the ground up, starting with the user interface and basic functionality. We cover the 3D scene, cameras, lights and materials and some basic mesh editing and animation. It helps to have some familiarity with basic 3D graphics concepts, such as 3D geometry, transformations and rendering. But if not, you will probably pick those up quite quickly during the course. In the Advanced part of the course, we assume participants already have basic knowledge of Blender, preferably by following our basics course. We assume participants are familiar with the Blender user interface, basic functionality and concepts like the 3D scene, cameras, lights, materials and some basic mesh editing and animation. The advanced part goes into detail on the Python API for scripting, node-based materials, mesh editing and animation. The main goal of the Advanced course is for you to realize your own project with Blender, based on data you choose. Context \u00b6 This course is aimed at scientists and researchers of all levels. We don't make many assumptions on use cases for Blender, but do assume the context to be an academic setting. So we won't go into creating visual effects for putting a massive CGI tornado in your backyard that scoops up your neighbours. But if you happen to write a tornado simulation for your research we will be more than happy to see how we can use Blender to make attractive visuals of the data. This doesn't mean that we only assume to apply Blender to existing scientific data. Sometimes certain concepts are best explained by creating a 3D scene, say to produce a nice looking cover image for your PhD thesis, or to illustrate or visualize a certain concept. From previous editions of the course we know many participants bring their own data and want to apply Blender to it. We encourage you to do that as well, as it will also help in providing some focus to your use of Blender. Blender version \u00b6 We currently use Blender 3.1 for this course and the materials provided. Blender as a software package is a fast moving target, usually with lots of shiny new features and bug fixes in each release. This is great, of course, but with each release usually also a lot of small tweaks and improvements are made, especially in the user interface and workflow. Course videos using previous Blender versions Some of the videos used in the course might still show an earlier Blender version. In those cases we have estimated that the video is still (largely) up-to-date and have chosen not to update the video, as this is quite time-consuming. We originally planned to only base this course on the Blender LTS (Long-Term Support) releases, which remain more-or-less unchanged regarding UI and features for roughly 2 years. But there have been some major improvements in versions 3.0 and 3.1 which will only become available in the next LTS release in June 2023 (i.e. the follow-up of the 2.93 LTS release). Hence, we chose to update the course to 3.1 before the next LTS release. Specifically for Linux users that use their Linux distribution's package of Blender: Sometimes the distro package gets built with slightly different versions of software libraries, compared to the official Blender distribution. This is known to sometimes cause different behaviour or even bugs, for example in the handling of video files by the FFmpeg library . In case you find strange issues or bugs with your distro's Blender you might want to try downloading the official Blender binaries to see if that fixes those issues. Issues with course materials \u00b6 We try to keep this course up to date to match the specific version mentioned above. But we might have missed small things. If so, please let us know through Github by reporting an issue. If you don't have a Github account, or would rather not create one, then telling us through Discord is fine as well. Prerequisites \u00b6 You will need: A system (PC or laptop) to work on. This can be a Linux, macOS or Windows system. It is preferred to use a system with a somewhat recent GPU (or at most 10 years old) with working OpenGL 4.3 support. See the section \"Hardware Requirements\" on this page for the official requirements for running Blender. Blender 3.1 installed on the above system. You can download it from here , or you can use your system package manager to install it. Warning It is in general not recommended to use a wildly different Blender version for this course, due to possible mismatches in the user interface and functionality with the course material. A different minor version, e.g. 3.1.1 when it becomes available, should not cause issues, but a future 3.2 release might have some major changes. Please test the Blender installation before the course starts using the instructions sent by e-mail. This will tell you if Blender is working correctly and can save you (and us) time fixing any system-related issues during the course period. Recommended: Using a 3-button mouse is preferred, as not all Blender functionality is easily used through a laptop trackpad Feedback \u00b6 We will ask for feedback on this in the online sessions, but if you have remarks then please let us know. You can do this either through Github by reporting an issue. , or in the Discord sessions.","title":"Introduction"},{"location":"overview/introduction/#introduction","text":"This Blender course consists of two parts, that are each taught separately over the course of a number of weeks: In the Basics part we assume no prior knowledge of Blender. We will introduce Blender from the ground up, starting with the user interface and basic functionality. We cover the 3D scene, cameras, lights and materials and some basic mesh editing and animation. It helps to have some familiarity with basic 3D graphics concepts, such as 3D geometry, transformations and rendering. But if not, you will probably pick those up quite quickly during the course. In the Advanced part of the course, we assume participants already have basic knowledge of Blender, preferably by following our basics course. We assume participants are familiar with the Blender user interface, basic functionality and concepts like the 3D scene, cameras, lights, materials and some basic mesh editing and animation. The advanced part goes into detail on the Python API for scripting, node-based materials, mesh editing and animation. The main goal of the Advanced course is for you to realize your own project with Blender, based on data you choose.","title":"Introduction"},{"location":"overview/introduction/#context","text":"This course is aimed at scientists and researchers of all levels. We don't make many assumptions on use cases for Blender, but do assume the context to be an academic setting. So we won't go into creating visual effects for putting a massive CGI tornado in your backyard that scoops up your neighbours. But if you happen to write a tornado simulation for your research we will be more than happy to see how we can use Blender to make attractive visuals of the data. This doesn't mean that we only assume to apply Blender to existing scientific data. Sometimes certain concepts are best explained by creating a 3D scene, say to produce a nice looking cover image for your PhD thesis, or to illustrate or visualize a certain concept. From previous editions of the course we know many participants bring their own data and want to apply Blender to it. We encourage you to do that as well, as it will also help in providing some focus to your use of Blender.","title":"Context"},{"location":"overview/introduction/#blender-version","text":"We currently use Blender 3.1 for this course and the materials provided. Blender as a software package is a fast moving target, usually with lots of shiny new features and bug fixes in each release. This is great, of course, but with each release usually also a lot of small tweaks and improvements are made, especially in the user interface and workflow. Course videos using previous Blender versions Some of the videos used in the course might still show an earlier Blender version. In those cases we have estimated that the video is still (largely) up-to-date and have chosen not to update the video, as this is quite time-consuming. We originally planned to only base this course on the Blender LTS (Long-Term Support) releases, which remain more-or-less unchanged regarding UI and features for roughly 2 years. But there have been some major improvements in versions 3.0 and 3.1 which will only become available in the next LTS release in June 2023 (i.e. the follow-up of the 2.93 LTS release). Hence, we chose to update the course to 3.1 before the next LTS release. Specifically for Linux users that use their Linux distribution's package of Blender: Sometimes the distro package gets built with slightly different versions of software libraries, compared to the official Blender distribution. This is known to sometimes cause different behaviour or even bugs, for example in the handling of video files by the FFmpeg library . In case you find strange issues or bugs with your distro's Blender you might want to try downloading the official Blender binaries to see if that fixes those issues.","title":"Blender version"},{"location":"overview/introduction/#issues-with-course-materials","text":"We try to keep this course up to date to match the specific version mentioned above. But we might have missed small things. If so, please let us know through Github by reporting an issue. If you don't have a Github account, or would rather not create one, then telling us through Discord is fine as well.","title":"Issues with course materials"},{"location":"overview/introduction/#prerequisites","text":"You will need: A system (PC or laptop) to work on. This can be a Linux, macOS or Windows system. It is preferred to use a system with a somewhat recent GPU (or at most 10 years old) with working OpenGL 4.3 support. See the section \"Hardware Requirements\" on this page for the official requirements for running Blender. Blender 3.1 installed on the above system. You can download it from here , or you can use your system package manager to install it. Warning It is in general not recommended to use a wildly different Blender version for this course, due to possible mismatches in the user interface and functionality with the course material. A different minor version, e.g. 3.1.1 when it becomes available, should not cause issues, but a future 3.2 release might have some major changes. Please test the Blender installation before the course starts using the instructions sent by e-mail. This will tell you if Blender is working correctly and can save you (and us) time fixing any system-related issues during the course period. Recommended: Using a 3-button mouse is preferred, as not all Blender functionality is easily used through a laptop trackpad","title":"Prerequisites"},{"location":"overview/introduction/#feedback","text":"We will ask for feedback on this in the online sessions, but if you have remarks then please let us know. You can do this either through Github by reporting an issue. , or in the Discord sessions.","title":"Feedback"},{"location":"overview/schedule/","text":"Schedule \u00b6 When What Where Purpose 19-04-22 Basics session #1 Online Intro to the course, getting to know each other 26-04-22 Basics session #2 Online Feedback on first week, Q&A 03-05-22 Basics session #3 Online Feedback on course, Q&A, closing 17-05-22 Advanced session #1 Online Intro to the course, getting to know each other 24-05-22 Advanced session #2 Online Feedback on first week, Q&A 31-05-22 Advanced session #3 Online Feedback on course, Q&A, closing","title":"Schedule"},{"location":"overview/schedule/#schedule","text":"When What Where Purpose 19-04-22 Basics session #1 Online Intro to the course, getting to know each other 26-04-22 Basics session #2 Online Feedback on first week, Q&A 03-05-22 Basics session #3 Online Feedback on course, Q&A, closing 17-05-22 Advanced session #1 Online Intro to the course, getting to know each other 24-05-22 Advanced session #2 Online Feedback on first week, Q&A 31-05-22 Advanced session #3 Online Feedback on course, Q&A, closing","title":"Schedule"},{"location":"overview/setup/","text":"Course setup \u00b6 Warn To be updated Info Although this course material is available online at any time, we only provide the support mentioned at scheduled course periods throughout the year. Please check the PRACE Training Centre Events calendar when the next Blender course is scheduled. We use a combination of different media within the course, but the basis is for you to follow the training in your own pace over a period of two weeks. The online material consists of: Videos that introduce new concepts and features within Blender. Slides (also presented as part of the videos) for explanations. These are basically the presentations we would normally do plenary. Exercises for you to explore new topics and to train your skills We have scheduled a few short plenary online sessions in the course period to provide general feedback and/or guidance. Support \u00b6 Detailed interaction and support during the course period is provided through our Discord server. Here you can ask questions by chat, upload an image or (if needed) start a video session or share your screen with one of us . In the section 2021 Intr. to Blender Course there are two channels available: #chat-support : this is a (shared) text channel for interacting via chat #plenary-sessions : this is a voice/video channel in which we host the online sessions For one-on-one contact, including the option for screen sharing, right-click on one of our names as shown above. We will be active on Discord during office hours (CET time zone) and will try to also be on-line outside of those hours, all on a best-effort basis. Data files \u00b6 Most of the exercises require you to load a Blender scene file that we provide. These files can be found at https://edu.nl/8n7en . It is best to download the full content of the share to your local system using the Download button in the upper-right. This share contains: data - Blender files (and other data) for the assignments, with a subdirectory per chapter slides - The slides (in PDF) walkthroughs - Some of the files used in the presentations cheat-sheat-2.9.pdf - A 2-page cheat sheet with often used operations and their short cuts Time investment \u00b6 The precise amount of time needed to follow this course depends for a large part on how much attention you devote to each topic, your available time, your learning pace, etc. However, the previous in-person course setup we used in previous years was a full-day course (but with quite a high pace). For the Basics course the time spent on the different subjects and their assignments in that setup is shown below. This might give you some idea on the relative depth of the topics. Topic Time in schedule (previous in-person course) Videos (this course) Introduction 30 minutes 5 minutes Blender basics 120 minutes 45 minutes Importing data 30 minutes 30 minutes Rendering, lighting & materials 105 minutes 65 minutes Simple mesh editing 30 minutes 20 minutes Basic animation 45 minutes 35 minutes For the Advanced course it is hard to give a general indication of the expected time investment needed for the course. It depends partially on your own goals and ambitions for the main task: the project of visualizing your own data in the way you see fit. In terms of topics the Advanced materials and Animation chapters are relatively straightforward and can probably be completed in a day. In contrast, Python scripting in Blender is a very extensive topic and can end up taking a lot of time if you want to work with the more complex parts of the API.","title":"Course setup"},{"location":"overview/setup/#course-setup","text":"Warn To be updated Info Although this course material is available online at any time, we only provide the support mentioned at scheduled course periods throughout the year. Please check the PRACE Training Centre Events calendar when the next Blender course is scheduled. We use a combination of different media within the course, but the basis is for you to follow the training in your own pace over a period of two weeks. The online material consists of: Videos that introduce new concepts and features within Blender. Slides (also presented as part of the videos) for explanations. These are basically the presentations we would normally do plenary. Exercises for you to explore new topics and to train your skills We have scheduled a few short plenary online sessions in the course period to provide general feedback and/or guidance.","title":"Course setup"},{"location":"overview/setup/#support","text":"Detailed interaction and support during the course period is provided through our Discord server. Here you can ask questions by chat, upload an image or (if needed) start a video session or share your screen with one of us . In the section 2021 Intr. to Blender Course there are two channels available: #chat-support : this is a (shared) text channel for interacting via chat #plenary-sessions : this is a voice/video channel in which we host the online sessions For one-on-one contact, including the option for screen sharing, right-click on one of our names as shown above. We will be active on Discord during office hours (CET time zone) and will try to also be on-line outside of those hours, all on a best-effort basis.","title":"Support"},{"location":"overview/setup/#data-files","text":"Most of the exercises require you to load a Blender scene file that we provide. These files can be found at https://edu.nl/8n7en . It is best to download the full content of the share to your local system using the Download button in the upper-right. This share contains: data - Blender files (and other data) for the assignments, with a subdirectory per chapter slides - The slides (in PDF) walkthroughs - Some of the files used in the presentations cheat-sheat-2.9.pdf - A 2-page cheat sheet with often used operations and their short cuts","title":"Data files"},{"location":"overview/setup/#time-investment","text":"The precise amount of time needed to follow this course depends for a large part on how much attention you devote to each topic, your available time, your learning pace, etc. However, the previous in-person course setup we used in previous years was a full-day course (but with quite a high pace). For the Basics course the time spent on the different subjects and their assignments in that setup is shown below. This might give you some idea on the relative depth of the topics. Topic Time in schedule (previous in-person course) Videos (this course) Introduction 30 minutes 5 minutes Blender basics 120 minutes 45 minutes Importing data 30 minutes 30 minutes Rendering, lighting & materials 105 minutes 65 minutes Simple mesh editing 30 minutes 20 minutes Basic animation 45 minutes 35 minutes For the Advanced course it is hard to give a general indication of the expected time investment needed for the course. It depends partially on your own goals and ambitions for the main task: the project of visualizing your own data in the way you see fit. In terms of topics the Advanced materials and Animation chapters are relatively straightforward and can probably be completed in a day. In contrast, Python scripting in Blender is a very extensive topic and can end up taking a lot of time if you want to work with the more complex parts of the API.","title":"Time investment"},{"location":"references/cheat_sheet/","text":"Cheat sheet \u00b6 With this course we provide a 2-page cheat sheet that lists basic and often-used operations and their shortcut keys. It also includes a summarize of major interface elements. The cheat sheet can be found here as a double-sided PDF, which can easily be printed.","title":"Cheat sheet"},{"location":"references/cheat_sheet/#cheat-sheet","text":"With this course we provide a 2-page cheat sheet that lists basic and often-used operations and their shortcut keys. It also includes a summarize of major interface elements. The cheat sheet can be found here as a double-sided PDF, which can easily be printed.","title":"Cheat sheet"},{"location":"references/community/","text":"Community resources \u00b6 On blenderartists.org lots of Blender users and artists are hanging out. There you can ask questions or feedback, show off your work or check out the vast amount of knowledge, tips and Blender renderings in the forums. BlenderNation gathers information on different topics and includes video tutorials, blog posts on art created with Blender and a lot more. The Blender subreddit contains many different posts, ranging from simple questions to artists show off their amazing work. Well-known artists and gurus working with Blender are: Jan van den Hemel shares many tips and tricks through Twitter, both on Blender usage as well as making a scene look a certain way. He also publishes these tricks in an e-book. Andrew Price ( twitter ) aka \"Blender Guru\" provides many cool tutorials on https://www.blenderguru.com/ and his YouTube channel . He is well-known for a multi-part tutorial series on modeling a realistic donut! Glex Alexandrov ( twitter and twitter ) aka \"Creative shrimp\" has some very creative and inspirational tutorials on his YouTube channel . Ian Hubert ( YouTube and twitter ), famous for his Lazy tutorials (very efficient 1 minute tutorials), has videos on advanced green screen techniques and VFX in Blender. Simon Thommes ( twitter and YouTube ) is a materials wizard, he is able to create complex geometry out of one cube or sphere with just the Shader editor. Steve Lund has some great Blender tutorials on his YouTube channel . Zach Reinhardt has some great modeling, texturing and VFX tutorials on his YouTube channel Peter France is the Blender artist at the Corridor Crew which just started his own YouTube channel with some instructive tutorials. YanSculpts does not fit this course material perse but it goes to show how versatile Blender can be, this artist creates some amazing sculptures in Blender of which he shows the process on his YouTube channel . Josh Gambrell shares a lot of tips and tricks for advanced mesh editing on his youtube channel (mostly hard surface modeling).","title":"Community resources"},{"location":"references/community/#community-resources","text":"On blenderartists.org lots of Blender users and artists are hanging out. There you can ask questions or feedback, show off your work or check out the vast amount of knowledge, tips and Blender renderings in the forums. BlenderNation gathers information on different topics and includes video tutorials, blog posts on art created with Blender and a lot more. The Blender subreddit contains many different posts, ranging from simple questions to artists show off their amazing work. Well-known artists and gurus working with Blender are: Jan van den Hemel shares many tips and tricks through Twitter, both on Blender usage as well as making a scene look a certain way. He also publishes these tricks in an e-book. Andrew Price ( twitter ) aka \"Blender Guru\" provides many cool tutorials on https://www.blenderguru.com/ and his YouTube channel . He is well-known for a multi-part tutorial series on modeling a realistic donut! Glex Alexandrov ( twitter and twitter ) aka \"Creative shrimp\" has some very creative and inspirational tutorials on his YouTube channel . Ian Hubert ( YouTube and twitter ), famous for his Lazy tutorials (very efficient 1 minute tutorials), has videos on advanced green screen techniques and VFX in Blender. Simon Thommes ( twitter and YouTube ) is a materials wizard, he is able to create complex geometry out of one cube or sphere with just the Shader editor. Steve Lund has some great Blender tutorials on his YouTube channel . Zach Reinhardt has some great modeling, texturing and VFX tutorials on his YouTube channel Peter France is the Blender artist at the Corridor Crew which just started his own YouTube channel with some instructive tutorials. YanSculpts does not fit this course material perse but it goes to show how versatile Blender can be, this artist creates some amazing sculptures in Blender of which he shows the process on his YouTube channel . Josh Gambrell shares a lot of tips and tricks for advanced mesh editing on his youtube channel (mostly hard surface modeling).","title":"Community resources"},{"location":"references/interface/","text":"User Interface elements \u00b6 The default layout of the Blender user interface is shown below. Note that the layout is fully configurable. * Scene statistics By default the status bar at the bottom only shows the Blender version number. You can add extra statistics, such as the number of 3D objects in the scene and memory usage in the preferences. You can either right-click on the status bar to enable display of extra values. Or use the application menu Edit > Preferences , select the Interface tab, in the Editors > Status Bar section and check all marks ( Scene Statistics , System Memory , Video Memory ). Editor type menu \u00b6 The yellow highlight indicates often used ones for this course","title":"User Interface elements"},{"location":"references/interface/#user-interface-elements","text":"The default layout of the Blender user interface is shown below. Note that the layout is fully configurable. * Scene statistics By default the status bar at the bottom only shows the Blender version number. You can add extra statistics, such as the number of 3D objects in the scene and memory usage in the preferences. You can either right-click on the status bar to enable display of extra values. Or use the application menu Edit > Preferences , select the Interface tab, in the Editors > Status Bar section and check all marks ( Scene Statistics , System Memory , Video Memory ).","title":"User Interface elements"},{"location":"references/interface/#editor-type-menu","text":"The yellow highlight indicates often used ones for this course","title":"Editor type menu"},{"location":"references/official/","text":"Official sources \u00b6 The official home for Blender is blender.org The Blender Reference Manual can be found here . The documentation on the Python API here . Tip You can open the Blender documentation pages from within Blender itself, using the options in the Help menu. Official demo files showing off lots of cool features and scenes can be found here , including the scene files used to render the splash images of different Blender versions. If you are interested in following recent development of new features in Blender then the Blender Today Live channel on YouTube is a good resource. On Twitter you can follow @Blender for official Blender news or @BlenderDev for more in-depth development information. The hashtag to use for Blender is #b3d .","title":"Official sources"},{"location":"references/official/#official-sources","text":"The official home for Blender is blender.org The Blender Reference Manual can be found here . The documentation on the Python API here . Tip You can open the Blender documentation pages from within Blender itself, using the options in the Help menu. Official demo files showing off lots of cool features and scenes can be found here , including the scene files used to render the splash images of different Blender versions. If you are interested in following recent development of new features in Blender then the Blender Today Live channel on YouTube is a good resource. On Twitter you can follow @Blender for official Blender news or @BlenderDev for more in-depth development information. The hashtag to use for Blender is #b3d .","title":"Official sources"},{"location":"references/scene/","text":"Scene resources (3D models, materials, textures) \u00b6 Here we list a number of online resources for 3D models, textures, shaders, etc. In general certain 3D models might be free for download, while others might only be available paid (usually for a small amount). Usually, the nicer the 3D model the higher the cost. Also, different licenses are used for the models and these will describe how you can use the models and any attribution you might need to give when using it. 3D Models \u00b6 Turbosquid is one of the oldest 3D model websites and provides models in all sort of topics, some free, some paid. Sketchfab hosts a large collection of 3D models from many different categories. Many 3D models are textured and some are even animated. 3D Model Haven distributes freely usable 3D models, many of them textured. It is not as extensive as other websites, but the upside is that all models can be freely used. CGTrader also hosts many 3D models, some of them free, some paid There's a section on BlenderNation where Blender models are shared. Again, some of these might be free, others will involve some payment. BlenderMarket contains a section with 3D models Quixel's Megascans is a great \"paid\" source for 3D models as well as textures which can be used for free when it's attached to an Epic account and the assets are only used for an Unreal Engine application. It's great for personal use but if you publish anything containing an asset from Quixel without Unreal Engine attached to it you have to pay for the asset. Textures and images \u00b6 Texture Haven provides textures to be used in materials and shaders. All textures available are free. CC0 Textures has many high-quality textures BlenderMarket has a section with shaders, materials and textures . HDRI Haven is similar to Texture Haven, but contains many freely available HDRI 360 images that can be used for realistic environment lighting in Blender Poliigon , where the CEO is the Blender Guru himself, has some great looking free samples and otherwise high quality paid textures. textures.com has some high quality, high resolution, movie grade textures under a paid subscription or credit-based payment model. BlenderKit \u00b6 BlenderKit is an online repository of materials, 3D models and a few other things. It used to come bundled with Blender as an add-on, but since Blender 3.0 this is no longer the case. You need to download and install the add-on yourself, for which instructions can be found here . When the add-on is installed and enabled it provides some extra elements in the Blender interface for searching, say a material or 3D model, by name, which can then be easily used in a Blender scene: Note that many of the assets in BlenderKit are free, but some are only available by buying a subscription. The add-on has quite a few options and performs certain operations that you would otherwise do manually or maybe not use at all. As such, it can set up the scene in more exotic ways, for example by linking to another Blender file. Also, the materials provided by BlenderKit can use pretty complex shader graphs, involving multiple layers of textures, or advanced node setups. Warning When applying a BlenderKit material on your own object the rendering might not look like the material preview in all cases. Especially use of displaced materials involves specific settings for the Cycles renderer and use of subdivision on the object. Warning Textures from BlenderKit are by default stored in a separate directory on your system ( ~/blenderkit_data on Linux). There is an option to pack the textures within the Blender file, making it larger in size but also completely independent of any external files, which is useful if you want to transfer the Blender file to a different system. The option for packing files is File > External Data > Pack All into .blend .","title":"Scene resources (3D models, materials, textures)"},{"location":"references/scene/#scene-resources-3d-models-materials-textures","text":"Here we list a number of online resources for 3D models, textures, shaders, etc. In general certain 3D models might be free for download, while others might only be available paid (usually for a small amount). Usually, the nicer the 3D model the higher the cost. Also, different licenses are used for the models and these will describe how you can use the models and any attribution you might need to give when using it.","title":"Scene resources (3D models, materials, textures)"},{"location":"references/scene/#3d-models","text":"Turbosquid is one of the oldest 3D model websites and provides models in all sort of topics, some free, some paid. Sketchfab hosts a large collection of 3D models from many different categories. Many 3D models are textured and some are even animated. 3D Model Haven distributes freely usable 3D models, many of them textured. It is not as extensive as other websites, but the upside is that all models can be freely used. CGTrader also hosts many 3D models, some of them free, some paid There's a section on BlenderNation where Blender models are shared. Again, some of these might be free, others will involve some payment. BlenderMarket contains a section with 3D models Quixel's Megascans is a great \"paid\" source for 3D models as well as textures which can be used for free when it's attached to an Epic account and the assets are only used for an Unreal Engine application. It's great for personal use but if you publish anything containing an asset from Quixel without Unreal Engine attached to it you have to pay for the asset.","title":"3D Models"},{"location":"references/scene/#textures-and-images","text":"Texture Haven provides textures to be used in materials and shaders. All textures available are free. CC0 Textures has many high-quality textures BlenderMarket has a section with shaders, materials and textures . HDRI Haven is similar to Texture Haven, but contains many freely available HDRI 360 images that can be used for realistic environment lighting in Blender Poliigon , where the CEO is the Blender Guru himself, has some great looking free samples and otherwise high quality paid textures. textures.com has some high quality, high resolution, movie grade textures under a paid subscription or credit-based payment model.","title":"Textures and images"},{"location":"references/scene/#blenderkit","text":"BlenderKit is an online repository of materials, 3D models and a few other things. It used to come bundled with Blender as an add-on, but since Blender 3.0 this is no longer the case. You need to download and install the add-on yourself, for which instructions can be found here . When the add-on is installed and enabled it provides some extra elements in the Blender interface for searching, say a material or 3D model, by name, which can then be easily used in a Blender scene: Note that many of the assets in BlenderKit are free, but some are only available by buying a subscription. The add-on has quite a few options and performs certain operations that you would otherwise do manually or maybe not use at all. As such, it can set up the scene in more exotic ways, for example by linking to another Blender file. Also, the materials provided by BlenderKit can use pretty complex shader graphs, involving multiple layers of textures, or advanced node setups. Warning When applying a BlenderKit material on your own object the rendering might not look like the material preview in all cases. Especially use of displaced materials involves specific settings for the Cycles renderer and use of subdivision on the object. Warning Textures from BlenderKit are by default stored in a separate directory on your system ( ~/blenderkit_data on Linux). There is an option to pack the textures within the Blender file, making it larger in size but also completely independent of any external files, which is useful if you want to transfer the Blender file to a different system. The option for packing files is File > External Data > Pack All into .blend .","title":"BlenderKit"}]}