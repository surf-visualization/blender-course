{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>These pages form the main online content for the two modules of the course Introduction to Scientific Visualization with Blender.  In this course you will learn how to use the 3D rendering and animation package Blender for creating images and animations from (scientific) data. </p> <p>This course consists of two parts: Basics and Advanced. The Basics part assumes no knowledge of Blender, while the Advanced part builds upon the skills and knowledge of the Basics part.</p> <p>We provide this course either as an online self-paced course over a period of 2-3 weeks, or as an in-person full-day course. Please check the Schedule and News pages for upcoming dates. Or search the Euro CC course agenda for the course modules:</p> <ul> <li>Introduction Scientific Visualisation with Blender: Data, Lights, Camera, Action!</li> <li>Advanced topics in scientific visualization with Blender: Geometry, Scripts, Animation, Action!</li> </ul> <p>This course is created and maintained by the visualization team of the SURF High-Performance Computing and Visualization group. This course is provided by SURF within the context of the EuroCC Netherlands NCC. We have been providing this course since 2018, usually twice a year, and initially in-person. Due to the  restrictions during the COVID-19 lock-down period we decided to turn this course into a fully online version, based on positive experiences with the first advanced Blender course we provided online in 2020. </p>"},{"location":"privacy/","title":"Privacy and cookie statement","text":""},{"location":"privacy/#privacy","title":"Privacy","text":"<p>No personal information is gathered by SURF of  visitors to this course website. </p>"},{"location":"privacy/#cookies","title":"Cookies","text":"<p>No cookies are used for the content published by SURF on this website, nor is any personal information about visits tracked by SURF. </p> <p>The underlying MkDocs content generation system uses the browser's session storage for storing general site-map data (called <code>/blender-course/.__sitemap</code>), which is sometimes reported as a cookie.</p>"},{"location":"privacy/#third-party-cookies","title":"Third-party cookies","text":"<p>The embedded videos are hosted on YouTube, but using its privacy-enhanced mode and the \"www.youtube-nocookie.com\" domain. YouTube might ask for placement of third-party cookies, in which case explicit permission needs to be granted by the user. For more information, see the privacy controls of YouTube and the information linked from that page.</p> <p>This website is hosted through GitHub Pages, which might set third-party cookies in which case explicit permission needs to be granted by the user. See here for the GitHub privacy policy.</p>"},{"location":"advanced/introduction/","title":"Introduction","text":"<p>The Advanced part of the course consists of a number of separate topics, each with a number of assignments:</p> <ul> <li>Python scripting for performing all kinds operations using code</li> <li>Advanced materials using the node-based shaders</li> <li>An introduction to Animation techniques</li> <li>Mesh edit mode for cleaning up and/or improving your (imported) meshes</li> </ul> <p>The final assignment is your own personal project of your choosing. If you want you can also work with a dataset we provide.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/","title":"\ud83d\udcbb The Shader Editor and advanced materials","text":"<p>In these two exercises in this chapter you will use the Blender Shader Editor on the familiar iso-surface of a CT scan of a fish from the basic course and try to make a visualization by using an advanced node setup. After that you will make a render of the moon with the high resolution textures of NASA with adaptive subdivision.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#the-fish","title":"\ud83d\udcbb The fish","text":"<p>When you opened the exercise blend file <code>advanced_materials_assignment.blend</code> you'll see the white fish iso-surface above a plane white plane. We are going to pimp this scene with advanced materials.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#shader-editor-materials-coloring-the-scene","title":"Shader editor materials - Coloring the scene","text":"<p>First we will add materials and give each object a different color.</p> <ol> <li>First activate the Rendered shading to see what kind of materials we are actually applying by pressing <code>Z</code> in the 3D Viewport panel and selecting <code>Rendered</code> from the radial pie-menu.</li> <li>Select the fishskin object and add a new material by clicking the <code>New</code> button in the middle of the top bar of the Shader Editor  panel.</li> <li>Now we see a graph appearing with 2 nodes a Principled BSDF-node and a Material output-node also in the side panel you will see the familiar material settings. Change the <code>Base Color</code> to an appropriate color of a fish.</li> <li>Repeat step 2 and 3 for each 3D object in the scene (see Outliner) and give them a color of your choice.</li> </ol>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#texture-mapping-placing-the-fish-on-a-picknick-table","title":"Texture mapping - Placing the fish on a picknick table","text":"<p>Now that the scene has some color we can start applying some realistic colors and texture to the ground plane or should we say table? We will do that by adding wood textures to the ground plane and connecting those textures to their appropriate parameters of the Principle BSDF.</p> <ol> <li>Select the <code>groundplane</code> 3D object.</li> <li>Add a Image texture-node to the Shader Editor graph of the groundplane with <code>Shift-A &gt; Texture &gt; Image Texture</code>.</li> <li>Connect the <code>Color</code> output of this node to the <code>Base color</code> input of the Principled BSDF-node.</li> <li>Now the groundplane doesn't look anything like a picknick table, its pink. This pink color comes from the fact that an image is missing from the Image Texture-node. Open an image by pressing the <code>Open</code>-button on the Image Texture-node, this will open a file browser window. Now select the <code>blue_painted_planks_diff_4k.png</code> image from the <code>data/wood_textures/</code> directory and press <code>Open Image</code>.</li> </ol> <p>Now we have our first image mapped on an object! Although you might have noticed that the fish is really small or rather the planks are very big. We are gonna solve that by scaling the texture coordinates.</p> <ol> <li>Before we can do that we first need to add the texture coordinates to the graph with <code>Shift-A &gt; Input &gt; Texture Coordinates</code> and connect the <code>UV</code> output to the <code>Vector</code> input of the Image Texture-node.</li> <li>Nothing changed because we didn't apply the scaling yet. Now add a Mapping node with <code>Shift-A &gt; Vector &gt; Mapping</code> and drag it on top of the edge between the Texture Coordinate-node and the Image Texture-node and let it go. As you can see it is automatically connected in between the nodes.</li> <li>Now on the Mapping-node change the <code>Scale</code> parameter <code>x</code>,<code>y</code> and <code>z</code> to <code>2</code>. As you can see that reduced the planks to a smaller and better size.</li> </ol> <p>Tip!</p> <p>With the Node Wrangler Blender add-on you can just select a texture node and press <code>CTRL+T</code> to automatically add the Texture Coordinate and Mapping node. Node Wrangler can be added with: <code>Menu-bar Edit &gt; Preferences &gt; Add-ons tab &gt; Type 'Node Wranger' in search &gt; check Node Wrangler add-on to activate</code>.</p> <p>Now we'll roughen the planks a bit with a Roughness map, a texture that will be use to change the <code>Roughness</code> parameter of the Principled BSDF.</p> <ol> <li>Select the previously added Image Texture-node and press <code>Shift-D</code> and place the new duplicated node underneath the other Image Texture-node.</li> <li>Connect its <code>Vector</code> input to the <code>Vector</code> output of the Mapping-node just like the other Image Texture-node and connect the <code>Color</code> output to the <code>Roughness</code> input of the Principled BSDF-node.</li> <li>As you can see became shiny, which wood is not (rotate the view around the object in the 3D Viewport to see the plane from different angles). This is because we haven't changed the texture yet. In this new Image Texture-node <code>Open</code> the <code>blue_painted_planks_rough_4k.png</code> from <code>data/wood_textures</code>.</li> <li>Now it is still a bit too shiny for wood. This is because the output is interpreted as an <code>sRGB</code> value. We need to change the <code>Color Space</code> parameter of this Image Texture-node to <code>Non-color</code>. Now the ground plane has the right rough look like wood.</li> </ol> <p>The look of the wood is still very \"flat\" (the light still bounces of it at a straight angle), this is because we didn't add a normal map to the material yet. This normal map will accentuate all the nooks and crannies naturally present in wood which normally catch light to.</p> <ol> <li>As the previous <code>Image Texture</code>-node we again need to make a new one by duplicating (see step 8).</li> <li>Again the Mapping-node <code>Vector</code> output needs to be connected to the new Image Texture-node <code>Vector</code> input. The <code>Color</code> output however needs to go to a Normal Map-node.</li> <li>Add a Normal Map-node with <code>Shift-A &gt; Vector &gt; Normal Map</code> and connect the Image Texture-node <code>Color</code> output to the Normal Map-node <code>Color input</code> and connect the Normal Map-node <code>Normal</code> output to the Principled BSDF-node <code>Normal</code> input.</li> <li>Again this is also not a color so the <code>Color Space</code> needs to be set to <code>Non-color</code>.</li> </ol> <p>Now you have a fully textured wooden ground plane! To see the full effect, rotate the view around it and see the light bounce off the surface based on the different texture types you just applied.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#multiple-materials-one-object-window-to-the-inside-of-the-fish","title":"Multiple materials one object - Window to the inside of the fish","text":"<p>We only see the fish, not the fish bones. In the Blender Basics course we learned how to reveal the bones on the inside by using a Boolean modifier, but we can achieve the same with just materials!</p> <ol> <li>Select the <code>fishskin</code> 3D object.</li> <li>If everything in the first couple of assignments the fish should already have one material called Material. For administrative reasons lets <code>rename</code> the material by clicking its name Material in the middle of the top bar of the Shader Editor  panel and typing the new name called <code>fishskinmat</code>.</li> <li>Now left next to the rename box you have drop-down menu called <code>Slot 1</code> when you click this you will see the material slots menu. In our case its only one material called fishskinmat.</li> <li>Now add a new Material slot by clicking the <code>plus icon</code> in this menu. The added material slot is still empty and needs a second material.</li> <li>Add a new material by clicking the <code>New</code> button in the middle of the top bar of the Shader Editor  panel.</li> <li>Rename this material to <code>fishskintransparentmat</code>.</li> </ol> <p>Now as you can see adjusting any value on the Principled BSDF-node doesn't seem to do anything. This is because there aren't any vertices assigned to this material slot yet (by default all vertices are assigned to the first material slot).</p> <ol> <li>To assign vertices we need to be able to select them and this can be done in the <code>Edit Mode</code> of the <code>3D Viewport</code>-panel. With the fishskin 3D object selected and the focus on the 3D Viewport-panel (hovering over the 3D Viewport panel with your mouse) press <code>TAB</code>.</li> <li>First press 1 to see the vertices and then select a window of vertices on the side of the fish with the <code>Border select</code> tool by pressing <code>B</code> in the 3D Viewport-panel and dragging over the area you want to select.</li> <li>With these vertices selected press the <code>Material slots</code> button, select the <code>fishskintransparentmat</code>-material and press the <code>Assign</code>-button. </li> </ol> <p>Now you can see the selected faces in that selection look different! This is because they are assigned to the second material. Now we'll make the <code>fishskintransparentmat</code> actually transparent with a combination of the Transparent BSDF and Principled BSDF through a <code>Mix Shader</code>. That way we can control the amount of transparency!</p> <ol> <li>In the Shader editor add a <code>Mix Shader</code>-node with <code>Shift-A &gt; Shader &gt; Mix Shader</code>.</li> <li>Drag this Mix Shader-node over the edge connecting the Principled BSDF-node and the Material Output-node to place it connected in between.</li> <li>Now add a <code>Transparent BSDF</code> with <code>Shift-A &gt; Shader &gt; Transparent BSDF</code>.</li> <li>Connect the <code>BSDF</code> output to the Mix Shader-node <code>Shader</code> input.</li> <li>Now the material is half shaded by the Transparent BSDF-node and half by the Principled BSDF-node. Experiment with the Mix shader-node's <code>fac</code> parameter to see how it changes the transparency of the fishskintransparentmat.</li> </ol> <p>Now you have a window looking inside the fish! Now it's time to give the fish some actually fishy colors with the Project from view UV-mapping!</p> <p>Bonus (Only when you have time left): As you can see the bones also contain the swim bladder which looks the same as the bones because the same material is assigned to it. Try to select the swim bladders vertices and assign a different more fitting material to the swim bladder.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#project-from-view-uv-mapping-add-actual-skin-to-the-fish","title":"Project from view UV-mapping - Add actual skin to the fish.","text":"<p>To add a real fish texture, or actually a photo from a carp, to the <code>fishskin</code> 3D object you can use the technique called <code>Project from view</code> UV-mapping. For this we introduce a new panel called the <code>UV Editor</code>. Before we go to the <code>UV Editor</code> we need to add a Image Texture-node to the <code>fishskinmat</code>. </p> <ol> <li>In the Shader Editor select the <code>fishskinmat</code> (slot 1) from the Material slot menu in the middle left of the top bar of the Shader Editor.</li> <li>Add a Image Texture-node to the material with <code>Shift-A &gt; Texture &gt; Image Texture</code> and connect the <code>Color</code> output to the Principled BSDF-node <code>Base Color</code> input and open the <code>carp.jpg</code> texture from the <code>data/</code> directory.</li> <li>Next add a Texture Coordinate node with <code>Shift-A &gt; Input &gt; Texture Coordinates</code> and connect the <code>UV</code> output to the Image texture-node <code>Vector</code> input.</li> </ol> <p>This fish is now black because the <code>UV</code> coordinates are not defined yet. That is what we will do in the <code>UV Editor</code>.</p> <ol> <li>Now that we do not need the <code>Shader editor</code> anymore we can replace it with the <code>UV Editor</code>. In the corner of the panel click the Editor Type-button  and select the <code>UV Editor</code> from the list.</li> <li>Before we can start UV-mapping we need to be in <code>Edit mode</code> in the 3D viewport. In the 3D viewport panel press <code>TAB</code> to enter edit mode.</li> <li>Now select all geometry by pressing <code>A</code>.</li> </ol> <p>To properly project from view you have to choose the right view to project from. We are gonna map a photo of a carp which has been taken from the side. In order to properly map the photo on the 3D object we also need to look at it from the side.</p> <ol> <li>Press <code>`</code> (the back-tick) to open the view radial pie-menu and select <code>Right</code> or through the 3D Viewport menu in the header (<code>View &gt; Viewpoint &gt; Camera</code>).</li> <li>Now press <code>U</code> to open the UV-mapping-menu and select <code>Project from view</code>.</li> </ol> <p>Now you can see the <code>UV</code> coordinates are mapped in the <code>UV Editor</code> but they are not properly scaled to fit the photo of the carp.</p> <ol> <li>Make sure that everything is still selected and then within the <code>UV Editor</code> press <code>S</code> and scale the UV-coordinates until they aligns with the photo of the carp. </li> <li>Scaling it alone is not enough. The UV-coordinates need to be moved a bit, use <code>G</code> to grab the UV-coordinates and translate them to better match the photo.</li> </ol> <p>As you might have noticed it is not possible to completely match the photo without deforming the UV-coordinates.</p> <ol> <li>Before we start deforming parts of the UV-coordinates you need to activate Proportional editing by pressing the <code>Proportional editing</code> button  in the top bar of the <code>UV Editor</code>. This proportional editing moves all UV-coordinates in the adjacent defined radius along with the currently selected UV-coordinates.</li> <li>Now select a UV-coordinate in the UV Editor that needs to be moved and press <code>G</code>.</li> <li>While grabbing, scroll with your mouse wheel to decrease or increase the Proportional editing radius and move your mouse to see the effect.</li> <li>Now with this <code>Proportional editing</code> try to match the UV-coordinates to the photo of the carp as good as possible.</li> </ol> <p>Tip!</p> <p>Whenever you are editing the UV-map in the UV editor it can be difficult to see how the texture is mapped on the 3D-object because of the visibility of all vertices, edges and faces because of the activated Edit mode. You can toggle between Edit mode and Object mode in the 3D Viewport panel to have a better look at the mapped texture.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#the-moon","title":"\ud83d\udcbb The moon","text":"<p>This moon exercise doesn't have a prepared blend file because you are gonna make it all by yourself! So open a new blend file and start to make the moon.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#the-basic-scene-sphere-sun-and-the-darkness-of-space","title":"The basic scene - Sphere, sun and the darkness of space","text":"<p>To create the moon we first need to prepare a very simple scene.</p> <ol> <li>First off we need to remove the Default cube (the cube that comes with a new blend file which only function is to be removed :( ).</li> <li>Add a <code>UV Sphere</code> instead with <code>Shift-A &gt; Mesh &gt; UV sphere</code>.</li> <li>Set the <code>UV Sphere</code>'s shading to smooth through the 3D Viewport menu in at the top of the <code>3D Viewport</code> (<code>Object &gt; Shade Smooth</code>).</li> <li>Select the default <code>Light</code> object in the Outliner and change it to a Sun light in the Light-tab  in the Properties-panel on the right.</li> <li>Now change the shading in the 3D viewport to <code>Rendered</code> by pressing <code>Z</code> and then select <code>Rendered</code>. This rendered view is by default set to Eevee, to change that to Cycles for more realistic lighting go to the Render Properties-tab  in the Properties-panel and change the <code>Render Engine</code> to Cycles.</li> <li>As you can see the sun is now way too bright. Lower the <code>Strength</code> of the sun from 1000 to <code>10</code> in the Light-tab in the Properties-panel. No need to have the power of a 1000 suns.</li> <li>Now that we have the sun we need to disable the World-lighting (the grey ambient light) since we only need the sun as a direct light source like it is in space. Go to the World properties-tab in the Properties-panel and set the <code>Color</code> in the Surface-section all the way to black.</li> </ol> <p>Now we have the basic scene of a sphere in space, now we are gonna make it look like the moon by adding textures.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#applying-a-material-and-texturing-the-moon-thats-one-small-step","title":"Applying a material and texturing the moon - That's one small step...","text":"<p>Before we can edit the material we need to open the Shader Editor. For this we need to slightly modify the interface.</p> <ol> <li>Grab the edge between the 3D viewport-panel and the Timeline-panel by hovering above the edge until you see resize cursor  then click and drag the edge until half of the Blender window.</li> <li>Now click the upper left Editor type dropdown menu (now the Timeline-icon ) and select the Shader Editor.</li> <li>In the Shader Editor add a new material.</li> <li>In this material add <code>2</code> Image Texture-nodes, <code>1</code> Texture Coordinate-node and <code>1</code> Displacement-node (<code>Shift-A &gt; Vector &gt; Displacement</code>).</li> <li>Connect the Texture Coordinate-node <code>UV</code> output to both Image Texture-nodes <code>Vector</code> inputs.</li> <li>Connect one of the Image Texture-nodes <code>Color</code> output to the Principled BSDF-node <code>Base Color</code> input and the others <code>Color</code> output to the Displacement-node <code>Height</code> input. </li> <li>Finally connect the Displacement-node <code>Displacement</code> output to the Material output-node <code>Displacement</code> input.</li> <li>Open the <code>data/moon_textures/lroc_color_poles_8k.tif</code> in the Image Texture-node that is connected to the <code>Principled BSDF</code>-node <code>Base Color</code>.</li> <li>Open the <code>data/moon_textures/ldem_16.tif</code> in the Image Texture-node that is connected to the Displacement-node <code>Height</code> input.</li> <li>Then finaly set the Image Texture-node <code>Color Space</code>-parameter of the node with the displacement texture to <code>Non-Color</code>. </li> <li>Initially the Displacement-node <code>Scale</code> parameter is set way too high making the moon look horrible. Set this parameter to <code>0.001</code>.</li> </ol> <p>As you can see it already looks quite like the moon but with some final tweaking you will get even more realism.</p>"},{"location":"advanced/advanced_materials/advanced_materials_assignment/#adaptive-displacement-revealing-the-craters-mooore-details","title":"Adaptive displacement - Revealing the craters! Mooore details!","text":"<p>Everything we have seen until now has been rendered in the default EEVEE rendering engine, which is for visualization purposes very powerful, but if you want to add that extra little realism with adaptive displacement you have to use the Cycles rendering engine.</p> <ol> <li>Active the <code>Cycles</code> rendering engine with the <code>Render Engine</code> setting in the Rendering properties-tab  of the Properties-panel. </li> </ol> <p>While we are there, to be able to use adaptive displacement, we need to activate the Cycles experimental feature set.</p> <ol> <li>Set the <code>Feature Set</code> to <code>Experimental</code>.</li> <li>This Experimental feature set added an extra section in the current properties panel tab called <code>Subdivision</code>. In this section set <code>Viewport</code> to <code>2</code>.</li> </ol> <p>Now we need to add a <code>Subdivision modifier</code> that also got a new setting from the Experimental feature set that enables the adaptive displacement. </p> <ol> <li>Add a <code>Subdivision modfier</code> in the Modifier properties-tab  of the Properties-panel.</li> <li>Enable the Adaptive Subdivision setting in this modifier.</li> </ol> <p>Until now you only saw some slight differences because there is only one setting that has to be changed to make all of this worth it.</p> <ol> <li>Change the <code>Displacement</code> setting to <code>Displacement Only</code> in the Properties-panel &gt; Material properties-tab  &gt; Settings-section &gt; Surface-subsection.</li> <li>Now zoom in and toggle to the Edit mode and back, which re-triggers the adaptive subdivision computations, and see the craters in their full glory.</li> </ol> <p>Bonus: For an artists rendition of the moon change the Displacement-node <code>Scale</code> parameter to a higher value and see how the craters get more noticeable (although less realistic).</p>"},{"location":"advanced/advanced_materials/introduction/","title":"Introduction","text":"<p>This chapter will introduce the Shader Editor and UV Editor of Blender which lets you create advanced materials to improve the look of your visualizations. The Shader editor and UV editor go hand in hand, with the UV-editor (and 3D viewport) you'll learn how to UV-unwrap your meshes and manipulate the UV-coordinates and with the Shader editor you'll project procedural or image textures based on the created UV-coordinates. </p> <p>You'll learn how to apply PBR (Physically based rendering) style textures and where to find them, to make your objects look photo real. </p> <p>And lastly a commonly used experimental feature called Adaptive Subdivision will be combined with vertex displacement to create some great looking micro-displacement details on the surfaces of your objects.</p> <p>Before you start with the exercises the following video will give you the theoretical and practical background to make these exercises. In this video there are some Blender walk-throughs, if you want to follow along you can use the walk-through files in the <code>walkthroughs/advanced/advanced_materials</code> directory.</p> <p>Blender 4.1 changes</p> <p>The walkthrough video was recorded with Blender 2.92, however the current version (Blender 4.1) used for the this course has some additional changes and features. Most are minor UI changes which will not conflict with the course material, although some of them are worth to mention and could cause confusion. These changes are:</p> <ul> <li>The Principled BSDF node menu is subdivided into sub-menus in Blender 4.1, giving it a better overview. Luckily all inputs that are hidden under a sub-menu will not be used in the course material.</li> <li>The strength input for the <code>Normal map</code>-node handles the strength values differently now. Previously the value of <code>1.0</code> was too strong, but now that value is fine while the value <code>0.1</code> is too small and not noticeable.</li> <li>The modifiers in the add-modifier menu are now shown in sub-menus instead of a column-based view. </li> </ul> <p>After you watched the video about advanced materials you are ready for the exercises!</p>"},{"location":"advanced/advanced_materials/node-wrangler/","title":"Node-wrangler reference","text":"<p>The node-wrangler add-on brings a wide variety of new features and hot-keys to automate steps within the Shader Editor to make life easier. In the walk-through only 2 features where shown, the Shader viewer (<code>Ctrl+Shift+LMB</code>) and Add Texture Setup (<code>Ctrl+T</code>), two very useful hot-keys but this is only the tip of the iceberg.</p> <p>To see the full set of features/hotkeys that node-wrangler provides you need to go to <code>Menu bar 'Edit' &gt; Preferences... &gt; Tab 'Add-ons' &gt; Search for 'Node wrangler' &gt; Show Hotkey List</code> (see image below). For additional information on what each individual feature does please refer to the official documentation. </p> <p></p>"},{"location":"advanced/advanced_materials/vertex_colors/","title":"Visualizing vertex colors with the Attribute node","text":"<p>In the basics course we already introduced the use of vertex colors with the Material-tab  in the Properties-panel. What happens under the hood is that you basically add an Attribute-node to the node-network and attached its Color-output to the Base Color-input of the Principled BSDF shader-node (see images below).</p> <p> Shader Editor node-network </p> <p> 3D viewport result </p> <p>The blend file for the image above, <code>vertex-color.blend</code>, can be found among the walk-through files in the <code>walkthroughs/advanced/advanced_materials</code> directory.</p>"},{"location":"advanced/animation/2_assignment_cars/","title":"\ud83d\udcbb \"Cars\": the movie","text":"<p>In this exercise you can do some more complex keyframe animation by having multiple objects move to create a city full of driving cars. You will need basic keyframing skills and use of the Graph Editor.</p> <ol> <li>Load <code>cars.blend</code></li> </ol> <p>This scene has a very simple city with some building and some cars. An animation of 250 frames has been set up in the file, starting at frame 1, ending at frame 250.</p> <p>Tip</p> <p>All the geometry of the buildings is in the so-called collection \"Collection 2\". You can hide all these objects by clicking the eye icon right of \"Collection 2\" in the outliner.</p> <ol> <li>Change to the first frame in the animation with <code>Shift-Left</code>. Note that you can    see the current frame you're working in by the blue vertical line in the Timeline    at the bottom. Also, in the 3D view there's a piece of text in the upper-left    that reads <code>(1) Scene Collection | Plane</code>: the current frame is listed between    the parentheses.</li> <li>In the scene there's two cars behind each other. Select the front car of the two.</li> <li>Enter a keyframe for the car's location and rotation: press <code>I</code> followed    by picking <code>LocRot</code></li> <li>Change to the last frame in the animation with <code>Shift-Right</code></li> <li>Move the car to the end of the road it's on, along the Y axis</li> <li>Enter another <code>LocRot</code> keyframe with <code>I</code></li> <li>Check the car movement by playing back the animation with <code>Space</code>,    or by changing the time in the Timeline editor with <code>Shift-RMB</code></li> </ol> <p>The car's speed currently is not constant: it speeds up near the beginning of the animation and slows down starting somewhere halfway. We can edit the curve for the Y location channel in the Graph Editor to influence this behaviour.</p> <ol> <li>In the Graph Editor on the left of the screen show all the location and rotation values being    animated for the selected car by using the little triangle left of the    name <code>Object Transforms</code>. Below the <code>Object Transforms</code> you should now see the 6 channels for which you    created keyframes in steps 4 and 7: X, Y and Z Location, and X, Y and Z Euler Rotation.</li> <li>Click the eye icon next to <code>Object Transforms</code> to hide all the channels. Then click the eye    next to <code>Y Location</code> to only show the graph for the Y location. Note that you can    use the <code>Home</code> key to zoom to the full extent of the graph.</li> </ol> <p>You should now see a curved line in green with two orange filled circles at the times of the beginning and end of the animation, i.e. frames 1 and 250. Attached to the squares are \"handles\" (the lines that end in open circles) that influence the shape of the curve.</p> <ol> <li>Select the open circular endpoints of the handles and move them around. See what this does for the shape of the curve and the subsequent behaviour of the car in the animation.</li> </ol> <p>The two curve points are selectable with <code>Shift-LMB</code>, but also, for example, border select (<code>B</code> key). This works just like you normally select objects. Deleting keyframes can then be done with <code>X</code>.</p> <ol> <li>Select both curve points with <code>A</code>, Press <code>V</code> to bring up the Keyframe Handle type. This menu allows you to change how the curve is shaped based on the position of the handles.</li> <li>Select <code>Vector</code>. Notice how the curve's shape changes. See what happens when you move the handle endpoints.</li> <li>Press <code>V</code> again and choose <code>Free</code>. Again change the handle endpoints.</li> <li>Try out how the different curve shapes you can produce influence the car behaviour.</li> </ol> <p>Now let's animate another car: the one at the start of the road with the bend in it.</p> <ol> <li>Animate the second to move over the bended road all the way to the end.</li> </ol>"},{"location":"advanced/animation/2_assignment_cars/#bonus","title":"Bonus","text":"<p>Make the cars drive over the road, choosing yourself which cars goes in what direction, how fast, which turns are made, etc. But don't make cars go through each other and have them wait if needed.</p> <p>Add a camera that shows the busy streets in action :)</p>"},{"location":"advanced/animation/3_assignment_flipbook/","title":"Flipbook animation","text":"<p>As mentioned in this chapter's video, flipbook animation is a simple animation technique in which a mesh is changed over time. Such a changing mesh occurs quite frequently in (scientific)  simulations.</p> <p>In general there's two different situations when it comes to an animated mesh:</p> <ul> <li>The mesh topology stays fixed over time, but the vertex positions change each time step</li> <li>The mesh topology and its vertices change over time</li> </ul> <p>The exercise below shows a general technique how to handle any set of animated meshes (so works for both types above). The meshes are loaded individually from files. This technique has no restrictions on changing mesh topology, but is somewhat involved as it uses a Python script to set up the animation. </p> <p>Below we also describe two modifiers that are available in Blender, each usable for one of the types above.</p> <p>For the second case specifically, fixed topology and only changing vertex positions,  the chapter on shape keys describes a simpler alternative animation method.</p>"},{"location":"advanced/animation/3_assignment_flipbook/#using-python-to-set-up-an-animated-mesh","title":"\ud83d\udcbb Using Python to set up an animated mesh","text":"<p>Here, we'll get more familiar with the flipbook animation approach, in which a series of meshes is  animated over time by switching a single object's mesh data each frame. The approach we use here is to have a single mesh object on which we change the associated mesh data each frame.  So even though all timesteps are loaded only one of them is visible at a time. </p> <p>We take advantage of the Blender scene organization, where each object (Mesh) refers  to object data (one of the meshes in the animation). We use a small Python script, called a frame handler, to respond to a change of the current animation time.</p> <p>Data files and scripts</p> <p>The data for this example can be found in the data share https://edu.nl/hrvbe under <code>data/animation</code>.</p> <p>The <code>animated_ply_imports.blend</code> scene file contains two Python scripts  in the Text Editor called <code>1. import ply files</code> and <code>2. register anim handler</code>.</p> <p>The <code>dambreak.tar.gz</code> file contains a set of animated meshes in binary PLY format and so is somewhat large (255MB) when extracted.</p> <ol> <li> <p>Extract <code>dambreak.tar.gz</code> in the same directory as <code>animated_ply_imports.blend</code>. These files are located in the <code>data/advanced/animation</code> directory.</p> </li> <li> <p>Load <code>animated_ply_imports.blend</code></p> <p>This blend file contains not only a 3D scene, but also some Python scripts we use to set up the flipbook animation.</p> </li> </ol> <p>The first step is to load all the timesteps in the dataset using one of the scripts. This might take a bit of time, depending on the speed of your system. By default, only the first 100 steps are loaded. You can increase the number of files to the full 300 if you like by updating the variable <code>N</code> in both the import script and the animation handler script.</p> <ol> <li> <p>Execute the script that imports the PLY files of the time steps. To do     this make sure the script called <code>1. import ply files</code> is shown     in the text editor panel. Then press the  button in the top bar to run the script.</p> <p>Memory usage</p> <p>This step will import all meshes in the animation into the current scene. This uses a bit of memory (around 2.0 GB for all 300 steps in our own tests).</p> </li> <li> <p>Depending on your system loading all the PLY files may take a few seconds.    The cursor will change to an animated circle, indicating the    import is running. In case you get the idea something is wrong, check the console    output in the terminal where you started Blender, to see if there    are any error messages.</p> </li> <li> <p>After all PLY files are loaded, execute the script that installs the    frame change handler. This script is called <code>2. register anim handler</code>.    Make sure the text editor is switched to this script and press the play button.</p> </li> <li> <p>Verify that the flipbook animation works with <code>Space</code> and/or    moving the time slider in the Timeline with <code>Shift-RMB</code>.    You should see the fluid simulation evolve with each frame in the 3D viewport.     You can also check the object data associated with the <code>Fluid sim</code> object in the Outliner    to see that it changes.   </p> <p>The playback speed will not only depend on the framerate setting, but also on your system's performance</p> </li> <li> <p>Change the <code>Frame Rate</code> value (in the <code>Output</code> properties tab    at the right side of the screen, icon ) to different values to see how your system handles it.    Is 60 fps feasible?</p> </li> <li> <p>The <code>Fluid sim</code> object is still transformable as any normal object.    Experiment with this, to see how it influences the flipbook animation.</p> </li> </ol>"},{"location":"advanced/animation/3_assignment_flipbook/#bonus","title":"Bonus","text":"<p>Use your skills with keyframe animation to do one of the following things (or both if you feel like it ;-)):</p> <ul> <li>Have a camera follow the moving water in some cool way</li> </ul> <ul> <li>Place a surfer on the moving wave of water. You can import the PLY model <code>silver_surfer_by_melic.ply</code> to use as 3D model. You can load it in Blender with <code>File &gt; Import &gt; Stanford PLY (.ply)</code>.</li> </ul>"},{"location":"advanced/animation/3_assignment_flipbook/#alternatives-using-modifiers","title":"Alternatives using modifiers","text":"<p>The above method uses a bit of a hack with Python to set up mesh changes over time. Although it's flexible (it can work with any type of file format by editing the import code), it is also a bit fragile, needs to load all meshes in memory all at once, etc.</p> <p>In recent versions of Blender two modifiers were introduced that can be used for similar animation setups, although they each have their limitations. We describe them here in case they are useful for certain situations you might encounter.</p>"},{"location":"advanced/animation/3_assignment_flipbook/#mesh-sequence-cache-modifier-usd-and-alembic","title":"Mesh Sequence Cache Modifier (USD and Alembic)","text":"<p>The Mesh Sequence Cache Modifier takes one or more Alembic or USD files and sets up a time-varying mesh from those. The animated mesh data can either come from a single file (containing multiple time steps), or from multiple files (each containing a single time step).</p> <p>The limitation of only supporting Alembic and USD file formats is somewhat unfortunate, but understandable, since those formats support storing animated meshes in a single file and they are used extensively in visual effects and animation.</p> <p>If you want to use this modifier then you need to create an Alembic or USD file (or set of files) containing your animated mesh. If you then import that file the Mesh Sequence Cache modifier will be added automatically to set up the animation.</p> <p>Tip</p> <p>An example USD file to load can be found in <code>data/advanced/animation/animated_plane.usdc</code>. The file was created by exporting the example animation described below (involving <code>gen_pc2_anim.py</code>) from Blender to a USD file.</p>"},{"location":"advanced/animation/3_assignment_flipbook/#mesh-cache-modifier","title":"Mesh Cache Modifier","text":"<p>The Mesh Cache Modifier works somewhat differently in that it is applied to an existing mesh object and will  animate vertex positions (only) of that mesh. The modifier supports reading the animated vertex data from a MMD or PC2  file. </p> <p>Fixed mesh topology</p> <p>The animated vertex data in the MMD or PC2 file is assumed to use the same vertex order over all time steps. The animated mesh can also not have a varying number of vertices, or a changing topology.</p> <p>This means that, for example, the animated wave dataset from the exercise above cannot be represented as a series of .pc2 files, as the mesh size in vertices and its topology changes.</p> <p>The MDD file format is mostly used to exchange data with other 3D software, while the PC2 is a general and simple point cloud caching format. Blender contains add-ons for exporting MDD and PC2 files, but they are not enabled by default. When enabled you can use them to convert a mesh sequence in a different format to one of these.</p> <p>The PC2 file format is very simple, and can easily be written from, say, Python or C++. The format looks like this (based on information referenced here, and example Python code here):</p> <ul> <li> <p>The start of a .pc2 file is a 32-byte header containing:</p> <pre><code>char    cacheSignature[12];   // 'POINTCACHE2' followed by a trailing null character.\nint32   fileVersion;          // Currently 1\nint32   numPoints;            // Number of points (i.e. vertices) per sample\nfloat   startFrame;           // Frame number where animation starts\nfloat   sampleRate;           // Duration of each sample *in frames*\nint32   numSamples;           // Defines how many samples are stored in the file.\n</code></pre> </li> </ul> <ul> <li>Following the header, each set of point positions (collectively called a \"sample\") is stored consecutively. Each sample is stored one after the other as a flat array of x/y/z 32-bit floats for each point. So each sample uses <code>numPoints * sizeof(float) * 3</code> bytes.</li> </ul> <p>All in all, a .pc2 file provides a fairly compact method of storing a set of animated mesh vertices. Together with the Mesh Cache modifier they can be used to easily set up a mesh animation, for cases where only vertex positions need to be animated.</p> <p>Tips</p> <ul> <li>Note that the topology of the animated mesh is not stored in the .pc2 file and needs to be defined by creating a  mesh in Blender first. After that, apply the Mesh Cache modifier and set the .pc2 file to use.</li> </ul> <ul> <li>You can update the .pc2 file without having to re-apply the modifier. Blender will re-read the file when the frame number changes.</li> </ul> <ul> <li>See <code>data/advanced/animation/gen_pc2_anim.py</code> for a simple example of generating and using a .pc2 file.</li> </ul>"},{"location":"advanced/animation/everything/","title":"Animating everything","text":"<p>Here, we'll show how generic and powerful the Blender animation system is.</p>"},{"location":"advanced/animation/exercise_manual_camera_orbit/","title":"\ud83d\udcbb Orbiting an object manually","text":"<p>Info</p> <p>The steps in this exercise were partly shown in the presentation as well, but that was mostly to illustrate keyframe animation. Here, you can redo those steps in detail and experiment with them.</p> <p>To orbit an object the camera needs a circular path around the object's location.</p> <ol> <li>Load <code>orbit.blend</code></li> </ol> <p>The scene contains a single monkey (centered at the origin) and a camera. Note that the animation has a length of 100 frames, starting at frame 0.</p> <p>As a first way of doing an orbit we're going to add keyframes for the camera position, as it rotates around the monkey, using the 3D cursor pivot mode.</p> <ol> <li>Set the Pivot Point mode to 3D cursor (bring up the Pivot Point pie menu    with period <code>.</code>, select <code>3D Cursor</code>).</li> <li>Make sure the 3D cursor is located in the origin by resetting its position with <code>Shift-C</code>.    This will also change the view to fit the scene extents.    In general, you can check the current position of the 3D cursor in the sidebar    (<code>N</code> to toggle) on the <code>View</code> tab under <code>3D Cursor</code></li> <li>Select the camera. Verify that as you rotate it around the Z axis the camera indeed    orbits the 3D cursor, and therefore also orbits around the monkey head.</li> <li>Add 4 keyframes at intervals of 25 frames and 90 degrees rotation around Z to complete a 360 degree rotation of the camera    around the object over the full animation of 100 frames</li> <li>Play the animation with <code>Spacebar</code>. Is the camera orbit usable? Why not? Also check the camera view during    playback.</li> <li>Check the graphs in the Graph Editor. See if you can improve the camera orbit, either    by changing the graphs, inserting more keyframes, or both. One way to influence    the shape of the curves is to edit the handles attached to each control point, or to change     the keyframe interpolation for a control point with <code>T</code>.</li> </ol> <p>Tip</p> <p>If you have only a single object in front of the camera around which you want to orbit, an alternative approach is to simply rotate the object itself while keeping the camera in a fixed position. However, this might not always be feasible or preferable.</p>"},{"location":"advanced/animation/exercise_parented_camera_orbit/","title":"\ud83d\udcbb Camera orbiting using parenting","text":"<p>We will try another way of doing a camera orbit. This method involves parenting the camera to an empty. Parenting is creating a hierarchical relation between two objects. An empty is  a special 3D object with no geometry, but which can be placed and oriented in the scene as usual.  It is shown as a 3D cross-hairs in the 3D view. It is often used when doing parenting.</p> <ol> <li>Load <code>orbit.blend</code>.</li> <li>If you happened to have saved the file in the previous assignment with some    keyframes set on the camera you can delete these by selecting the Camera. Then go    into the Timeline editor at the bottom and select all keyframes (diamond markers)    with <code>A</code>, press <code>X</code>, choose <code>Delete Keyframes</code>.</li> <li>Reset the 3D cursor to the origin with <code>Shift-C</code></li> <li>Add an Empty to the scene: <code>Shift-A &gt; Empty &gt; Arrows</code></li> <li>Select only the camera, then add the Empty to the selection by clicking <code>Shift-LMB</code> with the cursor over the empty (or using <code>Ctrl-LMB</code> in the outliner). The camera should now have a dark orange selection outline, while the empty should have a light orange outline, as the latter is the active object.</li> <li>Press <code>Ctrl-P</code> and pick <code>Object</code> to add a parent-child relationship</li> </ol> <p>A black dotted line from the camera to the empty should now be visible in the scene. This means the camera is now parented to the empty. Any transformation you apply to the empty will get applied to the camera as well.</p> <p>Bad Parenting</p> <p>If you made a mistake in the parenting of step 6 then you can clear  an object's parent by selecting that object, pressing <code>Alt-P</code> and picking <code>Clear Parent</code>.</p> <ol> <li> <p>Verify in the outliner that the Camera object is now indeed a child of the Empty (you might have to use the little  white triangles to open the necessary tree entries)</p> </li> <li> <p>Make the empty the single selected object. Enter Z rotation mode by pressing <code>R</code> followed by <code>Z</code>.     Note that as you move the mouse both the empty and camera are transformed. Exit the rotation mode with <code>Esc</code>,     leaving the Z rotation of the empty set to zero.</p> </li> <li> <p>Add key frames at the beginning and end of the animation to have the empty rotate 360 degrees around Z over the animation period</p> </li> <li> <p>Check the camera orbit, including how it looks in the camera view. Is this orbit better?</p> </li> </ol> <p>You might have noticed that, even though we now have a nice circular rotation of the camera around the object, the rotation speed actually isn't constant. If you select the empty and look at the Graph Editor you can see that the graph line representing the Z rotation value isn't straight, but looks like an S. This is due to the default interpolation mode that is used between key frames.</p> <ol> <li> <p>To make the rotation speed constant make sure the empty is selected. Then in the Graph Editor     select all curve points with <code>A</code> and press <code>V</code> to set the handle type, pick <code>Vector</code>.     The curves should now have become straight lines. Check the animation to see the rotation speed     has become constant.</p> </li> <li> <p>Depending on how exactly you set up the animation you might notice a hickup at the moment that the     animation wraps around from frame 99 to frame 0. This happens in case you set the same visible rotation     of the empty for frame 0 and 99 (e.g. 0 degrees for frame 0 and 360 degrees     for frame 99). You can fix this by changing the animation length to 99 frames by setting     <code>End</code> to 98 in the Output properties  panel (the value is directly below <code>Frame Start</code>).     Now, the animation should wrap around smoothly.</p> </li> </ol>"},{"location":"advanced/animation/exercise_track_to/","title":"\ud83d\udcbb Track To constraint","text":"<ol> <li>Load <code>track_to.blend</code></li> </ol> <p>This scene contains two moving cubes and a single camera.</p> <p>We would like to keep the camera pointed at one of the cubes as it moves across the scene. We could animate the camera orientation ourselves, but there is an easier way using a constraint. A constraint operates on an object and can influence things like orientation or scale amount based on another object's properties.</p> <p>We will be using a Track To constraint here, which keeps one object pointing at another object.</p> <ol> <li>Select the camera</li> <li>Switch the Properties panel to the Object Constraints tab using the  icon</li> <li>In the <code>Add Object Constraint</code> menu pick <code>Track To</code> under <code>Tracking</code></li> </ol> <p>The Track To constraint will keep the object, in this case our camera, oriented at another object all the time. The other object is called the Target object (in this case one of the cubes).</p> <ol> <li>In the constraint settings under <code>Target</code> (the top one!) pick <code>Cube</code></li> </ol> <p>If you had the 3D View set to view through the active camera (the view will be named <code>Camera Perspective</code>) one of the cubes should now be nicely centered in the view. </p> <ol> <li>Check that when playing the animation the cube indeed stays centered in the camera view.</li> <li>Orient the 3D view so you can see the camera's orientation in relation to the scene, specifically the targeted cube.</li> </ol> <p>There is a blue dotted line indicating the constraint between the camera and the cube. To understand how the Track To constraint works in this case we need to understand the basic orientation of a Blender camera.</p> <ol> <li>Add a new Camera (<code>Shift-A &gt; Camera</code>)</li> <li>Select it and clear its rotation with <code>Alt-R</code>.</li> <li>Zoom in on the new camera so you can see along which axis it is looking. Also note which axis is the Up direction of the camera (i.e. pointing towards the top of the view as seen by this camera).</li> <li>Select the original camera we wanted to animate and which has the Track To constraint. </li> <li>Change the 3D view so you can see the whole scene, including the selected camera.     Change the <code>Track Axis</code> value of the Track To constraint to different values. Also experiment with different values for the <code>Up</code> setting. Compare these settings against what you concluded from step 10.</li> </ol>"},{"location":"advanced/animation/introduction/","title":"Introduction","text":"<p>Animation is a very broad topic and we will only cover a very small part of what is possible in Blender. We'll begin with an introduction into animation and then  focus on basic keyframe animation.  </p>"},{"location":"advanced/animation/introduction/#summary-of-basic-ui-interaction-and-shortcut-keys","title":"Summary of basic UI interaction and shortcut keys","text":""},{"location":"advanced/animation/introduction/#all-3d-view-timeline-graph-editor","title":"All (3D View, Timeline, Graph Editor)","text":"<ul> <li><code>Shift-Left</code> for moving time to the first frame in the animation, <code>Shift-Right</code> for the last frame </li> <li><code>Left</code> key for 1 frame back, <code>Right</code> for 1 forward </li> <li><code>Up</code> key for 1 keyframe forward, <code>Down</code> for 1 back </li> <li><code>Spacebar</code> for toggling animation playback</li> </ul>"},{"location":"advanced/animation/introduction/#3d-view","title":"3D view","text":"<ul> <li><code>I</code> in the 3D view for inserting/updating a keyframe for the current frame (pick the type)</li> <li><code>Alt-I</code> in the 3D view for deleting the keyframe data for the current frame</li> </ul>"},{"location":"advanced/animation/introduction/#timeline","title":"Timeline","text":"<ul> <li>Changing current frame (either click or drag):<ul> <li><code>LMB</code> on the row of frame numbers at the top</li> <li>OR <code>Shift-RMB</code> within the full area</li> </ul> </li> <li>Change zoom with mouse <code>Wheel</code>, zoom extent with <code>Home</code></li> <li><code>LMB</code> click or <code>LMB + drag</code> for selecting keyframes (the yellow diamonds)</li> <li>The usual shortcuts for editing keyframes, e.g. <code>A</code> for selecting all keyframes, <code>X</code> for deleting all selected keyframes, <code>G</code> for grabbing and moving, etc</li> </ul>"},{"location":"advanced/animation/introduction/#graph-editor","title":"Graph editor","text":"<ul> <li>Change current frame with <code>Shift-RMB</code> </li> <li>Change zoom with <code>Ctrl-MMB</code> drag, or mouse <code>Wheel</code></li> <li>Translate with <code>Shift-MMB</code> (same as in 3D view)</li> <li>Zoom graph extent with <code>Home</code> (same as in 3D view)</li> <li>The usual shortcuts for editing curve control points, e.g. <code>A</code> for selecting all, <code>X</code> for deleting all selected points, <code>G</code> for grabbing and moving, etc</li> </ul> <p>Tip</p> <p>If one or more curves in the graph editor don't seem to be editable (and they show as dotted lines) then you might have accidentally disabled editing. To fix: with the mouse over the graph editor select all curves with <code>A</code> and press <code>TAB</code> to toggle editability.</p>"},{"location":"advanced/animation/introduction/#further-reading","title":"Further reading","text":"<ul> <li>This section in the Blender manual contains many more details on keyframing, particularly with respect to the curves in the Graph Editor.</li> <li>The proper definitions of the colors of keyframed values is described here </li> </ul>"},{"location":"advanced/animation/shape_keys/","title":"Shape keys","text":""},{"location":"advanced/animation/shape_keys/#overview","title":"Overview","text":"<p>The previous chapter dealt with flipbook animation, which is a general method for animating meshes over time. It does take a bit of work to set up, and in some cases you might just have a single mesh with fixed topology of which only the vertex positions change over time.</p> <p>For this case Shape keys can be used, either to morph one mesh into another over time, or to blend multiple meshes together into one (static) result. This can be used, for example, to show the time-evolution of an object, or the highlight differences between two meshes. Although this is a fairly specific use case, shape keys aren't too difficult too understand and use, hence we describe them in this section.</p> <p>There are some limitations, though, to using shape keys:</p> <ul> <li>The two meshes must have the same number of vertices</li> <li>The two meshes should have the same topology (i.e. the way in which the vertices are connected to form polygons). If the topology doesn't   match then strange results can occur during morphing between one set of vertices and another.</li> </ul> <p>The above are fairly annoying limitations, but there is currently no easy way around it in Blender.</p>"},{"location":"advanced/animation/shape_keys/#poor-bunny","title":"\ud83d\udcbb Poor Bunny","text":"<ol> <li>Load <code>bunny_shape_keys.blend</code></li> <li>This scene contains the Stanford Bunny and a completely flattened version of the Bunny</li> <li>Verify that these meshes have the same number of vertices. Also do a visual comparison in wireframe mode (<code>Z &gt; Wireframe</code>)</li> </ol> <p>We'll now add some shape keys:</p> <ol> <li>Select the healthy (non-flattened) Bunny. </li> <li>Under <code>Shape Keys</code> in the Mesh properties, add a shape key using the <code>+</code> button. The new shape keys will be called <code>Basis</code>.</li> <li>Add a second shape key. This will be called <code>Key 1</code> and have a default influence <code>Value</code> of <code>0.000</code>.</li> <li>Select the <code>Key 1</code> shape key and enter mesh edit mode in the 3D view with <code>TAB</code> and make sure you're in vertex mode by pressing <code>1</code></li> <li>Select parts of the Bunny mesh and transform them as you like. The changes should be clearly visible.</li> <li>Exit mesh edit mode with <code>TAB</code>. You should notice that the mesh returns to its normal shape.</li> <li>Change the influence <code>Value</code> of <code>Key 1</code> to see what happens to the resulting mesh. You can either click on it and enter a number, or click and drag the value.</li> </ol> <p>Let's add another shape key:</p> <ol> <li>Add a third shape key, it will be called <code>Key 2</code>. </li> <li>Select <code>Key 2</code> and apply a second set of mesh changes in edit mode. Note that when you enter edit mode the mesh changes to the Basis data, and not the Key 1 data.</li> <li>Once again exit edit mode.</li> <li>Play around with the influence values of both <code>Key ?</code> shape keys, as well as the checkboxes next to the influence values.</li> </ol> <p>Let's check the difference between relative and absolute shape keys:</p> <ol> <li>Uncheck the <code>Relative</code> checkbox to switch to absolute shape keys. Notice that the influence values have now disappeared.</li> <li>Change the <code>Evaluation Time</code> value to understand how the morphing of the meshes is done now (notice the values right of the shape keys names in the list).</li> </ol> <p>We can use a second mesh to define a shape key:</p> <ol> <li>Delete shape keys <code>Key 1</code> and <code>Key 2</code> using the <code>-</code> button and change back to relative shape keys by checking the <code>Relative</code> checkbox.</li> <li>Select the flattened mesh and the <code>Shift-click</code> the Bunny mesh to add it to the selection and make it the active object.  </li> <li>Open the shape key menu using the downward arrow  below the <code>+</code> and <code>-</code> buttons. Select <code>Join as Shapes</code>. </li> <li>There should now be a new shape key called <code>flattened mesh</code>. Note that this shape key is only set on the <code>bunny</code> mesh, not on the <code>flattened mesh</code> mesh. You can verify this by selecting either mesh and checking which shape keys they have.</li> <li>Vary the influence of the shape key called <code>flattened mesh</code> to see the Bunny melt.</li> <li>Delete the <code>flattened mesh</code> object in the Outliner. Does the shape key that morphs the Bunny to its melted flat shape still work?</li> </ol> <p>Look closer at the behaviour of the mesh morphing:</p> <ol> <li>Try to reason why the head of the Bunny is the last part to melt.</li> <li>Zoom in a bit to see if you can spot the twisting motion that mesh makes as it melts. </li> <li>Switch to edit mode to try to transform the mesh in the melted shape key in such as way as to minimize the twist. Or toy around with other mesh transforms to see what morphs come out.</li> </ol>"},{"location":"advanced/animation/tradeoffs_settings_output/","title":"Trade-offs, settings and output","text":"<p>Here, we look into trade-offs that you can make in terms of chosen frame rate,  animation length, quality, etc. </p> <p>Secondly, we will look in detail into the different settings available  for an animation, including the type of output (images or video file) and strategies to handle long render times. We also describe how to do command-line rendering.</p>"},{"location":"advanced/animation/tradeoffs_settings_output/#easy-command-line-rendering","title":"Easy command-line rendering","text":"<p>If you have set up the animation and its settings (i.e. frame rate, start &amp; end frame, output name, etc) as you like in the Blender file then rendering from the command-line usually doesn't involve much more than running this command:</p> <pre><code>blender -b file.blend -a\n</code></pre> <p>The <code>-b</code> option makes sure Blender renders in the background without opening a window. The <code>-a</code> starts animation rendering, as if you had pressed <code>Ctrl+F12</code> or <code>Render &gt; Render Animation</code> in the UI. You only need to add extra options if you want to override values set in the Blender file.</p> <p>Also see this chapter in the basics course.</p>"},{"location":"advanced/blogs/vertex_colors_ppdisk/","title":"Using Python and custom data to (vertex) color your Blender model","text":"<p>Using Blender 3.4.0 and the Python API for vertex colors and scientific visualisations</p> <p>In Blender vertex colors allow you to give a color to the vertices in your mesh. Often vertex colors are used to paint color on your models. But I wanted to use scientific data to color the vertices of a mesh. This can be done using the Python API*. As an example, see the video below of a proto-planetary disk (see the wikipedia page here). It is colored using vertex colors and the colors are based on calculated temperatures of the rocky material in the disk. In this tutorial I will show you the basics of how to do this.</p> <p> Video: Video of a scientific visualisation of a proto-planetary disk. Click here to watch. </p> <p>The most important thing to know when working with vertex colors is that a vertex has a color for every polygon it is part of. We do not set one color for every vertex, but we set a color for every vertex in every polygon it is part of (see Fig. 1).</p> <p> Fig. 1: two connected triangles with vertex colors (in Blender edit mode). As can be seen here, a vertex can have different colors for different polygons (faces) it is in. </p> <p>We will work towards making Fig. 1, so let\u2019s start by making the mesh and geometry that makes up the two triangles:</p> <pre><code>import bpy\n\n# Create a new mesh\nob_name = \"triangles\"\nmesh = bpy.data.meshes.new(ob_name + \"_mesh\")\n\n# Create a new object with the mesh\nob = bpy.data.objects.new(ob_name, mesh)\n\n# Define the geometry\nverts = [ \n        (0,0,0), (0,2,0), (0,1,2) ,\n        (0,3,2)\n        ]\nedges = [ \n        (0,1), (1,2), (2,0),  \n        (1,3), (3, 2)\n        ] \nfaces = [ (0,1,2), (1,3,2) ]\n\n# Add the geometry to the mesh\nmesh.from_pydata(verts, edges, faces)\n\n# Link the object to the Scene collection\nbpy.context.scene.collection.objects.link(ob) \n</code></pre> <p>To add vertex colors we need to make a vertex color layer that is linked to the mesh. Here I show you how to do this in Python:</p> <pre><code># This is to reference the vertex color layer later\nvertex_colors_name = \"vert_colors\"\n\n# Here the color layer is made on the mesh \nmesh.vertex_colors.new(name=vertex_colors_name)\n\n# We define a variable that is used to easily reference \n# the color layer in code later\ncolor_layer = mesh.vertex_colors[vertex_colors_name]\n</code></pre> <p>Now if you would inspect the length of the color layer, len(color_layer.data), you would find that it has six entries while we only have four vertices (see Fig. 1). This is because two of the vertices are in two of the polygons (in this case the triangles). And these vertices can have a different color for every triangle they are in (see again Fig. 1). So, for the vertex colors we need to index the color layer which has an entry for every vertex present in every polygon (these are called loop indices, since they loop over the vertices in all polygons). This in contrast to indicing every unique vertex in a mesh (all entries in mesh.vertices).</p> <p>Now there is a third way to index a vertex we need to know. Every vertex also gets a unique index in every polygon. These indices are used to loop over the vertices in the i\u2019th polygon in this way: mesh.polygons[i].vertices[vert_i_poly].</p> <p>Fig. 2 might help understanding the different indices. Take for example the vertex at coordinates (0,2,0), it can be referenced using the following indices:</p> <ul> <li>It has an index in the mesh of vert_i_mesh = 1 (in green).</li> <li>It has two loop indices in the color layer of vert_i_loop = 1 for the left polygon and vert_i_loop = 3 in the right polygon (numbers in red).</li> <li>The index in the polygon for the (0,2,0) vertex is vert_i_poly = 1 for the left polygon (in blue). And vert_i_poly = 0 in the right polygon.</li> </ul> <p> Fig. 2: Showing the vertex index in the mesh in green (outside of the faces), the loop indices used for the color layer in red (inside the faces) and the polygon indices in blue (innermost inside the faces). The coordinates of the vertices are also shown in black. </p> <p>Lets see how we can link the three indiced to each other:</p> <ul> <li>We can link the vert_i_poly to vert_i_mesh by using the vert_i_mesh = poly.vertices[vert_i_poly].</li> <li>The link between the vert_i_poly and the vert_i_loop can be found using vert_i_loop = poly.loop_indices[vert_i_poly].</li> </ul> <p>So coloring would look something like this:</p> <pre><code># We list a color for every vertex in every polygon in the loop order\nvert_colors = [\n        [1,0,0,1], [0,1,0,1] , [0,0,1,1]  ,\n        [1,0,0,1], [0,0,1,1] , [0,1,0,1]\n        ]\n\n# We loop over all the polygons\nfor poly in mesh.polygons:\n    # We get the polygon index and the corresponding mesh index\n    for vert_i_poly, vert_i_mesh in enumerate(poly.vertices):  \n        # We get the loop index from the polygon index   \n        vert_i_loop = poly.loop_indices[vert_i_poly]\n        # We set the color for the vertex\n        color_layer.data[vert_i_loop].color = vert_colors[vert_i_loop]\n        # A print statement to see how the indices relate to each other \n        print(vert_i_poly, vert_i_mesh, vert_i_loop)\n</code></pre> <p>The print statement gives the following output, showing the link between the poly index (vert_i_poly), the vertex index in the mesh (vert_i_mesh) and the loop index (vert_i_loop):</p> <pre><code>0 0 0\n1 1 1\n2 2 2\n0 1 3\n1 3 4\n2 2 5\n</code></pre> <p>If your run all the code above you will not see anything yet, since you still need to add a material (see Fig. 3). In order to do this, go to the Shading workspace, make a material for the object and add a Color Attribute node (in the inputs section of nodes) and connect this to your shader Base Color input. Now fill in the name of your vertex color layer in the field of the Color Attribute node. You defined this name in the code (here it is vert_colors).</p> <p>(Note: you can also find your vertex color layers in the Properties editor under the Object Data Properties tab and the Color Attributes tab).</p> <p> Fig. 3: Shader and material setup to show the vertex colors on the mesh as seen in Fig. 1. </p> <p>I hope this small tutorial was useful to you. Maybe you think that I made it too complicated with all the indices. But when you want to control the colors of your vertices completely (as I want making a scientific visualisation) then you need to understand them well. I wish you the best of luck coloring your meshes!</p> <p>Published: Feb 7 2023 Ben de Vries, PhD Visualisation Advisor &amp; Astrophysicist SURF.nl, the Dutch ICT cooperation for science and education.</p> <p>* If you are new to the Python API in Blender, we have some material for you to get started with the API. And here you can get even more advanced information on the API.</p>"},{"location":"advanced/final_project/final_project/","title":"\ud83d\udcbb Final project: making a visualization of your own data","text":"<p>We would like you to spend the remainder of your time in this course on doing this little project. We have two options for you to choose from. The first and recommended one is making a visualization of your own (research) data. The second option is that you work on a visualization of data we have prepared.</p> <p>Do not forget that if you are stuck to join us on Discord or in a feedback webinar so we can help. See the Course overview for more information.</p> <p>If you made a nice visualization and still have time left in the course, why not make an animation?</p>"},{"location":"advanced/final_project/final_project/#option-1-your-own-data","title":"Option 1: your own data","text":"<p>So far you have learned how to make meshes and vertex colors in Blender using Python. So, think about if you can visualize your data using these techniques. You need to think about what you need to do to transform your data into a form that can be used to generate vertices, faces and vertex colors. And how do you want to visualize your data values? Can you visualize them through the Cartesian coordinates of the vertices and faces and maybe some colors? Do you need to use vertex coloring? Or do you need something else? Note that volumetric data will be difficult in Blender and you may need to think of some tricks.</p>"},{"location":"advanced/final_project/final_project/#option-2-visualize-a-computer-model-of-a-proto-planetary-disk","title":"Option 2: visualize a computer model of a proto-planetary disk","text":"<p>Although we highly recommend you to work on your own data, if you have none to use, you can use the following data to work on. Here we give a brief introduction to the data.</p>"},{"location":"advanced/final_project/final_project/#what-is-a-proto-planetary-disk","title":"What is a proto-planetary disk","text":"<p>A proto-planetary disk is a disk-like structure around a newly born star. This disk is filled with dust (solid-state particles with a diameter in the order of 1 micrometer) and gas. In the course of time this dust and gas can coalesce into planets. In this option we will look at a computer model of the dust in such a disk. The model calculates the temperature and density of the dust in the disk, taking the radiation and gravity of the star into account. </p> <p>The calculations of the software (called MCMax) are done iteratively using Monte Carlo techniques. Packages of photons are emitted by the star in random directions and their wavelength sampled from the radiation distribution of the star (by default a blackbody). Using the absorption, scattering and emission properties of the dust grains in the disk, the scattering, absorption and re-emission of the photons are calculated throughout the disk. This is used to calculate a temperature structure in the disk. This temperature is then used to adapt the starting density structure of the disk after which a new pass is done by tracking a next set of photons and adapting the density subsequently. This is repeated until convergence is reached. The code uses a two dimensional (adaptable) grid in the radial and theta direction. The disk is assumed to be cylindrically symmetric around the polar axis (z-axis, see Fig. 1). The grid cell size is lowered in regions where the density becomes high.</p> <p> Figure 1: definition of coordinates </p>"},{"location":"advanced/final_project/final_project/#how-to-start-visualizing-such-a-proto-planetary-disk","title":"How to start visualizing such a proto-planetary disk","text":"<p>You could create a 3D model of the disk at constant density and display the temperature as colors on the surface of the model. You could use this to make nice renders and animations to show the temperature structure of the disk. For this we need to pre-process the data from the model to get the spatial coordinates of the disk at a constant density. These coordinates then need to be converted into Cartesian coordinates of vertices and faces before creating the geometry in Blender. You can then add the temperatures to the faces using vertex coloring and by adding the needed shaders to the model.</p>"},{"location":"advanced/final_project/final_project/#how-the-model-data-is-structured","title":"How the model data is structured","text":"<p>You can download the data here. An example output file of modeling code MCMax is shown below.</p> <p><pre><code># Format number\n     5\n# NR, NT, NGRAINS, NGRAINS2\n   100   100     1     1\n# Spherical radius grid [cm] (middle of cell)\n   7479900216981.22     \n   7479900572789.07     \n[...]\n# Theta grid [rad, from pole] (middle of cell)\n  9.233559849414326E-003\n  2.365344804038962E-002\n[...]\n# Density array (for ir=0,nr-1 do for it=0,nt-1 do ...)\n  1.001753516582521E-050\n  1.001753516582521E-050\n[...]\n# Temperature array (for ir=0,nr-1 do for it=0,nt-1 do ...)\n   1933.54960366819     \n   1917.22966277529     \n[...]\n# Composition array (for ir=0,nr-1 do for it=0,nt-1 do ...)\n   1.00000000000000     \n   1.00000000000000     \n[...]\n# Gas density array (for ir=0,nr-1 do for it=0,nt-1 do ...)\n  1.001753516582521E-048\n  1.001753516582521E-048\n[...]\n# Density0 array (for ir=0,nr-1 do for it=0,nt-1 do ...)\n  1.001753516582521E-050\n  1.001753516582521E-050\n[...]\n</code></pre> The file is structured in a way the scientist thought best at the time using the tools at hand. For us it is important to notice the NR and NT, which stands for number of radial and theta points respectively (NGRAINS is related to the number of different types of dust grains in the disk and you can ignore this). Further, the output file then lists the radius points and after that the theta points. Subsequently temperature and density values are listed by iterating over the radius and then the theta indices. The units of all the values in the MCMax output are: R[cm], Theta[radians], Density[gr/cm^3], Temperature[K].</p> <p>The data from the MCMax code is in spherical coordinates, while the system in Blender works with Cartesian coordinates. The theta in the output is defined as the angle with the z-axis (See Fig. 1).</p>"},{"location":"advanced/final_project/final_project/#how-it-could-look","title":"How it could look","text":"<p>To help you get an idea of what the data of the proto-planetary disk might look like, check this video we made: </p>"},{"location":"advanced/geometry_nodes/1_introduction/","title":"Introduction","text":"<p>Geometry Nodes are a way to program your own modifier that modifies your geometry, or generates new. Some predefined modifiers (like for example Subdivision or Decimate) in Blender were introduced in the section Simple Mesh Editing in the Basics part of the course. Modifiers can be added in the Modifiers tab in the Properties Editor (see Fig. 1). But how you will most likely be working with Geometry Nodes is through the Geometry Nodes Workspace, which can be found as a tab in the top part of the interface (see Fig. 1).</p> <p> Figure 1: The node tree contains an input and output Group node when you create a new Geometry Node group. Indicated in yellow outlines are the Geometry Node workspace and the Modifier tab. </p> <p>When you create a new Geometry Node Group you get a Group Input and a Group Output node, see Fig. 1. The geometry on which the geometry node modifier works is inputted through the Group Input node. And in Fig. 1 you see the Group Input node as an output 'Geometry' and this output has a circular, green, output socket. This output socket is connected to the input of the Group Output node. This node gives the output of the modifier and this is shown in the 3D view (passed to the next modifier if more are present on the object). </p> <p>\ud83d\udcbb Exercise 1</p> <p>See what happends when you disconnect the two Group nodes in Fig. 1. Figure out how to reconnect the two Group nodes again.</p>"},{"location":"advanced/geometry_nodes/1_introduction/#nodes-connections","title":"Nodes &amp; Connections","text":"<p>We will start with a simple example to see how Geometry Nodes work. In Fig. 2 we use a standard circle (in the 3d view type <code>Shift-A</code> and choose a circle from the Mesh menu) as the basis geometry on which we will create a Geometry Node modifier. The geometry on which the Geometry Node modifier is working comes with in through the Group Input node. We then create a point on every vertex using a Mesh to Points node. And then on every point we create a cube using the Instance on Points node which we tell to use a cube at every point by feeding it the Cube node. In the 3d view you can now see a whole bunch of cubes arranged on a circle. </p> <p> Figure 2: A node tree to create cube instances on every vertex of an object.  </p>"},{"location":"advanced/geometry_nodes/1_introduction/#sockets-fields","title":"Sockets &amp; Fields","text":"<p>Lets do something more with the example in Fig. 2. Lets make it so that the cubes we make scale with their position on the x-axis. Fig. 3 shows how you can do this. We use the blue diamand shaped input socket named 'Scale' of the Instances on Points node. We connect this to a couple of nodes that retrieve the x-position of the cubes. We start with a Position node. This outputs a position vector which we split in x, y and z components using the Separate XYZ node. We take the x-component and scale it using a Math node set to multiply. We then combine the scaled x-component with a constant y and z component using the Combine XYZ node. This way we scale the the cubes based on their position on the x-axis.</p> <p> Figure 3: In this node tree we scale the created cubes based on their x-coordinate. For this we use nodes connected through field lines (dashed connections), which work like functions. </p> <p>You can see that all the nodes building up to the 'Scale' socket, have dashed lines connecting diamond shaped sockets. This in contrast to the green solid lines with round sockets that are used for the flow of the geometry. The dashed lines are called fields and they are calculated differently from the solid lines. Field lines start with a node that generates data, in our example of Fig. 3 this is the Position node. These nodes need to know where to get the data from. What happens is that field lines are evaluated when they are connected to a node that has some geometry (solid line) fed into it. In our case this happens when the field is connected to the Instance on Points node. Now the Position node outputs the position of the points/vertices corresponding to the geometry that is connected to the Points socket in the Instance on Points. If you do not get this right away, do not worry. It is quite confusing at times, but it does allow for creating kind of functions as in programming languages. </p> <p> Figure 4: In this node tree we show that fields can be evaluated on different geometries. For example when you connect the Position node to different geometries, it gives different values depending on the geometries. </p> <p>To give an example of how fields work, lets connect some of the same nodes connected by field lines to another geometry. In Fig. 4 you can see I added a UV Sphere node that connects to a Delete Geometry node. Thus we create a sphere and now we want to delete part of the sphere using the 'Selection' socket of the Delete Geometry node. We will use the same Position node and Separate XYZ node as for the circle geometry, but now the position node will give the position of the vertices in the UV Sphere when I connect it to the Delete Geometry node. Here you see that you can connect the same Position node to different geometries and it will give different values. So you could also say that fields in Geometry Nodes are like functions that can be called by different nodes with different geometries.</p> <p>\ud83d\udcbb Exercise 2</p> <p>Starting with a UV Sphere. Make an egg shaped object by elongating the UV sphere along, for example the x-axis. Tip: you could use a Set Position node and some of the nodes from Fig. 3 to do this.</p>"},{"location":"advanced/geometry_nodes/1_introduction/#viewer-nodes","title":"Viewer nodes","text":"<p>Before we start building more geometry nodes, let me quickly tell you about the Viewer node. This node is very useful for debugging and inspecting parts of your node tree. In Fig. 5 you can see I connected a viewer node directly to the UV Sphere node. When the little eye is visible in the top right of the viewer node, you can see its input in the 3D view. For example you now see the UV Sphere before part of its geometry is deleted. I also connected the boolean output of the Less Then Math node to the value socket of the viewer node. Doing this lets you inspect this value in the Spreadsheet editor (see top left of Fig. 5). Also it visualizes the value on the geometry in the 3D view, now you can see that the top half of the UV Sphere is black (representing false) and the bottom is white (representing true). </p> <p> Figure 5: An example of the Viewer node. It is very useful for inspecting and debugging your node trees. </p>"},{"location":"advanced/geometry_nodes/2_rotation/","title":"Rotate that planet","text":"<p>In this section we will start with rotating a planet while in the next we will implement some physics to simulate the motion of a planet. Within Geometry Nodes we will use a Simulation Zone to quickly iterate over frames and easily make animations. For example see the animated gif in Fig. 1.</p> <p> Figure 1: rotating spheres using Geometry Nodes and Simulation Zones. </p> <p>The rotation you see in Fig. 1 is generated using the Transform Geometry Node (see Fig. 2). The Transform Geometry Node nicely combines the options to translate, rotate and scale the geometry. In Fig. 2 you also see we placed the Transform Geometry Node inside a Simulation Zone. This way you can generate an animation really easily using Geometry Nodes. </p> <p> Figure 2: ... </p> <p>Lets break down the Simulation Zone for a moment. As you can see in Fig. 2 the Simulation Zone consists out of two nodes, one input and one output, and an marked area in between them. How it works is that the geometry from the previous frame is entered at the left input Simulation node and you can do stuff with it in the marked zone. Then you pass the geometry into the right Simulation node and in the next frame you can continue with the geometry. And this way it iterates over frames and executes the nodes in the Simulation Zone every frame. When the animation starts the simulation zone starts with the input given to the input node. In Fig. 2 we take the geometry coming from the Group Input and give it to the Simulation Zone to start with at the first frame. The output sockets of the output node of the Simulation Zone gives you the geometry at every frame In our example in Fig. 2 this is connected to the Group Output, so we see the geometry and how it changes at every frame. </p> <p>Applying changes</p> <p>When using simulations zones, when you make changes to the node tree, you often need to slide the animation head back to frame 1 in order for the changes to take effect.</p> <p>When you have implemented this Geometry Node setup like in Fig. 2, you can go to the 3D view, set the animation timeline to frame zero and press play (or press the space bar on your keyboard). Blender now calculates the geometry at every frame using the Geometry Nodes and you will hopefully see a nice animation!</p> <p>\ud83d\udcbb Exercise: Rotation</p> <p>a) Implement the Geometry Node setup as seen in Fig. 2. You can use any type of object, a sphere with a texture or cube, as long as you can see its rotation. </p> <p>b) Now besides a rotation, also give the object a translation. You can do this with the same Transform Geometry node as is used for the rotation. Describe the motion of the object when it is both affected by a rotation and a translation. Would you conclude that the Transform Geometry node works on the global or local coordinates of the object?</p>"},{"location":"advanced/geometry_nodes/2_rotation/#links","title":"Links","text":"<ul> <li>For the textures of the planets I have used the beautiful textures from the Solar System Scope project which are based on NASA images.</li> </ul>"},{"location":"advanced/geometry_nodes/3_translation/","title":"Making a planet move","text":"<p>Now we will give our planet some movement around the star. For this we will implement, loosely, Newton's laws of gravitation. Fig. 1 shows what it looks like. First we will start to move the planet.</p> <p> Figure 1: This is not only an animation but also a simulation. At every frame the motion of the planet around a star is calculated using Newtons laws of motion and gravitation. </p>"},{"location":"advanced/geometry_nodes/3_translation/#setting-up-the-simulation-zone","title":"Setting up the Simulation Zone","text":"<p>I show the main loop of the Simulation Zone and the Geometry Nodes in Fig. 2 and the outliner with the objects in Fig. 3. The first thing to notice is that in the Geometry Nodes I do not use the geometry of the object since the Group Input is not connected to anything. Instead I create one point at the origin using the Points node. This point I use as input for the Simulation zone input node. And if we skip to the output of the simulation zone and ignore its body for a second, we see that the Simulation Zone output's geometry is connected to an Instance on Points node. This node puts an instance on every point in the geometry. The object we use as an instance to put on our point is the object called 'PlanetTemplate'. In Fig. 3 you can see that this is an object in the scene. We get this object into the geometry nodes using the Object Info node and selecting the object 'PlanetTemplate' in the object box.</p> <p> Figure 2: Simulation Zone setup and the core part of the Node tree for the animation in Fig. 1. </p> <p>There are a few reasons to set up the geometry nodes like this. We have another geometry node setup on the PlanetTemplate object to give it a rotation. By setting it up like this, translation and rotations are separated from each other and you get a parent-child kind of setup. Another reason is that it is often easier to manipulate one point in space instead of a whole bunch of points (vertices), edges and faces. There are also benefits in doing it this way regarding calculation efficiency, but I will not go into this in detail now.</p> <p> Figure 3: Scene Outliner </p>"},{"location":"advanced/geometry_nodes/3_translation/#working-out-the-velocity-of-the-planet","title":"Working out the velocity of the planet","text":"<p>Ok, now we know we only have to work with one point in the simulation zone body and we need to move it around according to the physical laws of gravitation and movement. First we look at how we keep track of the velocity of the planet and then we will talk about how we add acceleration (force) to it in Fig. 4. The Simulation Zone input and output nodes always have an unused and unnamed socket which you can connect to something. Doing this will make a new variable that is passed from frame to frame in the Simulation Zone. In this case I have done this and called it velocity. You can name the socket by bringing up the Interface panel by pressing <code>N</code>. We want to do two things to this velocity. The first is we want to use it to move the planet one step forward at every frame. And we want to update the velocity at every frame using the acceleration. </p> <p>We connect the velocity socket of the Simulation Zone input to a Vector Math node set to scale. With this scale node we want to calculate a change in position of the planet so we multiply the velocity with the time step of the frame. You get this time step from the Delta Time socket in the Simulation Zone input node. The Vector Math node then scales the velocity vector with the delta time to give the step in space. This step in space is then connected to the Offset socket of a Set Position node. This node moves the geometry in space by adding the offset to it. We then connect the geometry socket of the Set Position node to the output node of the Simulation Zone to pass the geometry to the next frame.</p> <p>Now at every frame we also want to update the velocity using the acceleration of the planet. The acceleration of the planet comes from connection outside Fig. 2 and we will talk about this in the next section and the nodes are shown in Fig. 4. We will now take this acceleration as a given and we will calculate the change in velocity per frame. For this we multiply the acceleration with the Delta time to get the change in velocity. We do this by scaling the acceleration with the delta time in a Vector Math node set to Scale. Then we add the change in velocity to the velocity of the previous frame. This comes from the Simulation Zone input. We add those also using a Vector Math node but set to Add. Now the new value of the velocity is connected to the velocity socket of the Simulation Zone output node in order to pass it to the next frame.</p>"},{"location":"advanced/geometry_nodes/3_translation/#determining-the-acceleration-of-the-planet","title":"Determining the acceleration of the planet.","text":"<p>Now we want to calculate the acceleration and let it change the velocity of the planet. The laws of Newton teach us that the acceleration (\\(\\vec{a}\\)) of a body times its mass \\(m\\) is equal to the sum of all the forces acting on the body. </p> \\[\\vec{F} = m \\vec{a}\\] <p>Here the arrow on the force and acceleration symbols indicates that it is a vector. A vector has both a direction and a length, thus the equation relates both the strength as the direction of the force and acceleration. The gravitational force depends on the two masses in question, on the gravitational constant \\(G\\) and inversely on the square of the distance between the two bodies:</p> \\[\\vec{F} = -\\frac{G M m}{r^2} \\hat{r}\\] <p>Note here that \\(r\\) is not a vector, but only the distance between the planet and the star. The direction of the force is given by \\(\\hat{r}\\). This is a vector with length 1 and points in the radial direction, thus from the star to the planet. Now because there is a minus sign in the equation the gravitational force is directed towards the central star. In other words, the planet is pulled to the star. The strength of the force depends on the distance between the planet and the star as \\(1/r^2\\). This means the force is strongest at shorter distances and becomes weaker when the two bodies move apart. </p> \\[\\vec{a}= -\\frac{G M}{r^2} \\hat{r}\\] <p>Fun</p> <p>For the physics interested peoples! Here we see that the acceleration of a body in a gravitational field does not depend on the mass of the body (only on the mass of the other body). Thus a bowling ball and a feather fall equally fast! But why do we almost always observe a difference on Earth? Do you want proof of this with less equations and more dramatic music? Check out this BCC clip with Brian Cox!</p> <p>Now, back to business. To make things a bit easier for ourselves we will skip all the exact values and units of the constants, we will simply take \\(G\\) and \\(M\\) into one constant \\(C\\) and we will tweak this one to get some different movements. So we need to program the following equation in Geometry Nodes:</p> \\[\\vec{a} = \\frac{C}{r^2} (-\\hat{r})\\] <p>Here I have separated the direction and strength of the force. The direction is \\(-\\hat{r}\\), while the size of it is \\(C/r^2\\).</p> <p> Figure 4: part of the Node tree to calculate the acceleration of the planet. </p> <p>It is time to implement our formula into Geometry Nodes. In order to calculate the separation (distance) vector \\(\\vec{r}\\) between the star and planet, we need the location of them. We get the location of the planet with a Position node. This node does not need any inputs, it collects the position of the geometry to which it is eventually connected in the Node tree. The position of the star we get by again using an Object Info node. In this node we select the star object and we set it to relative. We now get the distance vector \\(\\vec{r}\\) by subtracting the two position vectors of the objects using a Vector Math node set to Subtract. We now get the direction of \\(\\vec{r}\\), called \\(\\hat{r}\\) in our equation, by normalizing the vector \\(\\vec{r}\\). We get the size \\(r\\) of \\(\\vec{r}\\) by using the Vector Math node set to Length. We then square \\(r\\) and calculate \\(1/r^2\\) and we multiply it by a constant. Then we create one acceleration vector again by multiplying \\(C/r^2\\) with the direction of \\(\\hat{r}\\). The resulting acceleration is using in the Simulation Node to update the velocity at every frame.</p> <p>\ud83d\udcbb Exercise: Translation</p> <p>a) Implement the Geometry Nodes of this section in Blender yourself.</p> <p>b) In the Input node of the Simulation Node we have given the planet an initial velocity of (3,0,0). See Fig. 2. Try some different velocities and see what kind of orbits you get. Are they all stable (e.g. does the planet keep orbiting the star)?</p> <p>c) If you like a challenge. We know that all gravitational bodies pull on each other, but in this section we have only implemented the pull of the star on the planet. Often this is a nice approximation since the star is much heavier than the planet. But think about how you could incorporate the pull of the planet on the star also.</p>"},{"location":"advanced/geometry_nodes/3_translation/#links","title":"Links","text":"<ul> <li>For the textures of the planets I have used the beautiful textures from the Solar System Scope project which are based on NASA images.</li> </ul>"},{"location":"advanced/mesh_editing/introduction/","title":"Introduction","text":"<p>Info</p> <p>This chapter is an extension of the Basics course Simple mesh editing chapter, so the walkthrough of that chapter should suffice as background. </p> <p>This chapter will give you an introduction on the Edit mode of the 3D viewport where you will learn how to patch up your imported meshes/visualizations and even learn how to generate your own 3D shapes.</p> <p>To refresh your memory on basic mesh editing you can watch the Simple mesh editing intro video of the Basics part below:</p>"},{"location":"advanced/mesh_editing/mesh_editing_assignment/","title":"\ud83d\udcbb Mesh Editing with the Edit mode","text":"<p>This assignment will be a brief introduction on the Edit mode in the 3D viewport.</p> <p>Once you opened the exercise blend file <code>mesh_editing_assignment.blend</code> you'll see the familiar fish iso-surface above a plane.</p>"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#getting-familiar-with-the-edit-mode","title":"Getting familiar with the Edit mode","text":"<p>To edit the mesh we first need to go the Edit mode with the fish.</p> <ol> <li>Select the fish and enter the Edit mode by pressing <code>Tab</code>. Depending on the speed of the system you're working on edit mode might be entered instantly or might take half a second. In general, for larger meshes switching to edit may take longer.</li> </ol> <p>Now you will be able to see all the vertices, edges and faces that make up the 3D model. You will now try to select and move around some vertices, edges and/or faces.</p> <ol> <li>Change the Mesh Select Mode to <code>Vertex</code> by pressing <code>1</code> (or click the left icon in  at the 3D view header). This might already be active by default but it will be highlighted on the icons in the 3D view header ().</li> <li>Before you start selecting, de-select all all current selected vertices by pressing <code>Alt-A</code> or double <code>A</code> rapidly.</li> <li>Now try to select a single vertex by clicking on it with the <code>LMB</code>, or multiple with <code>Shift-LMB</code>. You might have to zoom in a bit to separate the vertices enough.</li> <li>Another method is to use the selection tools:<ol> <li>Box selection by pressing <code>B</code> and dragging a box around the vertices you want to select. Hold <code>Shift</code> to de-select.</li> <li>Circle selection by pressing <code>C</code> and left-clicking and dragging with the mouse over the vertices you want to select. To increase the size of the Circle selection tool simply scroll with your mouse <code>Wheel</code>. With <code>MMB</code> and dragging you can de-select vertices. Press <code>Enter</code> to exit circle select mode (or with <code>RMB</code> ).</li> </ol> </li> <li>Once you selected your vertices you can transform them the same way you can do with objects by pressing the hotkeys <code>G</code> for translation, <code>R</code> for rotation, and <code>S</code> for scaling, etc.</li> <li>Probably now you did the vertex editing the fish looks a bit scrabbled. One way to clean it up is, of course, using <code>Ctrl-Z</code> to undo it. Another way is simply deleting the vertices by using the Delete popup menu <code>X &gt; Vertices</code>. Try to remove part of the fish skin to it leaves a hole in the mesh which will reveal a part of the inside of the fish.</li> </ol> <p>Tip!</p> <p>If your fish has been \"meshed-up\" beyond repair you can always <code>revert</code> it to the last saved state with: <code>File &gt; Revert &gt; Confirm</code>. </p>"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#filling-the-holes","title":"Filling the holes","text":"<p>An imported mesh from a 3D visualization program can sometimes contain unwanted holes or separations in parts of the mesh, these can also be fixed in the edit mode. Conveniently the fish in the exercise file was already poked full of holes so you can fix these.</p> <p><code>In between</code>: To better inspect if there are any holes left you can switch back and forth between the Object mode and Edit mode because in the Object mode they are easier to see.</p> <ol> <li>First, make sure the whole mesh is selected by pressing <code>A</code> and then remove the small holes (the size of one triangle/quad) by pressing <code>F3</code> in the 3D viewport in Edit mode and type in <code>fill holes</code> and press <code>enter</code> or click on it with <code>LMB</code> (this might take some time). Now this already cleaned up a lot of the holes in the geometry!</li> <li>Through inspection you might notices there are some bigger wholes that were not filled yet because they were skipped by the previous step since they were to large. To fill these they first need to be selected by first de-selecting everything with <code>Alt-A</code> and then press <code>F3</code> and type in <code>non manifold</code> and press <code>Enter</code> or click on it with <code>LMB</code>.</li> <li>This selected the big holes but also other non-manifold geometry. To select only one of the holes hold <code>Ctrl+Shift</code> and drag with <code>LMB</code> over one of the holes. This de-selects everything excepts what was in the drag-box.</li> <li>Now this selected hole can easily be fixed by pressing <code>F</code>.</li> <li>Repeat step 2 to 4 for the other 2 holes.</li> </ol> <p>Tip!</p> <p>The fill with <code>F</code> fills the hole with an n-gon, a face with more then 4 vertices. These can sometimes create shading artifacts in your final render. Another way to fill these holes is to use grid-fill (<code>Ctrl+F</code>), this tries to fill the whole with a grid of quad shaped faces. This however might not always work for numerous reasons (uneven amount of vertices, closed loops etc) which can be fixed with additional mesh editing but the easy route would be to fill it with an n-gon face.</p>"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#separating-skin-from-bones","title":"Separating skin from bones","text":"<p>Now that you got a little familiar with mesh editing you can try to separate the skin from the bones by using mesh separation.</p> <ol> <li>While still in edit mode (press <code>Tab</code> if not), try to select all the outside skin with the select linked selection by hovering the mouse cursor over the geometry and pressing <code>L</code>. This will only select a connected part of the skin so continue this step until you think you selected all the outside skin. Note that it is difficult to do this perfectly, as some of the insides of the fish are sometimes also selectable. Unfortunately, this occurs frequently with this type of sensor-based 3D data.</li> <li>Once you think all the skin is selected you can press <code>P</code> and select <code>Selection</code> to separate the selected surfaces from the main mesh into another mesh object. This new mesh will be added to the Outliner with the name <code>fish.001</code>.</li> <li>In the Outliner double-click <code>LMB</code> on the mesh object <code>fish.001</code> to rename it to <code>fishskin</code>. Do the same for the <code>fish</code> mesh object and rename it to <code>fishbones</code>.</li> <li>If you now select the <code>fishskin</code> mesh object and hide it by clicking the little  icon in the Outliner will reveal the insides of the fish.</li> </ol> <p>Tips!</p> <ul> <li>To reverse the separation of the mesh into bone and skin you can select both the mesh objects in Object mode and press <code>Ctrl-J</code> to join them back together into a single mesh.</li> <li>Sometimes X-ray mode, toggled with <code>Alt-Z</code>, can be useful when editing a complex mesh, as it makes all geometry in a mesh partly transparent</li> </ul>"},{"location":"advanced/mesh_editing/mesh_editing_assignment/#bonus-make-your-own-annotation-arrow","title":"(BONUS) Make your own annotation arrow","text":"<p>Since the content of this course is mostly geared towards imported geometry or scripted geometry, you might not directly think about manually created geometry. This bonus exercise however will show you that it is relatively easy to create your own geometry in Blender. Lets start your manual mesh creation with an annotation arrow!</p> <ol> <li>In the 3D viewport make sure you are in <code>Object mode</code> and add a new cylinder with <code>Shift-A &gt; Mesh &gt; Cylinder</code>.</li> <li>Press <code>/</code> to isolate the mesh so that there are no distractions. This can be reversed again by pressing <code>/</code>.</li> <li>Press <code>Tab</code> to go into Edit mode.</li> <li>Grab the selected geometry by pressing <code>G</code> and press <code>Z</code> to move it along the z-axis only and press <code>1</code> to move it 1 unit up so that the origin is at the bottom.</li> <li>De-select all the geometry with <code>Alt-A</code> and press <code>1</code> to set the select mode to Vertex and select all the bottom vertices (with <code>LMB-drag</code> over the vertices or with the <code>b</code> Box-select).</li> <li>Press <code>S</code> to scale them to a tiny point and press <code>LMB</code> to confirm.</li> <li>Now select the top vertices the same way you did with the bottom vertices, make sure that none of the bottom vertices are selected.</li> <li>Press <code>I</code> to inset the faces and move your mouse until you are satisfied with the width of the arrow shaft.</li> <li>Press <code>E</code> to extrude the selection and move the mouse up until you are satisfied with the length of the arrow shaft.</li> <li>Now press <code>Tab</code> and admire your newly created arrow! </li> <li>The arrow might now be a bit too big compared to the fish so scale the arrow down with <code>S</code>, move it to a point of interest with <code>G</code> and rotate the arrow to your liking with <code>R</code> (which is made relatively easy because we made it so that the origin is at the point)</li> </ol> <p>Since the introduction of the Edit mode and switching back and forth between it and the Object mode you do need to make sure in which mode you are before adding new geometry or before using one of the transform operations (grab, scale and rotate). Otherwise you might add geometry to an already existing object instead of adding a new 3D object or you might move, scale or rotate 3D object geometry in the Edit mode and inadvertently change the origin of the object. This can be confusing sometimes but you'll get used to it! </p>"},{"location":"advanced/python_scripting/1_api_basics/","title":"Blender API basics","text":""},{"location":"advanced/python_scripting/1_api_basics/#introduction","title":"Introduction","text":"<p>Blender embeds a Python interpreter, which is used for multiple tasks. It is a central feature of Blender, as large parts of the user interface are set up and controlled from Python, as well as all add-ons (import/export, tools, etc) are written in Python.</p> <p>As a user you can run scripts directly on this interpreter and also access Python modules provided by Blender, like <code>bpy</code> and <code>mathutils</code> to access scene elements. The <code>bpy</code> module gives access to Blender's data, functions and classes. In this section we will focus on using the Python API for automation, custom data import and manipulating geometry, but this is not all that is possible with the API, of course. The official API manual states the following things are possible using the Python API:</p> <ul> <li>Edit any data the user interface can (Scenes, Meshes, Particles etc.).</li> <li>Modify user preferences, key-maps and themes.</li> <li>Run tools with own settings.</li> <li>Create user interface elements such as menus, headers and panels.</li> <li>Create new tools.</li> <li>Create interactive tools.</li> <li>Create new rendering engines that integrate with Blender.</li> <li>Subscribe to changes to data and it's properties.</li> <li>Define new settings in existing Blender data.</li> <li>Draw in the 3D view using Python.</li> </ul> <p>All in all, the Python API is very powerful.</p> <p>More detailed Python API reference</p> <p>In these chapters we provide an introduction to the Python API, using a number of examples. After finishing these chapters you can find a more extensive description of often-used Python API features in the separate API section.</p>"},{"location":"advanced/python_scripting/1_api_basics/#good-to-know","title":"Good to know","text":"<p>Before we continue, we list some bits of information and some tricks that are good to know.</p> <ul> <li>Blender uses Python 3.x, specifically 3.11 in Blender 4.1 - 5.0</li> <li>You can access the online API documentation from within Blender with <code>Help &gt; Python API Reference</code></li> <li>Starting Blender from the console will allow you to see important outputs channels (warnings, exceptions, output of <code>print()</code> statements, etc). See the next section how to do this.</li> <li> <p>The Python Console area in Blender is great for testing Python one-liners. It also has auto-completion so you can inspect the API quickly. Example code shown with <code>&gt;&gt;&gt;</code> lines in our course notes is assumed to be running in the Python Console.</p> <p>Python Console versus terminal console</p> <p>The Python Console is something different than the console we refer to below. The Python Console is an area within the Blender user interface in which you can enter and execute Python commands:</p> <p></p> <p>While the other type of \"console\" is a terminal window or DOS box from which you start Blender. This console will then contain any output and exceptions from Python scripts that you run:</p> <p></p> <ul> <li>By enabling the <code>Python Tooltips</code> option in the Preferences under <code>Interface &gt; Display</code> you can hover over almost any button, option, menu, etc and after a second a tool-tip is shown. This tool-tip shows information on how to access this element from the Python API. </li> <li>Right clicking on almost any button, option, menu, etc in Blender gives you the option to 1) directly go to the API documentation with <code>Online Manual</code> or 2) <code>Copy Data Path</code>. Option 2 copies Python API properties related to that element to your clipboard to paste into your script. Note however, that not always the full path is copied, but only the last part.</li> </ul> </li> </ul> <p>In the upcoming sections we will first look at how to run Python scripts in Blender. Then we look at how to access Blenders data through scripts and we follow this up with creating geometry, vertex colors and materials in the last section.</p>"},{"location":"advanced/python_scripting/1_api_basics/#starting-blender-from-the-command-line","title":"Starting Blender from the command line","text":"<p>It is important, when scripting, to start Blender from a command line interface (macOS and Linux). Warnings, messages and <code>print()</code> statements will output into the console. How to start Blender from the command line depends on your operating system.</p> <ul> <li> <p>For macOS it would be like this:</p> <pre><code>/Applications/Blender.app/Contents/MacOS/Blender\n</code></pre> </li> </ul> <ul> <li> <p>For Linux it would be something like:</p> <pre><code>$ &lt;blender installation directory&gt;/blender\n</code></pre> </li> </ul> <ul> <li>On Windows you can start Blender normally (i.e. from the Start menu) and then use <code>Window &gt; Toggle System Console</code> to open the console window from within Blender.</li> </ul> <p>More information on where the Blender executable is located on your system and where Blender directories of interest are located see this manual page.</p>"},{"location":"advanced/python_scripting/1_api_basics/#starting-blender-from-the-console","title":"\ud83d\udcbb Starting Blender from the console","text":"<p>Exercise:</p> <ol> <li>Find the Blender executable on your machine. </li> <li>Open Blender through the console. </li> <li>Delete the cube in the default project of Blender, what output is shown in the console?</li> </ol>"},{"location":"advanced/python_scripting/1_api_basics/#running-scripts-within-the-blender-interface","title":"Running scripts within the Blender interface","text":"<p>When scripting inside Blender it is convenient to use the Scripting workspace (see the arrow in Fig. 1 below). For running scripts within Blender you have two main options:</p> <ul> <li>Using the interactive Python Console (Fig. 1A)</li> <li>Using the built-in Text Editor (Fig. 1B)</li> </ul> <p>The Python Console is very useful for testing lines of Python code, and exploring the API using auto-complete (with <code>TAB</code>) to see what is available. The keyboard shortcuts are a bit different than you might be used to in other text editors. See this section in the Blender manual for an overview of menu options and shortcut keys.</p> <p>Blender also has its own built-in text editor which you can use (Fig. 1B) to edit Python code and execute it by pressing the  button in the top bar, or using <code>Alt-P</code>. Note that you can have multiple different text blocks, each with their own code.</p> <p>If you want to use your own editor to edit your scripts you can do this by opening the script in both the Blender Text Editor and your own editor. To refresh the Blender Text Editor use <code>Text &gt; Reload</code> or <code>Alt R</code> (or <code>Option R</code> on the Mac). You can also make a script that you open in the Blender Text Editor that executes an external script you edit in your own editor. See for example the script in Fig. 1B.</p> <p> Figure 1: The Scripting workspace in Blender </p>"},{"location":"advanced/python_scripting/1_api_basics/#running-scripts-from-the-command-line","title":"Running scripts from the command-line","text":"<p>You can also run Python scripts in Blender directly from the command-line interface. An example of executing a script (<code>-P</code>) without opening the Blender GUI (<code>-b</code>, for background) would be:</p> <pre><code>blender -b -P script.py\n</code></pre> <p>You can combine running a Python script with, say, rendering the first frame (<code>-f 1</code>) from an example test.blend file. The output will go to the directory of the blender file (<code>-o //...</code>) and it will generate a PNG image file (<code>-F PNG</code>):</p> <pre><code>blender -b test.blend -o //render_ -F PNG -f 1\n</code></pre> <p>More information on command line arguments is here in the Blender manual.</p>"},{"location":"advanced/python_scripting/1_api_basics/#custom-script-arguments","title":"Custom script arguments","text":"<p>You might want to pass extra arguments to your script, for example to provide a frame range, or file name, or anything else that makes sense for your script but not Blender. For this, Blender provides the <code>--</code> marker option. Any arguments passed to Blender that follow <code>--</code> will not get processed by Blender, but are passed in <code>sys.argv</code>:</p> <pre><code># useargs.py\nimport sys\n\nargs = []\nidx = sys.argv.index('--')\nif idx != -1:\n    args = sys.argv[idx+1:]\n\nprint(args)\n# Do something with values in args\n</code></pre> <pre><code>$ blender -b -P useargs.py -- -myopt 1,2,3\nBlender 4.1.0 Release Candidate (hash 3641b4b884ab built 2024-03-20 01:31:58)\nRead prefs: \"/home/melis/.config/blender/4.1/config/userpref.blend\"\n['-myopt', '1,2,3']\n\nBlender quit\n</code></pre> <p>You can then use these arguments in your script, even parse them using a regular option parsing Python module like argparse. </p>"},{"location":"advanced/python_scripting/1_api_basics/#using-modules-and-external-scripts","title":"Using modules and external scripts","text":"<p>As we've shown above there's multiple ways to run Python code within Blender, either from a text editor block, the Python Console or from the command-line. Usually, you want to use Python modules or other scripts from the code you're running. Below we describe some common situations and how to handle them.</p> <p>See this manual page for more tips and tricks related to working with Python scripting in Blender.</p> <p>NumPy</p> <p>The official binaries of Blender from blender.org include the <code>numpy</code> Python module, so if you need NumPy then <code>import numpy</code> should work out of the box.</p>"},{"location":"advanced/python_scripting/1_api_basics/#loading-modules-in-blender","title":"Loading modules in Blender","text":"<p>For modules you want to import you can use the normal Python method of editing <code>sys.path</code> (as needed) and importing the module:</p> <pre><code># Example code run from a text block within Blender\nimport sys\n\n# A path somewhere on your file system\nsys.path.append(\"/some_directory/\")\n\n# Or a path relative to the current blender file\nblendfile_location = os.path.dirname(bpy.data.filepath)\nsys.path.append(blendfile_location)\n\n# Import module\nimport my_python_module\n\n# Call a function from the module\nmy_python_module.do_something()\n</code></pre> <p>However, suppose you you keep Blender running and edit <code>my_python_module.py</code> to update <code>do_something()</code>. Re-executing the above code will not pick up the changes  in the module you're importing. The reason for this is that the Python interpreter doesn't reload a module if it is already loaded. So the <code>import my_python_module</code> has no effect the second time it is called.</p> <p>To force a module to get reloaded you can use the <code>importlib</code> module:</p> <pre><code>import my_python_module\n\n# Force reload\nimport importlib\nimportlib.reload(my_python_module)\n\nmy_python_module.do_something()\n</code></pre> <p>Note that this will re-load the module from disk every time you run the above piece of Python code.</p>"},{"location":"advanced/python_scripting/1_api_basics/#executing-external-scripts","title":"Executing external scripts","text":"<p>To execute an arbitrary Python script in an external file you can use the following:</p> <pre><code># Execute script_file \nexec(compile(open(script_file).read(), script_file, 'exec'))\n</code></pre> <p>You could, for example, put this snippet of code in a Blender text block and execute it every time you need to run it (or even paste it in the Python Console). This is a fairly simple way of executing externally stored Python code, while still being able to edit the external script as needed. </p>"},{"location":"advanced/python_scripting/1_api_basics/#using-an-external-editor","title":"Using an external editor","text":"<p>You might want to use your own favorite editor to write your Python scripts. In Blender you can open an existing file in the editor by clicking on the file icon (1 in the image below). If you then edit this file in your own editor outside of Blender, the editor in Blender will indicate that the file has changed with a red question mark (2). It might take one or two seconds for Blender to notice the file has changed.</p> <p></p> <p>Clicking on the question mark gives you three options:</p> <ul> <li><code>Reload from disk</code> - this will overwrite the script in the Text Block with whatever is loaded from file</li> <li><code>Make text internal (separate copy)</code> - this will ignore the changed file on disk from now, the text in the Text Block will be decoupled from it </li> <li><code>Ignore</code> - the script in the Text Block will stay unchanged, any future changes in the file on disk will retrigger the question mark</li> </ul> <p>To load the newest version of your file into Blender, you can use <code>Text &gt; Reload</code> (or <code>Alt-R</code>).</p>"},{"location":"advanced/python_scripting/1_api_basics/#adding-startup-scripts","title":"Adding startup scripts","text":"<p>You might want to permanently run one or more Python scripts when Blender starts. You can add these scripts in a special configuration directory. The location to place these scripts is system-dependent (see this manual page)  for details. In general you want to place the scripts within the \"USER\" location of the platform you're working on:</p> <ul> <li>Windows: <code>%USERPROFILE%\\AppData\\Roaming\\Blender Foundation\\Blender\\4.1\\</code></li> <li>Linux: <code>$HOME/.config/blender/4.1/</code></li> <li>macOS: <code>/Users/$USER/Library/Application Support/Blender/4.1/</code></li> </ul> <p>Inside the above directory create a <code>scripts/startup</code> directory. Any <code>.py</code> files placed there will be automatically executed when Blender starts. See this page for other special directories within the system-specific USER directory.</p>"},{"location":"advanced/python_scripting/2_accessing_data/","title":"Accessing Blender data","text":""},{"location":"advanced/python_scripting/2_accessing_data/#using-bpydata","title":"Using <code>bpy.data</code>","text":"<p>All data in a Blender file can be accessed through <code>bpy.data</code>. This contains, for example, all objects (<code>bpy.data.objects</code>), all meshes (<code>bpy.data.meshes</code>), all scenes (<code>bpy.data.scenes</code>) and all materials (<code>bpy.data.materials</code>). </p> <p>The data is stored in a data-type called <code>bpy_collection</code> whose members (data blocks) can be accessed with both an index as well as a string (this in contrary to regular Python dictionaries). For example. <code>bpy.data.objects[\"Camera\"]</code> and <code>bpy.data.objects[0]</code> will be equivalent if Camera is the first object in the collection:</p> <pre><code>&gt;&gt;&gt; bpy.data.objects\n&lt;bpy_collection[2], BlendDataObjects&gt;\n\n&gt;&gt;&gt; len(bpy.data.objects)\n2\n\n&gt;&gt;&gt; bpy.data.objects[0]\nbpy.data.objects['Camera']\n\n&gt;&gt;&gt; bpy.data.objects['Camera']\nbpy.data.objects['Camera']\n</code></pre> <p>Attributes of data blocks (e.g an object, collection or material) can be accessed as regular Python attributes, for example:</p> <pre><code>&gt;&gt;&gt; bpy.data.objects[0].name\n'Camera'\n</code></pre> <p>Here's two examples of changing those attributes (note that some operations only work if Blender is in the right mode):</p> <pre><code>bpy.data.objects[\"Cube\"].location.z += 1              # this works in both edit and object mode\nbpy.data.objects[\"Cube\"].data.vertices[0].co.z += 10  # this works only in object mode\n</code></pre> <p>Tips</p> <ul> <li>Use the Python Console in Blender and the auto-complete functionality (<code>TAB</code>) to see what attributes <code>bpy.data</code> has.</li> <li>The Info Editor in Blender shows the python commands being executed when you do operations manually in Blender (See Fig. 2.)</li> <li>Hovering over buttons and input boxes in Blender shows how to access the underlying values through the Python API.</li> </ul> <p> Figure 2: The Info Editor is a nice way to see what python commands are executed when you use Blender. In this figure we see that we deleted the initial cube, made a UV Sphere and translated it. </p>"},{"location":"advanced/python_scripting/2_accessing_data/#some-notes-on-bpycontext-and-bpyops","title":"Some notes on <code>bpy.context</code> and <code>bpy.ops</code>","text":"<p>In this section we want to briefly introduce how you can access something called the context, and use operators in the Blender Python API. <code>bpy.context</code> stores information about a user's selections and the context Blender is in. For example, if you want to check which mode is currently active in Blender you can check the value of <code>bpy.context.mode</code>. </p> <p>Now if you want to change the mode, you can use an operator. Operators are tools that are usually accessed through the user interface with buttons and menus. You can access these operators with Python through <code>bpy.ops</code>. If we would like to change the mode we can do this using an operator, e.g. <code>bpy.ops.object.mode_set(mode='OBJECT')</code></p> <p>Of course the possibility of switching to, say, edit mode, depends on which objects are selected, which can be checked with <code>bpy.context.selected_objects</code>. But keep in mind that many of the variables in the context are read-only, for example altering <code>bpy.context.selected_objects</code> directly is not possible. Instead, you can select an object with the <code>select_set()</code> method of the object, e.g. <code>bpy.data.objects['Cube'].select_set(True)</code>.  </p>"},{"location":"advanced/python_scripting/2_accessing_data/#running-a-script-and-rendering-from-the-console","title":"\ud83d\udcbb Running a script and rendering from the console","text":"<ol> <li>Write an external script that removes the Cube object that is part of the default scene <sup>1</sup> </li> <li>Then, from the command line and without opening the Blender GUI execute this script and render the first frame. Let it output a PNG image file in the directory of the blender file. </li> <li>Was the cube indeed removed from the rendered image?</li> <li>Extra question: is the cube removed from the blender file?</li> </ol> <ol> <li> <p>Although you might have altered your startup scene to not have the cube\u00a0\u21a9</p> </li> </ol>"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/","title":"Geometry, colors and materials","text":""},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#creating-an-object-with-a-mesh","title":"Creating an object with a mesh","text":"<p>If we want to create a new mesh we can do this by calling the <code>new</code> function like this: <pre><code>mesh = bpy.data.meshes.new(\"newMesh\")\n</code></pre> This will create the mesh but it is not linked to an object (it will not show in the Outliner). So we make a new object and link the object to the mesh: <pre><code>obj = bpy.data.objects.new(\"newObject\", mesh)\n</code></pre></p> <p>We can actually verify this worked correctly by checking the value of <code>obj.data</code>:</p> <pre><code>&gt;&gt;&gt; obj.data\nbpy.data.meshes['newMesh']\n</code></pre> <p>If you check the Outliner in the user interface you will see both the object <code>newObject</code> and the mesh <code>newMesh</code> linked to it.</p> <p>Now we have an empty mesh, linked to an object. We will now construct a simple piece geometry to show how this is done in Blender. Vertices are defined by their x, y and z values like this: <pre><code>verts = [ (0,0,0), (0,2,0), (0,1,2) ]\n</code></pre></p> <p>Edges are defined as a tuple holding two indices pointing to two vertices in the <code>verts</code> list. So (0,1) refers to a line from vertex (0,0,0) (index 0 in <code>verts</code>) to (0,2,0) (index 1 in <code>verts</code>) in this example. We make the following edges: <pre><code>edges = [ (0,1), (1,2), (2,0) ]\n</code></pre></p> <p>To make faces we need three or more vertices. Per face you make a tuple of three or more indices pointing to three vertices in the <code>verts</code> list. For example the face <code>(0,1,2)</code> is a face made up from the vertices (0,0,0), (0,2,0) and (0,1,2), which are at index 0, 1 and 2 in the <code>verts</code> list. For now lets make one face: <pre><code>faces = [ (0,1,2) ]\n</code></pre></p> <p>We now use a function from the Python API to make a mesh from our verts, edges and faces: <pre><code>mesh.from_pydata(verts, edges, faces)\n</code></pre></p> <p>Now the mesh and the object are created, but it does not yet show in the 3D viewport or the Outliner. This is because we still need to link the new object to an existing collection and in so doing to a scene.  <pre><code>bpy.data.collections[0].objects.link(obj)\n</code></pre></p> <p>To summarize here is the full code to generate this geometry: <pre><code>import bpy\n\n# Create a new mesh\nob_name = \"triangle\"\nmesh = bpy.data.meshes.new(ob_name + \"_mesh\")\n\n# Create a new object with the mesh\nob = bpy.data.objects.new(ob_name, mesh)\n\n# Define some geometry\nverts = [ (0,0,0), (0,2,0), (0,1,2) ]\nedges = [ (0,1), (1,2), (2,0) ] # These are indices pointing to elements in the list verts\nfaces = [ (0,1,2) ] # These are indices pointing to elements in the list verts\n\n# Add it to the mesh\nmesh.from_pydata(verts, edges, faces)\n\n# Link the object to the first collection\nbpy.data.collections[0].objects.link(ob)\n</code></pre></p> <p>Tips</p> <ul> <li>Note that in general you do not need to explicitly specify mesh edges, as these will be generated automatically based on the faces specified. It's only when you want to have edges that are not connected to faces that you need to specify them explicitly.</li> </ul> <ul> <li>All objects in Blender (and object data of the same type, i.e. all meshes) are enforced to have unique names. When using the Python API this is no different. So if you create an object with <code>bpy.data.objects.new(\"obj\", mesh)</code> and there already is an object named \"obj\" the name of the new object will be automatically set to something else. This can become important if you generate many objects (say in a loop) but still want to be able to refer to them later by name.</li> </ul>"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#a-filled-disk-from-scratch","title":"\ud83d\udcbb A filled disk from scratch","text":"<p>In the text above we created a triangle, now as an exercise let's create a spherical disk. First create a ring of vertices, then create edges and a face.</p>"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#adding-vertex-colors-to-a-mesh","title":"Adding vertex colors to a mesh","text":"<p>Not seeing vertex colors?</p> <p>In the video below there's an essential step that's only shown near the end (around 7:00), which setting a material on the geometry. If the correct material isn't set the vertex colors won't show.</p> <p> </p> <p>Vertex coloring is a way to color a mesh without using textures or uv-mapping. It works by assigning for every face that a vertex is a member of a color to that vertex. So a vertex can have different colors for each of the different faces it is in. Let's say we have a mesh, named \"triangle_mesh\": <code>mesh = bpy.data.meshes['triangle_mesh']</code>, the vertex colors for this mesh will be stored in <code>mesh.vertex_colors</code>. If the mesh does not have a vertex color layer yet, you can make a new one with: <code>mesh.vertex_colors.new(name='vert_colors')</code>. Now we have a color layer to work with: <code>color_layer = mesh.vertex_colors['vert_colors']</code>. </p>"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#making-triangles-and-a-vertex-color-layer","title":"\ud83d\udcbb Making triangles and a vertex color layer","text":"<p>Let's take the triangle we made above, but let's add another triangle to it, attached to the first. The code would look like this: <pre><code>import bpy\n\n# Create a new mesh\nob_name = \"triangle\"\nmesh = bpy.data.meshes.new(ob_name + \"_mesh\")\n\n# Create a new object with the mesh\nob = bpy.data.objects.new(ob_name, mesh)\n\n# Define some geometry\nverts = [ \n        (0,0,0), (0,2,0), (0,1,2) ,\n        (0,3,2)\n        ]\nedges = [ \n        (0,1), (1,2), (2,0),  \n        (1,3), (3, 2)\n        ] # These are indices pointing to elements in the list verts\nfaces = [ (0,1,2), (1,3,2) ] # These are indices pointing to elements in the list verts\n\n# Add it to the mesh\nmesh.from_pydata(verts, edges, faces)\n\n# Link the object to the first collection\nbpy.data.collections[0].objects.link(ob)\n</code></pre></p> <p>Now make a vertex color layer for your triangles. Then inspect how many entries are in <code>color_layer = mesh.vertex_colors['vert_colors']</code>. Why are they the same or different from the total number of vertices in the mesh?</p> <p>In an earlier exercise we saw that <code>color_layer.data</code> contains six entries while we only have four vertices in the mesh. This is because a vertex has a color for every face it is in. So vertex <code>(0,2,0)</code> and <code>(0,1,2)</code> are each in two faces, while the other two vertices are only in one face. So the former vertices have two entries in the color layer, one for each face they are in, the latter only one color entry.</p> <p>The link between vertex indices in a mesh and those in the vertex color layer can be deduced from the polygons in <code>mesh.polygons</code>. Let's take one polygon from the triangles, lets say the first (<code>poly = mesh.polygons[0]</code>). Now, for one vertex in the polygon, <code>poly.vertices</code> gives you the index of the vertex in the mesh and <code>poly.loop_indices</code> gives you the index of the vertex in <code>color_layer.data</code>. See Fig. 3.</p> <p> Figure 3: Sketch of the two triangles from Exercise 4. For the vertices are shown the coordinates (in black italic (x, x, x)), indices of the vertex in its mesh (green, outside of the face) and the indices in the loop_indices of the polygon (red, italic and inside the faces.) </p> <p>Once you have set colors for your vertices you need to set up the shader of the object. For this go to the <code>Shading</code> workspace. Create a <code>Vertex Color</code> node and connect it to a <code>Principled BSDF</code> (connect <code>Color</code> output to <code>Base Color</code> input). And then make a <code>Material Output</code> and connect the <code>Principled BSDF</code> to the <code>Surface</code> input of the <code>Material Output</code>. See Fig. 4.</p> <p> Figure 4: Shader setup for vertex colors </p>"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#coloring-your-triangles","title":"\ud83d\udcbb Coloring your triangles","text":"<p>Let's take the two connected triangles of exercise 4. We will color them in two different ways, using vertex coloring and Python scripting:</p> <ul> <li>Make the first triangle (face (0,1,2)) green and the second (face (1,3,2)) red.</li> <li>Now color vertex (0,0,0) and (0,3,2) red and (0,2,0) and (0,1,2) green.</li> </ul>"},{"location":"advanced/python_scripting/3_geometry_colors_and_materials/#adding-a-material","title":"Adding a material","text":"<p>You can also add materials through the Python API. As an example to show how you could do this, let's add a material to the triangle from exercise 4 in the last section. Materials are stored in <code>bpy.data.material</code> and we can make a new material:</p> <p><pre><code># Make material\ntriangle_material_name = \"triangle_mat\"\nmat = bpy.data.materials.new(triangle_material_name)\n</code></pre> The nodes and the node tree are stored in the material (node-based materials will be further described in another chapter).</p> <pre><code>mat.use_nodes = True\nnodes = mat.node_tree.nodes\n</code></pre> <p>Before we start making nodes we remove the automatically generated nodes.</p> <p><pre><code>nodes.clear()\n</code></pre> We will make two nodes, one Principled BSDF shader and an output node. We can make the shader by making a new node.</p> <p><pre><code>shader = nodes.new(type='ShaderNodeBsdfPrincipled')\n</code></pre> How a node type is called you can search up in Blender in the following way. Go to the <code>Shading</code> workspace and open the add menu in the Shader Editor. Now go to <code>Shader</code> and hover over <code>Principled BSDF</code> until an information pop-up appears. In the pop-up you can find how the node type is called. See Fig. 5.</p> <p> Figure 5: The type name of a node can be found by navigating to the <code>Add</code> menu and hovering over the node of your interest </p> <p>If you also want to organize the nodes in the Shader Editor you can place the node like this:</p> <p><pre><code>shader.location = 0, 300 # Location in the node window\n</code></pre> We can set the inputs of the Principled BSDF shader to a default_value.</p> <p><pre><code>shader.inputs['Base Color'].default_value = (1,0,0,1)\n</code></pre> We can now also make an output node and place it in the Shader Editor.</p> <p><pre><code>node_output = nodes.new(type='ShaderNodeOutputMaterial')\nnode_output.location = 400, 300\n</code></pre> Links between nodes can be made using the links in the node_tree. A new link will take outputs and inputs from the nodes you want to link.</p> <p><pre><code>links = mat.node_tree.links\nlinks.new(shader.outputs[0], node_output.inputs[0])\n</code></pre> Now we only need to add the material to the mesh containing the spherical disk.</p> <pre><code>mesh.materials.append( mat )\n</code></pre> <p>In summary, the total code for making the material is:</p> <pre><code># Make material\ntriangle_material_name = \"triangle_mat\"\nmat = bpy.data.materials.new(triangle_material_name)\n\nmat.use_nodes = True\nnodes = mat.node_tree.nodes\n\n# Clear default nodes\nnodes.clear()\n\nshader = nodes.new(type='ShaderNodeBsdfPrincipled')\nshader.location = 0, 300 # Location in the node window\nshader.inputs['Base Color'].default_value = (1,0,0,1)\n\n# Create an output for the shader\nnode_output = nodes.new(type='ShaderNodeOutputMaterial')\nnode_output.location = 400, 300\n\nlinks = mat.node_tree.links\nlinks.new(shader.outputs['BSDF'], node_output.inputs['Surface'])\n\nmesh.materials.append( mat )\n</code></pre>"},{"location":"advanced/python_scripting/4_volumetric_data/","title":"Visualizing volumetric data through OpenVDB","text":"<p>In this section we will show a simple example of how to visualize custom volumetric data with Blender and Python. The current support in Blender for volumetric data is directly tied to the OpenVDB file format. In fact, the only way to create a volume object is to load an OpenVDB file. This is a file format and data structure that originated from the motion-picture industry, where it is often used to show clouds, smoke and fire in computer graphics like movies and games. Here's an example of such a volumetric rendering:</p> <p> Gasoline explosion. Free example from Embergen. <p></p> <p>The reason OpenVDB is used for many volumetric data applications in computer graphics is that is allows sparse volumes to be stored efficiently, while also providing easy querying of the data, for example during rendering. OpenVDB is also a bit more than just a file format, as the OpenVDB library also supports more advanced operations. From the OpenVDB website:</p> <p>OpenVDB is an Academy Award-winning C++ library comprising a hierarchical data structure and a suite of tools for the efficient manipulation of sparse, time-varying, volumetric data discretized on three-dimensional grids. It is based on VDB, which was developed by Ken Museth at DreamWorks Animation, and it offers an effectively infinite 3D index space, compact storage, fast data access, and a collection of algorithms specifically optimized for the data structure for common tasks such as filtering, CSG, compositing, numerical simulation, sampling, and voxelization from other geometric representations.</p> <p>For more documentation on OpenVDB see here. Some example OpenVDB files can be found here, under Sample Models.</p>"},{"location":"advanced/python_scripting/4_volumetric_data/#example","title":"Example","text":"<p>OpenVDB models are mostly generated with specialized software like Houdini and Embergen. Volumetric data in general is also used for scientific visualizations, for example in ParaView, but support for OpenVDB is still lacking somewhat. In this section we will explain how OpenVDB files can be made from scratch. For example for when you have you own volumetric data in your own data format and you want to visualize or animate this in Blender. To convert your data to a OpenVDB format we will use the Python package pyopenvdb.</p> <p>First we will create data in Python and write it to an OpenVDB file using OpenVDB and the Python package pyopenvdb.</p> <p> </p>"},{"location":"advanced/python_scripting/4_volumetric_data/#pyopenvdb-dependency","title":"pyopenvdb dependency","text":"<p>Recent versions of the official Blender distribution from https://www.blender.org have the <code>openvdb</code> Python module included, so manual installation is not needed anymore.</p> <p>However, if you are using Blender based on a Linux distro package, or other method for installation, pyopenvdb might not be included. To check you can either go into the Python editor and execute the command <code>import openvdb</code>, or check from the command-line with <code>blender -b --python-expr 'import openvdb; print(openvdb.LIBRARY_VERSION)'</code>.</p>"},{"location":"advanced/python_scripting/4_volumetric_data/#manually-installing-pyopenvdb","title":"Manually installing pyopenvdb","text":"<p>Installing the Python module to access the OpenVDB functionality can be very easy or more difficult depending on your operating system. See the installation instructions on the pyopenvdb website. Note that the OpenVDB source code includes Python bindings, so if you know your way around building it from source you can also get Python bindings that way.</p> <p>Tip</p> <p>If you cannot get it to work that way, we made a simple Docker container you can use to run it, see here for the github repository.</p>"},{"location":"advanced/python_scripting/4_volumetric_data/#making-a-vdb-file-with-pyopenvdb","title":"Making a VDB file with pyopenvdb","text":"<p>Let us make a simple volumetric cube using the <code>openvdb</code> module. To start we first load pyopenvdb and numpy: <pre><code>import numpy as np\nimport openvdb as vdb\n</code></pre></p> <p>And we make a zero filled array of size 400x400x400: <pre><code>dimension = 400\narray = np.zeros((dimension, dimension, dimension))\n</code></pre></p> <p>We then fill a cube sized portion of the array with the value 1: <pre><code>for i in range(dimension):\n   for j in range(dimension):\n      for k in range(dimension):\n         if i &lt; 200 and i &gt;=100 and \\\n          j &lt; 200 and j &gt;=100 and \\\n          k &lt; 200 and k &gt;=100:\n\n            array[i,j,k] = 1.0\n</code></pre></p> <p>Now we come to the <code>openvdb</code> part, where we first need to make a grid. In this case we make a float grid (there are more grids besides a float grid for example a BoolGrid and Vec3SGrid are also standardly available).</p> <pre><code>grid = vdb.FloatGrid()\n</code></pre> <p>We now copy the values in the array into the grid:</p> <pre><code>grid.copyFromArray(array)\n</code></pre> <p>The last important thing we need to do before we save it to file is to name the grid. You will use this name later when using the grid in Blender.</p> <pre><code>grid.name = \"cube\"\n</code></pre> <p>The last thing left to do is to save the grid to file:</p> <pre><code>vdb.write('cube.vdb', grids=[grid])\n</code></pre>"},{"location":"advanced/python_scripting/4_volumetric_data/#loading-a-vdb-file-into-blender","title":"Loading a VDB file into Blender","text":"<p>Open a new Blender file and if its there, remove the starting cube. In the 3D viewport choose menu option <code>Add &gt; Volume &gt; Import OpenVDB...</code> (or use the shortcut <code>Shift-A</code> to open the Add menu). Locate the <code>cube.vdb</code> file we just made through the script. You will most likely not see anything yet, so scale the cube down using the shortcut <code>S</code> until you can see the outline of the cube. Now if you change the Viewport shading in the top right of the 3D viewport to <code>Rendered</code> (see Fig. 1, #1), you will not see anything beside the outline since we still need to add a shader to the model.</p> Figure 1: definition of coordinates <p>Change to the Shading workspace (see Fig. 1, #2) and in the Shader editor click on new to make a new material (see Fig. 1, #3). You see Blender makes a <code>Principled volume</code> and <code>Material output</code> node. To make the cube appear we need to change one thing and for this we need to know the name of the grid in the VDB file.</p> <p>From the Python script we know this is <code>cube</code>, but you can also figure out the grids and their names in a VDB file from within Blender. In the Properties panel go to Object Data Properties tab (see Fig. 1, #4). Here under Grids you can see the name(s) of the grids in the VDB file. For now, in the Principled Volume node, add the name of the grid (<code>cube</code>) into the field next to <code>Density Attribute</code> (see Fig. 1, #5). This tells the node to use the values in the grid for the scattering density of the voxels.</p> <p>Reloading a VDB file currently does not work</p> <p>There is a bug in Blender 4.5 (and even in the first 5.0 release candidate) that causes reloading of an already loaded VDB file to fail to update the data from disk. The only workaround currently is to restart Blender.</p>"},{"location":"advanced/python_scripting/4_volumetric_data/#coloring-the-cube","title":"\ud83d\udcbb Coloring the cube","text":"<p>Now make a cube similar to the one we just made, but color it blue on one side and red on the other (See Fig. 2). First alter the Python script to include a second grid in the VDB file. In this second grid set one side of the cube to value 1 and the other to zero. Use an Attribute node (do not forget to add the grid name to the <code>Name:</code> field in the attribute node) to feed the second grid into a Color Ramp node (and choose the colors you want). Now feed the Color Ramp into the <code>Color</code> field of the Principled Volume. Do not forget to set the original grid in the <code>Density Attribute</code>.</p> <p>Does it come out right? Maybe you need to play a bit with settings, like set the <code>Density</code> to 1. You might also need to play with the lighting. If you still have the original light in your scene, try increasing its <code>Power</code> and location. Now also see how it looks in Cycles compared to Eevee.</p> Figure 2: Colored cube"},{"location":"api/10000_foot_view/","title":"The 10,000 foot view","text":""},{"location":"api/10000_foot_view/#introduction","title":"Introduction","text":"<p>The Blender Python API mostly consists of a thin layer on top of the underlying Blender C/C++ data structures and methods. The underlying C/C++ code is used for automatically generating the Python API during the build process of the Blender executable, which means the API is always up-to-date with respect to the underlying code. </p> <p>The user-facing Python API isn't the only part of Blender that uses Python. Large parts of the user interface, most import/export functionality and all add-ons are written in Python. It is therefore relatively easy to extend Blender with, say, new UI dialogs or a custom importer. This is one of the strengths of the Blender Python API.</p> <p>Some things to be aware of:</p> <ul> <li>Blender 4.1 embeds the Python 3.11 interpreter.</li> <li>You can access the online API documentation from within Blender with <code>Help &gt; Python API Reference</code></li> <li>Starting Blender from the console will allow you to see important outputs channels (warnings, exceptions, output of <code>print()</code> statements, etc).</li> </ul> <p>The earlier chapter on the Python API provides a hands-on introduction, including basic information on how to execute Python scripts in Blender.</p> <p>Be careful</p> <p>Since the API provides access to Blender internals at a very low level you can screw up the Blender state, causing unexpected behaviour, data corruption or even crashes. In the worst case you can end up with a file that will no longer load in Blender at all, although that's rare.</p> <p>So when working with Python scripting, save your session to file often, preferably in a number of incremental versions, so you can recover or go a step back when needed.</p> <p>In cases where you suspect Blender's current internal state has been corrupted you can save the current state to a temporary file, start a second instance of Blender (keeping the first Blender running!) and then open the temporary file in the second instance to help ensure you can start from a known-good state. This prevents you from saving a corrupt Blender state and overwriting your last known-good file.</p>"},{"location":"api/10000_foot_view/#api-modules","title":"API modules","text":"<p>The Blender Python API is comprised of several modules, with <code>bpy</code> being the main one. But there's also useful routines in <code>mathutils</code>, <code>bmesh</code> and a few others.</p> <p>Accessing API reference documentation</p> <p>The API documentation on these modules can be easily accessed from within Blender using <code>Help &gt; Python API Reference</code>.</p> <p>No modules loaded by default</p> <p>By default none of the API modules, not even <code>bpy</code>, are loaded in the environment where a script file runs, so you need to import the ones you need explicitly.</p> <p>The Python Console does import quite a few things by default and also sets some useful variables, like <code>C</code> to access <code>bpy.context</code> and <code>D</code> to access <code>bpy.data</code> with less typing:</p> <pre><code>PYTHON INTERACTIVE CONSOLE 3.11.7 (main, Feb  7 2024, 17:07:03) [GCC 11.2.1 20220127 (Red Hat 11.2.1-9)]\n\nBuiltin Modules:       bpy, bpy.data, bpy.ops, bpy.props, bpy.types, bpy.context, bpy.utils, bgl, gpu, blf, mathutils\nConvenience Imports:   from mathutils import *; from math import *\nConvenience Variables: C = bpy.context, D = bpy.data\n\n&gt;&gt;&gt; D.objects.values()\n[bpy.data.objects['Camera'], bpy.data.objects['Cube'], bpy.data.objects['Light']]\n\n&gt;&gt;&gt; C.active_object\nbpy.data.objects['Cube']\n</code></pre>"},{"location":"api/10000_foot_view/#developer-settings","title":"Developer settings","text":"<p>When developing Python scripts in Blender it can be useful to enable a few extra settings:</p> <ul> <li>The Python Tooltips under <code>Interface &gt; Display &gt; Python Tooltips</code>. When enabled a tooltip will show the corresponding Python command or a path to the data for a UI element.     </li> <li>The Developer Extras under <code>Interface &gt; Display &gt; Developer Extras</code>. When enabled this provides multiple things:<ul> <li>The 3D viewport overlay for a mesh in edit mode will now have an extra setting <code>Indices</code> to show the low-level indices of selected vertices/edges/faces. This can be very useful when debugging Python code that works on mesh geometry.</li> <li>The right-click menu for a UI item, such as a button or menu entry, will now also contain an entry called <code>Online Python Reference</code> linking to the relevant Python documentation page.</li> <li>It will enable Operator Search, which will add entries to the <code>F3</code> search menu for operators. These will be listed after the regular menu entries in the search results.</li> <li>It adds a new menu option <code>Help &gt; Operator Cheat Sheet</code> that will create a new text area called <code>OperatorList.txt</code>, which contains all available operators (see Operators) and their default parameters. This list can give you a quick overview of the available operators, with the API documentation providing all the details.</li> </ul> </li> </ul>"},{"location":"api/10000_foot_view/#info-area","title":"Info area","text":"<p>As mentioned in the video in the introductory chapter the Info area can be useful if you want to inspect which Python calls Blender performs for certain operations. This certainly will not provide all the details in all cases, but can give some insight. You can either switch to the default Scripting workspace (using the tabs at the top of the window) to check the output, or use the normal UI area operations to add/change an area to an Info area. The latter is shown below:</p> <p></p>"},{"location":"api/10000_foot_view/#sources-of-examples","title":"Sources of examples","text":"<p>This chapter provides small snippets of code and serves mostly as a reference. Sometimes it can be useful to get more information or examples of how specific parts of the Blender Python API are used. Some good sources for other code are:</p> <ul> <li>The add-ons included with Blender show many uses of the Python API. They can be found in the directory <code>&lt;blender-version&gt;/scripts/addons</code> in the Blender distribution directory.</li> <li>A number of script templates are also included, in <code>&lt;blender-version&gt;/scripts/templates_py</code>, mostly examples of defining custom operators or UI elements.</li> </ul>"},{"location":"api/10000_foot_view/#data-blocks","title":"Data-blocks","text":"<p>The different types of data in Blender are stored in data-blocks. For example, there's Mesh, Object, Texture and Shader data-blocks, but there's quite a few more. One of the clever bits in the way Blender is programmed is that data-blocks written to file contain enough information about their content (i.e. metadata) to make them readable by both older and newer versions of Blender than the one they were written with. This metadata system also makes it possible to automatically provide the Python API for accessing those data-blocks without much manual work from Blender's developers.</p> <p>Data-blocks are available through Python, per type, under <code>bpy.data</code>. For example, there's <code>bpy.data.objects</code> and <code>bpy.data.meshes</code>. The type of a data-block is the corresponding class under <code>bpy.types</code>:</p> <pre><code>&gt;&gt;&gt; type(bpy.data.objects['Cube'])\n&lt;class 'bpy_types.Object'&gt;\n\n&gt;&gt;&gt; bpy.types.Object\n&lt;class 'bpy_types.Object'&gt;\n</code></pre> <p>Each type of data-block has its own set of attributes and methods, particular to that type. Learning the Blender Python API involves getting to know the details of the data-block types you want to work with and how they interact.</p> <p>Automatic data-block garbage collection</p> <p>Blender keeps track of which data-blocks are no longer being referenced to decide when a data-block does not need to be saved (so-called garbage collection). Usually you don't need to explicitly interact with this system, but it is good to be aware that it is there, see this section for more details.</p>"},{"location":"api/10000_foot_view/#unique-data-block-names","title":"Unique data-block names","text":"<p>Per type of data all the data-blocks need to have a unique name. This is enforced automatically by Blender when a data-block is created by appending a number to make the name unique. For example:</p> <pre><code>&gt;&gt;&gt; bpy.data.meshes.new('my object')\nbpy.data.meshes['my object']\n\n&gt;&gt;&gt; bpy.data.meshes.new('my object')\nbpy.data.meshes['my object.001']\n\n&gt;&gt;&gt; bpy.data.meshes.new('my object')\nbpy.data.meshes['my object.002']\n</code></pre> <p>This usually isn't an issue, but just something to be aware of when working with referencing objects by name, as the name of a data-block you created might sometimes actually be different than you expect.</p>"},{"location":"api/10000_foot_view/#objects-and-object-data","title":"Objects and object data","text":"<p>When we use the word \"Object\" in these pages we mean one of the object types that can be present in a 3D scene, for example a camera, mesh or light. Such objects are of type <code>bpy.types.Object</code> and all have general properties related to their presence in the 3D scene. For example, their name, 3D transformation, visibility flags, parent, etc.</p> <p>But a Light object needs to specify different properties than, say, a Camera object and these per-type properties are stored as \"object data\". The object data can be accessed through the <code>data</code> attribute of an Object:</p> <pre><code># Both lights and cameras are Objects\n&gt;&gt;&gt; type(bpy.data.objects['Light'])\n&lt;class 'bpy_types.Object'&gt;\n\n&gt;&gt;&gt; type(bpy.data.objects['Camera'])\n&lt;class 'bpy_types.Object'&gt;\n\n# But their object data are of a different type\n&gt;&gt;&gt; type(bpy.data.objects['Camera'].data)\n&lt;class 'bpy.types.Camera'&gt;\n\n&gt;&gt;&gt; type(bpy.data.objects['Light'].data)\n&lt;class 'bpy.types.PointLight'&gt;\n\n# And have different attributes, relevant to that type\n&gt;&gt;&gt; dir(bpy.data.objects['Camera'].data)\n[..., 'angle', ..., 'clip_start', ..., 'dof', ...]\n\n&gt;&gt;&gt; dir(bpy.data.objects['Light'].data)\n[..., 'color', ..., 'cutoff_distance', ..., 'energy', ...]\n</code></pre>"},{"location":"api/10000_foot_view/#objects-of-a-specific-type","title":"Objects of a specific type","text":"<p>Sometimes you want to iterate over all objects in a scene, but only perform some operation on a specific type of object. You can use the <code>type</code> attribute for checking an object's type:</p> <pre><code>&gt;&gt;&gt; bpy.data.objects['Camera'].type\n'CAMERA'\n\n&gt;&gt;&gt; bpy.data.objects['Light'].type\n'LIGHT'\n\n&gt;&gt;&gt; for obj in bpy.data.objects:\n    if obj.type == 'MESH':\n        # Do something\n        ...\n</code></pre>"},{"location":"api/10000_foot_view/#native-blender-data-structures","title":"Native Blender data structures","text":"<p>When working with the Python API will you frequently use internal Blender types that appear similar to regular Python types, like lists and dictionaries. However, the Blender types are not real native Python types and behave differently in certain aspects.</p> <p>For example, the different collections of scene elements, such as objects or meshes, that are available under <code>bpy.data</code> are of type <code>bpy_prop_collection</code>. This type is a combination of the Python list and dictionary types, sometimes called an ordered dictionary, as it allows indexing by both array position as well as key:</p> <pre><code>&gt;&gt;&gt; type(bpy.data.objects)\n&lt;class 'bpy_prop_collection'&gt;\n\n# Some of its methods match those of native Python data types\n&gt;&gt;&gt; dir(bpy.data.objects)\n['__bool__', '__contains__', '__delattr__', '__delitem__', '__doc__', \n'__doc__', '__getattribute__', '__getitem__', '__iter__', '__len__', \n'__module__', '__setattr__', '__setitem__', '__slots__', 'bl_rna', \n'find', 'foreach_get', 'foreach_set', 'get', 'items', 'keys', 'new', \n'remove', 'rna_type', 'tag', 'values']\n\n# Index by position\n&gt;&gt;&gt; bpy.data.objects[0]\nbpy.data.objects['Camera']\n\n# Index by key\n&gt;&gt;&gt; bpy.data.objects['Camera']\nbpy.data.objects['Camera']\n\n# (key, value) pairs\n&gt;&gt;&gt; bpy.data.objects.items()\n[('Camera', bpy.data.objects['Camera']), ('Cube', bpy.data.objects['Cube']), \n('Light', bpy.data.objects['Light'])]\n</code></pre> <p>Note that the position of an item in the collection, and hence its index, can change during a Blender session.</p>"},{"location":"api/10000_foot_view/#inspecting-values","title":"Inspecting values","text":"<p>One of the more annoying aspects when working in the Blender Python Console inspecting these kinds of values is that the elements in a <code>bpy_prop_collection</code> (or other Blender types) aren't printed by default, this in contrast to a regular Python dictionary. You need to, for example, cast to a list or call its <code>values()</code> method:</p> <pre><code># Regular Python dict, prints both keys and values\n&gt;&gt;&gt; d = dict(a=1, b=2, c=3)\n&gt;&gt;&gt; d\n{'a': 1, 'b': 2, 'c': 3}\n\n# No items printed\n&gt;&gt;&gt; bpy.data.objects\n&lt;bpy_collection[3], BlendDataObjects&gt;\n\n# values() returns a list, so gets printed in detail\n&gt;&gt;&gt; type(bpy.data.objects.values())\n&lt;class 'list'&gt;\n\n&gt;&gt;&gt; bpy.data.objects.values()\n[bpy.data.objects['Camera'], bpy.data.objects['Cube'], bpy.data.objects['Light']]\n\n# Difference in list() result:\n&gt;&gt;&gt; list(d)\n['a', 'b', 'c']\n# Returns dict *keys*\n\n&gt;&gt;&gt; list(bpy.data.objects)\n[bpy.data.objects['Camera'], bpy.data.objects['Cube'], bpy.data.objects['Light']]\n# Returns collection *values*\n</code></pre> <p>The choice for not printing the values inside a <code>bpy_prop_collection</code> is (most likely) that in many cases the collection will contain large numbers of objects, so printing them all would not be too useful, or might even make the UI non-responsive for a short time.</p>"},{"location":"api/10000_foot_view/#data-organization","title":"Data organization","text":"<p>In certain cases Blender uses a more elaborate data structure where you might except low-level values, like lists. For example, the set of vertices that make up a mesh are only accessible as a collection of <code>MeshVertex</code> objects:</p> <pre><code>&gt;&gt;&gt; m\nbpy.data.meshes['Cube']\n\n&gt;&gt;&gt; type(m.vertices)\n&lt;class 'bpy_prop_collection'&gt;\n\n&gt;&gt;&gt; len(m.vertices)\n8\n\n&gt;&gt;&gt; m.vertices[0]\nbpy.data.meshes['Cube'].vertices[0]\n\n&gt;&gt;&gt; type(m.vertices[0])\n&lt;class 'bpy.types.MeshVertex'&gt;\n\n&gt;&gt;&gt; dir(m.vertices[0])\n['__doc__', '__module__', '__slots__', 'bl_rna', 'co', 'groups', \n'hide', 'index', 'normal', 'rna_type', 'select', 'undeformed_co']\n\n# Vertex coordinate (object space)\n&gt;&gt;&gt; m.vertices[0].co\nVector((1.0, 1.0, 1.0))\n\n# Vertex normal\n&gt;&gt;&gt; m.vertices[0].normal\nVector((0.5773491859436035, 0.5773491859436035, 0.5773491859436035))\n</code></pre> <p>The reason for this is that there's several types of data associated with a single vertex, which are all centralized in a <code>MeshVertex</code> object. In short, Blender uses a so-called array-of-structs design. The alternative design choice would have been to have separate arrays for vertex coordinates, vertex normals, etc (which would be a struct-of-arrays design).</p>"},{"location":"api/10000_foot_view/#vertices-and-matrices","title":"Vertices and matrices","text":"<p>The example above also shows that even a vertex coordinate is not accessed as a low-level Python data type, like a tuple, but by the <code>Vector</code> type (which is in the <code>mathutils</code> module). This has the advantage of providing many useful methods for operating on vector values:</p> <pre><code>&gt;&gt;&gt; v = m.vertices[0].normal\n&gt;&gt;&gt; v\nVector((0.5773491859436035, 0.5773491859436035, 0.5773491859436035))\n\n&gt;&gt;&gt; v.length\n0.999998137353116\n\n# Return a new vector that's orthogonal to v\n&gt;&gt;&gt; w = v.orthogonal()\n&gt;&gt;&gt; w\nVector((0.5773491859436035, 0.5773491859436035, -1.154698371887207))\n\n# Dot product (should be zero as v and w are orthogonal)\n&gt;&gt;&gt; v.dot(w)\n0.0\n\n# Note: v*w is element-wise product, not dot product!\n&gt;&gt;&gt; v*w\nVector((0.3333320915699005, 0.3333320915699005, -0.666664183139801))\n\n# Cross product between two vectors\n&gt;&gt;&gt; v.cross(w)\nVector((-0.9999963045120239, 0.9999963045120239, 0.0))\n\n# Swizzling (returning vector elements in a different order)\n&gt;&gt;&gt; w\nVector((0.5773491859436035, 0.5773491859436035, -1.154698371887207))\n\n&gt;&gt;&gt; w.zxy\nVector((-1.154698371887207, 0.5773491859436035, 0.5773491859436035))\n</code></pre> <p>The builtin <code>mathutils</code> module contains many useful data types and methods for working with 3D data, including vectors and matrices, but also different methods for working with transformations (like quaternion) and colors spaces.</p> <pre><code># Transformation matrix for an object with uniform scale 2 and \n# translation in Z of 3. These values will match with the Transform UI area\n&gt;&gt;&gt; o\nbpy.data.objects['Cube']\n\n&gt;&gt;&gt; o.matrix_world\nMatrix(((2.0, 0.0, 0.0, 0.0),\n        (0.0, 2.0, 0.0, 0.0),\n        (0.0, 0.0, 2.0, 3.0),\n        (0.0, 0.0, 0.0, 1.0)))\n\n# Create a rotation matrix\n&gt;&gt;&gt; m = Matrix.Rotation(radians(90.0), 4, 'X')\n&gt;&gt;&gt; m\nMatrix(((1.0, 0.0, 0.0, 0.0),\n        (0.0, 7.549790126404332e-08, -1.0, 0.0),\n        (0.0, 1.0, 7.549790126404332e-08, 0.0),\n        (0.0, 0.0, 0.0, 1.0)))\n\n&gt;&gt;&gt; v = Vector((1,2,3))\n\n# Transform the vector using the matrix. Note the different outcomes \n# depending on the multiplication order.\n&gt;&gt;&gt; m @ v\nVector((1.0, -2.999999761581421, 2.000000238418579))\n\n&gt;&gt;&gt; v @ m\nVector((1.0, 3.000000238418579, -1.999999761581421))\n\n# Also, a 3-vector is assumed to have a fourth element equal to *one* when \n# multiplying with a matrix:\n&gt;&gt;&gt; m = Matrix.Translation((4, 5, 6))\n&gt;&gt;&gt; m\nMatrix(((1.0, 0.0, 0.0, 4.0),\n        (0.0, 1.0, 0.0, 5.0),\n        (0.0, 0.0, 1.0, 6.0),\n        (0.0, 0.0, 0.0, 1.0)))\n\n&gt;&gt;&gt; m @ Vector((1, 2, 3))\nVector((5.0, 7.0, 9.0))\n\n&gt;&gt;&gt; m @ Vector((1, 2, 3, 0))\nVector((1.0, 2.0, 3.0, 0.0))\n</code></pre>"},{"location":"api/10000_foot_view/#api-quirks","title":"API quirks","text":"<p>Working with the Blender Python API has some peculiarities compared to your average Python scripting. These have to do with the way the API is structured, but also how it interacts with the Blender internals. The API manual contains a lengthy page on some gotchas, but here we list some of the common ones.</p>"},{"location":"api/10000_foot_view/#object-interaction-modes","title":"Object interaction modes","text":"<p>An object is always in one of several interaction modes. These modes are the same ones you work with in the UI: Object mode, Edit mode, etc. The current mode for an object can be retrieved through the <code>mode</code> property:</p> <pre><code>&gt;&gt;&gt; o = bpy.data.objects['Cube']\n&gt;&gt;&gt; o.mode\n'OBJECT'\n\n# &lt;enter edit mode with TAB&gt;\n\n&gt;&gt;&gt; o.mode\n'EDIT'\n</code></pre> <p>Depending on the current mode of a mesh object certain data might not be up-to-date, or even unavailable, when accessing it through the Python API. This is especially true when an object is in Edit Mode. </p> <p>This is because the edit mode uses its own copy of the data to let you edit, which is synced with the underlying mesh data when going in and out of edit mode. See here for the relevant section in the Blender API docs.</p> <p>An example continuing with the Cube mesh above:</p> <pre><code>&gt;&gt;&gt; o.mode\n'OBJECT'\n\n&gt;&gt;&gt; m = o.data\n&gt;&gt;&gt; m\nbpy.data.meshes['Cube']\n\n# Check UV map data\n&gt;&gt;&gt; len(m.uv_layers[0].data)\n24\n\n# &lt;enter edit mode in the UI with TAB&gt;\n\n&gt;&gt;&gt; o.mode\n'EDIT'\n\n# UV map data now empty...\n&gt;&gt;&gt; len(m.uv_layers[0].data)\n0\n</code></pre> <p>In most cases when working on low-level data such as mesh geometry you want the object to be in object mode, or you can use the <code>bmesh</code> module when you need the object be in edit mode. It's usually a good idea to add a check in your script to verify the current mode is what you expect:</p> <pre><code>o = bpy.context.active_object\nif o.mode != 'OBJECT':\n    raise ValueError('Active object needs to be in object mode!')\n</code></pre> <p>There are alternatives for still allowing a mesh to be in edit-mode when accessing its data from a script, see the API docs for details.</p>"},{"location":"api/10000_foot_view/#interrupting-long-running-scripts","title":"Interrupting (long-running) scripts","text":"<p>During script development you might get in a situation where your code is stuck in a loop, or takes much longer than you like. Interrupting a running script can usually be done by pressing <code>Ctrl-C</code> in the terminal console window:</p> <pre><code>&gt;&gt;&gt; while True:\n...     pass\n...     \n\n# Uh oh, execution stuck in a loop and the Blender UI will now have become unresponsive\n</code></pre> <p>Pressing Ctrl-C in the terminal console window interrupts script execution, as it raises a KeyboardInterrupt:</p> <pre><code># Ctrl-C in terminal window\nTraceback (most recent call last):\n  File \".../4.1/scripts/startup/bl_operators/console.py\", line 32, in poll\n    @classmethod\n\nKeyboardInterrupt\n</code></pre>"},{"location":"api/10000_foot_view/#interaction-with-the-undo-system","title":"Interaction with the Undo system","text":"<p>In some cases when you undo an operation Blender might re-create certain data, instead of going back to a stored version still in memory. This might cause existing references to the original data to become invalid. This can be especially noticeable when working interactively in the Python Console. </p> <p>For example, with a cube object as active object in the 3D viewport:</p> <pre><code># The Cube is the active object\n&gt;&gt;&gt; bpy.context.active_object\nbpy.data.objects['Cube']\n\n# Save a reference to it\n&gt;&gt;&gt; o = bpy.context.active_object\n\n# &lt;Grab the object in the 3D viewport and move it somewhere else&gt;\n\n# Object reference still valid\n&gt;&gt;&gt; o\nbpy.data.objects['Cube']\n\n# &lt;Undo the object translation in the 3D viewport&gt;\n\n# Uh oh, object reference has now become invalid\n&gt;&gt;&gt; o\n&lt;bpy_struct, Object invalid&gt;\n\n# Reason: object referenced under name 'Cube' has changed\n&gt;&gt;&gt; bpy.data.objects['Cube'] == o\nFalse\n\n&gt;&gt;&gt; id(o)\n140543077302976\n\n&gt;&gt;&gt; id(bpy.data.objects['Cube'])\n140543077308608\n\n# Will need to reacquire the active object, or consistently use bpy.data.objects['Cube'] \n&gt;&gt;&gt; o = bpy.context.active_object\n&gt;&gt;&gt; o\nbpy.data.objects['Cube']\n</code></pre>"},{"location":"api/bpy_data_and_friends/","title":"A note on <code>bpy.data</code>, <code>bpy.data.objects</code>, ...","text":"<p>We have been using <code>bpy.data.objects</code> in most examples above to access objects in the scene. This is actually not completely clean, as <code>bpy.data.objects</code> holds all objects in the Blender file. Usually, the distinction doesn't matter as you only have one scene, but a Blender file can hold multiple scenes, each with their own set of objects:</p> <pre><code># A file with two scenes, each with their own set of objects\n&gt;&gt;&gt; bpy.data.scenes.values()\n[bpy.data.scenes['Scene'], bpy.data.scenes['Scene.001']]\n\n# Current scene\n&gt;&gt;&gt; bpy.context.scene\nbpy.data.scenes['Scene']\n\n# And its objects\n&gt;&gt;&gt; bpy.context.scene.objects.values()\n[bpy.data.objects['Bottom cube'], bpy.data.objects['Top Cube']]\n\n# &lt;Select different scene&gt;\n\n# Different current scene\n&gt;&gt;&gt; bpy.context.scene\nbpy.data.scenes['Scene.001']\n\n# And its objects\n&gt;&gt;&gt; bpy.context.scene.objects.values()\n[bpy.data.objects['Bottom cube.001'], bpy.data.objects['Top Cube.001']]\n\n# All objects in the *file*\n&gt;&gt;&gt; bpy.data.objects.values()\n[bpy.data.objects['Bottom cube'], bpy.data.objects['Bottom cube.001'], \nbpy.data.objects['Top Cube'], bpy.data.objects['Top Cube.001']]\n</code></pre> <p>Although objects can also be shared between scenes:</p> <pre><code># Two scenes\n&gt;&gt;&gt; bpy.data.scenes.values()\n[bpy.data.scenes['Scene'], bpy.data.scenes['Scene.001']]\n\n# First scene, cubes are local to scene, torus is shared between scenes\n&gt;&gt;&gt; bpy.context.scene\nbpy.data.scenes['Scene']\n\n&gt;&gt;&gt; bpy.context.scene.objects.values()\n[bpy.data.objects['Torus'], bpy.data.objects['Bottom cube'], \nbpy.data.objects['Top Cube']]\n\n# Second scene, different cubes, torus is shared\n&gt;&gt;&gt; bpy.context.scene\nbpy.data.scenes['Scene.001']\n\n&gt;&gt;&gt; bpy.context.scene.objects.values()\n[bpy.data.objects['Bottom cube.001'], bpy.data.objects['Top Cube.001'], \nbpy.data.objects['Torus']]\n</code></pre> <p>The point here is that <code>bpy.data.objects</code>, and every other attribute under <code>bpy.data</code>, holds values of the complete Blender file. Per-scene values are available through attributes of a <code>Scene</code> object, e.g. <code>bpy.context.scene.objects</code>. For certain use cases this distinction matters.</p>"},{"location":"api/custom_properties/","title":"Custom properties","text":"<p>Sometimes it can be useful to be able to control certain values that you use in a script from the UI. The most flexible, but also most complex, approach would be write an add-on. This allows creation of a nice UI that fully integrates in Blender, but that approach can be quite a bit of work to create.</p> <p>Fortunately, in quite a few cases there's a simpler alternative if all you need to control are simple Python values, like an <code>int</code>, <code>float</code>, <code>string</code> or <code>list</code>. From Python you can set custom properties on pretty much any Blender Python data block (see here for more details) and then access those values from the UI:</p> <pre><code>&gt;&gt;&gt; o\nbpy.data.objects['Cube']\n\n&gt;&gt;&gt; o['My prop'] = 123.4\n&gt;&gt;&gt; o['My 2nd prop'] = (1, 1, 0.5)\n</code></pre> <p></p> <p>This actually works both ways: adding or editing a value from the UI will update the value(s) available through Python. For example, after adding a string value named <code>A string value</code> through the UI:</p> <p></p> <pre><code># New property\n&gt;&gt;&gt; o['A string value']\n'hello!'\n\n# Updated value\n&gt;&gt;&gt; o['My prop']\n999.0\n\n&gt;&gt;&gt; list(o.keys())\n['My prop', 'My 2nd prop', 'A string value']\n</code></pre> <p>One difference between creating properties through the UI versus through Python is how the limits on the property are initialized. When using the UI the default allowable range for ints and floats is 0 to 1 annoyingly (see image below), while for a property created through Python they are initialized to -inf to inf.</p> <p></p> <p>Custom properties are saved to the blend file, and will get copied along when duplicating an object having them set.</p> <p>You can then use these values in a script, for example to control a number of objects to create, set a 3D coordinate, or to give certain objects specific roles. You can even animate the values of custom properties.</p> <p>See this documentation page for a bit more detail.</p>"},{"location":"api/data_block_users_and_gc/","title":"Data-block users and garbage collection","text":"<p>Blender uses a system based on reference-counting to decide when data-blocks have become unused and can get purged. In the short video below we show some of the details of this scheme:</p> <p>The video shows the <code>Orphan Data</code> outliner mode, but there are several modes that can be used to get detailed insight into the current state of Blender internals:</p> <ul> <li>The <code>Blender File</code> mode gives a high-level overview of a file's contents, including some of the more implicit data block types, such as Workspaces.</li> </ul> <ul> <li>The <code>Data API</code> mode provides an even more detailed view. It is actually a great way to inspect all the gory details of Blender's internal data structures. It will show all data-blocks by type and their attributes. Some attributes can be even be edited in this outliner mode.</li> </ul> <ul> <li>The <code>Orphan Data</code> mode shows data blocks that do not have any users and which will not be saved (unless they are marked to have a fake user). Some of the data-blocks you see here might not have been created by you, but are used by Blender internally, for example the Brushes.</li> </ul> <p>Although the video only focused on materials, the way data-block lifetime is managed using the user counts is general to all types of data-blocks in Blender. But there are subtle differences in whether a data-block is really deleted or just has a link to it removed:</p> <ul> <li>Whenever the term \"unlink\" is used it means that a link to that data-block is removed and its user count decreased, but the data-block itself will still be in memory. An example of this is clicking the <code>\u2715</code> next to a mesh's material in the Material Properties.</li> </ul> <ul> <li>If the UI uses the term \"delete\" it means the data-block is deleted immediately from memory. Any data-blocks linked from the deleted data-block will have their user count decreased. An example of this is deleting a Camera object in the 3D view: the Camera object's data-block is deleted from memory, but the Camera object data data-block (containing the actual camera settings) is still in memory, which you can check in the Orphan Data mode of the outliner.</li> </ul>"},{"location":"api/data_block_users_and_gc/#python-api-support","title":"Python API support","text":"<p>The usage count of data-blocks can also be queried from Python:</p> <pre><code># Two cube meshes using the same material\n&gt;&gt;&gt; bpy.context.scene.objects.values()\n[bpy.data.objects['Cube'], bpy.data.objects['Cube.001']]\n\n&gt;&gt;&gt; bpy.data.materials['Material'].users\n2\n\n# Add a new material, set one of the cubes to use it\n&gt;&gt;&gt; bpy.data.materials['Material'].users\n1\n\n&gt;&gt;&gt; bpy.data.materials['Material.001'].users\n1\n\n# &lt;Delete Cube.001 object in the UI&gt;\n\n# Hmmm, still has a user?\n&gt;&gt;&gt; bpy.data.materials['Material.001'].users\n1\n\n# The reason is we deleted the Cube.001 *object*, but\n# the Cube.001 *mesh* is still alive (as its usage count\n# was merely decremented) and it still references the material\n&gt;&gt;&gt; bpy.data.objects['Cube.001']\nTraceback (most recent call last):\n  File \"&lt;blender_console&gt;\", line 1, in &lt;module&gt;\nKeyError: 'bpy_prop_collection[key]: key \"Cube.001\" not found'\n\n&gt;&gt;&gt; bpy.data.meshes['Cube.001']\nbpy.data.meshes['Cube.001']\n\n&gt;&gt;&gt; bpy.data.meshes['Cube.001'].users\n0\n\n&gt;&gt;&gt; bpy.data.meshes['Cube.001'].materials.values()\n[bpy.data.materials['Material']]\n</code></pre> <p>The <code>use_fake_user</code> attribute of a data block controls whether a Fake user is set, similar to the checkbox in the UI.</p> <p>Really deleting objects through Python</p> <p>In most cases you probably don't want to manually delete data blocks from a file and only use the normal UI operations for that. But it is possible for cases that need it. Truly purging a data block from Python can be done with the relevant <code>remove()</code> method, e.g. </p> <pre><code>&gt;&gt;&gt; bpy.context.scene.objects.values()\n[bpy.data.objects['Cube']]\n\n&gt;&gt;&gt; o = bpy.context.active_object\n&gt;&gt;&gt; o\nbpy.data.objects['Cube']\n\n&gt;&gt;&gt; m = o.data\n&gt;&gt;&gt; m\nbpy.data.meshes['Cube']\n\n# Remove the Mesh data-block from the file\n&gt;&gt;&gt; bpy.data.meshes.remove(m)\n&gt;&gt;&gt; bpy.data.meshes.values()\n[]\n\n&gt;&gt;&gt; bpy.data.objects.values()\n[]\n</code></pre> <p>Note that in the case of deleting object data (in this case a Mesh) any Objects referencing that object data also get removed!</p> <p>A second thing to note is the above code does not actually update the current Blender file on disk. That only happens on an explicit save action (e.g. through the <code>File</code> menu or using the relevant operator from Python). </p>"},{"location":"api/materials/","title":"Materials","text":"<p>As shown in one of the introductory exercises for the Python API it is possible to use Python to create a node-based shader. In most cases using the node-based editor in the UI is the preferred option due to its interactivity, but for certain cases it can be interesting to use Python. </p> <p>The general workflow for this is to create the necessary shader nodes, connect them through links as needed and then set the material on the relevant mesh.</p> <pre><code># Create a new material\nmat = bpy.data.materials.new(\"my material\")\n\n# Enable shader nodes on the material\nmat.use_nodes = True\n\n# Remove the default nodes\nnodes = mat.node_tree.nodes\nnodes.clear()\n\n# Add a Principled BSDF shader node and set its base color\nshader = nodes.new(type='ShaderNodeBsdfPrincipled')\nshader.location = 0, 300\nshader.inputs['Base Color'].default_value = (1,0,0,1)\n\n# Add a Material Output node\nnode_output = nodes.new(type='ShaderNodeOutputMaterial')\nnode_output.location = 400, 300\n\n# Add a link between the nodes\nlinks = mat.node_tree.links\nlinks.new(shader.outputs['BSDF'], node_output.inputs['Surface'])\n\n# Add material to the mesh's material slots\nmesh.materials.append(mat)\n</code></pre> <p>A node's inputs and outputs can be referenced by name. This can then be used to set values on inputs, or connect outputs to inputs, as shown. For example, for the Principled BSDF node used above:</p> <pre><code>&gt;&gt;&gt; sorted(shader.inputs.keys())\n['Alpha', 'Anisotropic', 'Anisotropic Rotation', 'Base Color', \n'Coat IOR', 'Coat Normal', 'Coat Roughness', 'Coat Tint', 'Coat Weight', \n'Emission Color', 'Emission Strength', 'IOR', 'Metallic', 'Normal', \n'Roughness', 'Sheen Roughness', 'Sheen Tint', 'Sheen Weight', \n'Specular IOR Level', 'Specular Tint', 'Subsurface Anisotropy', \n'Subsurface IOR', 'Subsurface Radius', 'Subsurface Scale', \n'Subsurface Weight', 'Tangent', 'Transmission Weight', 'Weight']\n\n&gt;&gt;&gt; shader.outputs.keys()\n['BSDF']\n</code></pre> <p>The <code>location</code> attributes set above are not strictly needed if you're not going to work on the shader network in the Shader Editor in the UI. But they help to make the node network layout somewhat visually pleasing.</p>"},{"location":"api/materials/#material-slots","title":"Material slots","text":"<p>The last line in the Python code above adds the created material to the mesh's material slots. An object can have multiple materials assigned to it and each assigned material uses a so-called material slot. </p> <p>Each polygon in a mesh can only use a single material, by specifying the material index (i.e. slot) to use for that polygon. This allows different parts of a mesh to use different materials/shaders. </p> <p>By default all faces in a mesh will reference material slot 0. But here's an example of a cube mesh that uses 3 different materials:</p> <p></p> <p>Inspecting the underlying material data:</p> <pre><code># Get the mesh, as the material is linked to the mesh by default\n&gt;&gt;&gt; o = bpy.data.objects['Cube']\n&gt;&gt;&gt; m = o.data\n\n# The material slots used\n&gt;&gt;&gt; list(m.materials)\n[bpy.data.materials['red'], bpy.data.materials['black-white checkered'], \nbpy.data.materials['voronoi']]\n\n# Polygon -&gt; slot index\n&gt;&gt;&gt; m.polygons[0].material_index\n2\n&gt;&gt;&gt; m.polygons[1].material_index\n0\n&gt;&gt;&gt; m.polygons[2].material_index\n0\n&gt;&gt;&gt; m.polygons[3].material_index\n0\n&gt;&gt;&gt; m.polygons[4].material_index\n1\n&gt;&gt;&gt; m.polygons[5].material_index\n0\n</code></pre> <p>Material indices can be set per polygon, or set as an array in one go:</p> <pre><code># Material slot index for a single polygon \nm.polygons[0].material_index = 0\n\n# Set all polygon material indices\nface_materials = [0, 1, 2, 2, 1, 0]\nm.polygons.foreach_set('material_index', face_materials)\n# Force an update of the mesh, needed in this case\nm.update()\n</code></pre>"},{"location":"api/meshes/","title":"Meshes","text":"<p>One of the more common scene data types to work with from Python are 3D meshes. Meshes in Blender can contain polygons of an arbitrary number of vertices (so-called N-gons), can contain wire edges and support extra layers of data, such as vertex colors and UV coordinates. </p> <p>We go into a fair amount of detail on how to create and access mesh data, in several ways. As usual, the Blender API docs on the <code>Mesh</code> type contain many more details, but we feel the discussion below is a good summary to get you started for many use cases.</p>"},{"location":"api/meshes/#creating-a-mesh-high-level","title":"Creating a mesh (high-level)","text":"<p>As shown earlier the <code>Mesh.from_pydata(vertices, edges, faces)</code> method allows a simple and high-level way of creating a mesh. This method doesn't offer full control over the created mesh and isn't very fast for large meshes, but it can be good enough in a lot of cases. </p> <p>It takes three lists of values, or actually, any Python iterable that matches the expected form:</p> <ul> <li>vertices: a sequence of float triples, e.g. <code>[(1.0, 2.0, 3.0), (4, 5, 6), ...]</code></li> <li>edges: a sequence of integer pairs (vertex indices), that define edges by. If <code>[]</code> is passed then edges are inferred from polygons</li> <li>faces: a sequence of one or more polygons, each defined as a sequence of 3 or more vertex indices. E.g. <code>[(0, 1, 2), (1, 2, 3, 4), ...]</code></li> </ul> <p>Info</p> <p>The choice of how the mesh data is passed might incur an overhead in memory usage and processing time,  especially when regular Python data structures, like lists, are used. An alternative would be to pass NumPy arrays.</p> <p>For the examples below we assume that no explicit list of edges is passed. Edges will then  be created implicitly based on the polygons specified, which is usually what is preferred. We discuss explicitly specifying edges below. </p> <p>An example of creating a simple mesh:</p> <pre><code># Create a mesh consisting of 3 polygons using 6 vertices\n\nvertices = [\n    (0, 0, 0),      (2,  0,  0),    (2,  2,  0.2),    \n    (0,  2,  0.2),  (1, 3, 1),      (1, -1, -1),    \n]\n\npolygons = [\n    (0, 1, 2, 3),   # Quad\n    (4, 3, 2),      # Triangle\n    (0, 5, 1)       # Triangle\n]\n\nm = bpy.data.meshes.new(name='my mesh')\nm.from_pydata(vertices, [], polygons)\n</code></pre> <p>At this point we have created a new <code>Mesh</code> object, which corresponds to Object Data of type Mesh. Object Data cannot be directly added to a scene, but needs to be referenced by a 3D Object:</p> <pre><code># Create an Object referencing the Mesh data\no = bpy.data.objects.new(name='my mesh', object_data=m)\n\n# Add the Object to the scene\nbpy.context.scene.collection.objects.link(o)\n</code></pre> <p>The resulting mesh and outliner entry looks like this:</p> <p></p>"},{"location":"api/meshes/#careful-invalid-data","title":"Careful: invalid data","text":"<p>Note that it is possible to set up a mesh with invalid/inconsistent data when setting the underlying arrays manually, as is the case here. This can cause weird behaviour or even crashes. </p> <p>For example:</p> <pre><code># 3 vertices\nvertices = [ (0, 0, 0), (1,  1, 1), (-1, 2, -1) ]\n\n# Invalid vertex index 3 used!\npolygons = [ (0, 1, 2, 3) ]   \n\nm = bpy.data.meshes.new(name='my invalid mesh')\nm.from_pydata(vertices, [], polygons)\n\no = bpy.data.objects.new(name='my invalid mesh', object_data=m)\nbpy.context.scene.collection.objects.link(o)\n</code></pre> <p>When executing the above code a new mesh is added to the scene, but it will show as a triangle in the 3D viewport, instead of a quad. And even though that doesn't appear to be unreasonable behaviour in this case Blender will crash if we subsequently enter edit mode on the mesh!</p> <p>So the lesson here is to be careful when specifying geometry using these low-level API calls. This actually applies to all parts of the Blender Python API in general.</p> <p>In this case, to make sure a created mesh has valid data we can use the <code>validate()</code> method on a <code>Mesh</code>. This will check the mesh data and remove any invalid values, e.g. by deleting the polygon using non-existent vertex index 3 above. This might not result in a mesh that  matches what you want based on the data, but at least you can detect this situation and handle it without Blender crashing.</p> <p>The <code>validate()</code> method has two issues to be aware of:</p> <ul> <li>The method returns <code>True</code> in case the mesh does not validate, i.e. when it has issues. More specifically, it returns <code>True</code> when changes were made to the mesh data to remove invalid values.</li> <li>It will only report on the specific issues found when called with <code>validate(verbose=True)</code> and then will only output to the console.</li> </ul> <p>But it is still a good idea to always validate a mesh when creating it manually:</p> <pre><code>...\nm = bpy.data.meshes.new(name='my invalid mesh')\nm.from_pydata(vertices, [], polygons)\n\nif m.validate(verbose=True):\n    print('Mesh had issues and has been altered! See console output for details')\n</code></pre> <p>In the example of the invalid mesh data above this results in these message being  printed in the console output:</p> <pre><code>ERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:339 BKE_mesh_validate_arrays:   Edge 0: v2 index out of range, 3\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:339 BKE_mesh_validate_arrays:   Edge 3: v2 index out of range, 3\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:600 BKE_mesh_validate_arrays:   Loop 3 has invalid vert reference (3)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:790 BKE_mesh_validate_arrays:   Loop 0 is unused.\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:790 BKE_mesh_validate_arrays:   Loop 1 is unused.\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:790 BKE_mesh_validate_arrays:   Loop 2 is unused.\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:790 BKE_mesh_validate_arrays:   Loop 3 is unused.\n</code></pre> <p>After <code>validate()</code> returns we can see in this case that invalid data was indeed removed: <pre><code>&gt;&gt;&gt; vertices = [ (0, 0, 0), (1,  1, 1), (-1, 2, -1) ]\n&gt;&gt;&gt; polygons = [ (0, 1, 2, 3) ]   \n&gt;&gt;&gt; m = bpy.data.meshes.new(name='my invalid mesh')\n&gt;&gt;&gt; m.from_pydata(vertices, [], polygons)\n\n&gt;&gt;&gt; len(m.polygons)\n1\n&gt;&gt;&gt; len(m.edges)\n4\n&gt;&gt;&gt; len(m.vertices)\n3\n\n&gt;&gt;&gt; m.validate()\nTrue\n\n&gt;&gt;&gt; len(m.polygons)\n0\n&gt;&gt;&gt; len(m.edges)\n2\n&gt;&gt;&gt; len(m.vertices)\n3\n</code></pre></p>"},{"location":"api/meshes/#creating-a-mesh-low-level","title":"Creating a mesh (low-level)","text":"<p>A second, and more flexible and performant, way of creating a mesh is using low-level calls for setting the necessary data arrays directly on a <code>Mesh</code> object. This is especially useful in combination with NumPy arrays, as this allows the creation of large meshes with relatively high performance and low memory overhead.</p> <p>Meshes in Blender are stored using 4 arrays, as attributes of the <code>bpy.types.Mesh</code> type:</p> <ul> <li><code>vertices</code>: vertex locations, each specified by 3 floats</li> </ul> <ul> <li><code>loops</code>: contains the vertex indices used for defining polygons of a mesh, each polygon as a sequence of indices in the <code>vertices</code> array</li> </ul> <ul> <li><code>polygons</code>: defines the start index of each polygon as an index in <code>loops</code>, plus the length of each polygon in number of vertices</li> </ul> <ul> <li><code>edges</code>: defines the edges of the mesh, using two vertex indices per edge</li> </ul> <p>So to create a mesh at this level we need to set up the necessary values for these arrays. Here, we create the same mesh as in the previous section, using NumPy arrays for storing the data.</p> <pre><code># Need to explicitly import numpy\nimport numpy\n\n# Vertices (8): x1 y1 z1 x2 y2 z2 ...\nvertices = numpy.array([\n    0, 0, 0,    2,  0,  0,    2,  2,  0.2,    0,  2,  0.2,\n    1, 3, 1,    1, -1, -1,    0, -2, -1,      2, -2, -1\n], dtype=numpy.float32)\n\n#\n# Polygons, defined in loops\n#\n\n# List of vertex indices of all loops combined\nvertex_index = numpy.array([\n    0, 1, 2, 3,                             # Quad\n    4, 3, 2,                                # Triangle\n    0, 5, 1                                 # Triangle\n], dtype=numpy.int32)\n\n# For each polygon the start of its indices in vertex_index\nloop_start = numpy.array([\n    0, 4, 7\n], dtype=numpy.int32)\n\n# Length of each polygon in number of vertices\nloop_total = numpy.array([\n    4, 3, 3\n], dtype=numpy.int32)\n</code></pre> <p>We additionally also specify UV coordinates (also known as texture coordinates) and vertex colors. This is something that is not possible with the high-level <code>from_pydata()</code> API shown above. Note that we need to specify these values per vertex per polygon loop.</p> <pre><code># Texture coordinates per vertex per polygon loop\nuv_coordinates = numpy.array([\n    0,   0,    1, 0,      1, 1,    0, 1,    # Quad   \n    0.5, 1,    0, 0,      1, 0,             # Triangle\n    0,   1,    0.5, 0,    1, 1              # Triangle\n], dtype=numpy.float32)\n\n# Vertex color (RGBA) per vertex per polygon loop\nvertex_colors = numpy.array([\n    1, 0, 0, 1,   1, 0, 0, 1,   1, 0, 0, 1,   1, 0, 0, 1,\n    0, 1, 0, 1,   0, 1, 0, 1,   0, 1, 0, 1,\n    1, 0, 0, 1,   0, 1, 0, 1,   0, 0, 1, 1,\n], dtype=numpy.float32)\n</code></pre> <p>Next, we create a new mesh using the above arrays:</p> <pre><code>num_vertices = vertices.shape[0] // 3\nnum_vertex_indices = vertex_index.shape[0]\nnum_loops = loop_start.shape[0]\n\nm = bpy.data.meshes.new(name='my detailed mesh')\n\n# Vertices\nm.vertices.add(num_vertices)\nm.vertices.foreach_set('co', vertices)\n\n# Polygons\nm.loops.add(num_vertex_indices)\nm.loops.foreach_set('vertex_index', vertex_index)\n\nm.polygons.add(num_loops)\nm.polygons.foreach_set('loop_start', loop_start)\nm.polygons.foreach_set('loop_total', loop_total)\n\n# Create UV coordinate layer and set values\nuv_layer = m.uv_layers.new(name='default')\nuv_layer.data.foreach_set('uv', uv_coordinates)\n\n# Create vertex color layer and set values\nvcol_layer = m.color_attributes.new(name='vcol', type='FLOAT_COLOR', domain='CORNER')\nvcol_layer.data.foreach_set('color', vertex_colors)\n\n# Done, update mesh object\nm.update()\n\n# Validate mesh\nif m.validate(verbose=True):\n    print('Mesh data did not validate!')\n\n# Create an object referencing the mesh data\no = bpy.data.objects.new(name='my detailed mesh', object_data=m)\n\n# Add the object to the scene\nbpy.context.scene.collection.objects.link(o)    \n</code></pre> <p>Multi-dimensional arrays</p> <p>Passing a multi-dimensional NumPy array directly to <code>foreach_set()</code> will not work:</p> <pre><code>&gt;&gt;&gt; vertices = numpy.array([\n...     (0, 0, 0),    (2,  0,  0),    (2,  2,  0.2),    (0,  2,  0.2),\n...     (1, 3, 1),    (1, -1, -1),    (0, -2, -1),      (2, -2, -1)\n... ], 'float32')\n&gt;&gt;&gt; vertices.shape\n(8, 3)\n\n&gt;&gt;&gt; m = bpy.data.meshes.new(name='my detailed mesh')\n&gt;&gt;&gt; m.vertices.add(8)\n&gt;&gt;&gt; m.vertices.foreach_set('co', vertices)\nTraceback (most recent call last):\n  File \"&lt;blender_console&gt;\", line 1, in &lt;module&gt;\nRuntimeError: internal error setting the array\n</code></pre> <p>However, passing a flattened array does work:</p> <pre><code>&gt;&gt;&gt; m.vertices.foreach_set('co', vertices.flatten())\n&gt;&gt;&gt; [v.co for v in m.vertices]\n[Vector((0.0, 0.0, 0.0)), Vector((2.0, 0.0, 0.0)), Vector((2.0, 2.0, 0.20000000298023224)), Vector((0.0, 2.0, 0.20000000298023224)), Vector((1.0, 3.0, 1.0)), Vector((1.0, -1.0, -1.0)), Vector((0.0, -2.0, -1.0)), Vector((2.0, -2.0, -1.0))]\n</code></pre>"},{"location":"api/meshes/#specifying-edges-when-creating-a-mesh","title":"Specifying edges when creating a mesh","text":"<p>In most cases we want to create a mesh consisting of only polygons and in that case don't need to specify edges. For certain mesh objects it can be of interest to also be able to specify edges explicitly, or even to create a mesh that consists only of vertices and edges between them. Edges can be used to add line segments that are not part of polygons.</p> <p>We build upon the example mesh we created above by adding a set of 3 edges:</p> <pre><code># Create a mesh consisting of 3 polygons using 8 vertices, with 3 extra edges\n# that are not part of the polygons\n\nvertices = [\n    (0, 0, 0),    (2,  0,  0),    (2,  2,  0.2),    (0,  2,  0.2),\n    (1, 3, 1),    (1, -1, -1),    (0, -2, -1),      (2, -2, -1)\n]\n\nedges = [\n    (5, 6), (6, 7), (5, 7)\n]\n\npolygons = [\n    (0, 1, 2, 3),   # Quad\n    (4, 3, 2),      # Triangle\n    (0, 5, 1)       # Triangle\n]\n\nm = bpy.data.meshes.new(name='my mesh with edges')\nm.from_pydata(vertices, edges, polygons)\n\no = bpy.data.objects.new(name='my mesh with edges', object_data=m)\nbpy.context.scene.collection.objects.link(o)\n</code></pre> <p>The resulting mesh and outliner entry looks like this:</p> <p></p> <p>Note that even though we specified only 3 edges explicitly the polygons in the mesh implicitly define 8 more. These are the edges making up those polygons, with shared edges being present only once. In total this results in 11 edges in the mesh:</p> <pre><code>&gt;&gt;&gt; len(m.edges)\n11\n</code></pre> <p>For the second, low-level, method of mesh creation edges are handled slightly different. Edges can be set explicitly by using <code>Mesh.edges</code>:</p> <pre><code># Vertices (8): x1 y1 z1 x2 y2 z2 ...\nvertices = numpy.array([\n    0, 0, 0,    2,  0,  0,    2,  2,  0.2,    0,  2,  0.2,\n    1, 3, 1,    1, -1, -1,    0, -2, -1,      2, -2, -1\n], dtype=numpy.float32)\n\n# Extra edges (3) not defined implicitly by polygons\nedges = numpy.array([\n    5, 6,    6, 7,    5, 7\n], dtype=numpy.int32)\n\n#\n# Polygons, defined in loops\n#\n\n# List of vertex indices of all loops combined\nvertex_index = numpy.array([\n    0, 1, 2, 3,                             # Quad\n    4, 3, 2,                                # Triangle\n    0, 5, 1                                 # Triangle\n], dtype=numpy.int32)\n\n# For each polygon the start of its indices in vertex_index\nloop_start = numpy.array([\n    0, 4, 7\n], dtype=numpy.int32)\n\n# Length of each polygon in number of vertices\nloop_total = numpy.array([\n    4, 3, 3\n], dtype=numpy.int32)\n\nnum_vertices = vertices.shape[0] // 3\nnum_edges = edges.shape[0] // 2\nnum_vertex_indices = vertex_index.shape[0]\nnum_loops = loop_start.shape[0]\n\nm = bpy.data.meshes.new(name='detailed mesh with edges')\n\n# Vertices\nm.vertices.add(num_vertices)\nm.vertices.foreach_set('co', vertices)\n\n# Edges\nm.edges.add(num_edges)\nm.edges.foreach_set('vertices', edges)\n\n# Polygons\nm.loops.add(num_vertex_indices)\nm.loops.foreach_set('vertex_index', vertex_index)\n\nm.polygons.add(num_loops)\nm.polygons.foreach_set('loop_start', loop_start)\nm.polygons.foreach_set('loop_total', loop_total)\n\n# Done, update mesh object\nm.update()\n\n# Validate mesh\nif m.validate(verbose=True):\n    print('Mesh data did not validate!')\n</code></pre> <p>Here, we only specify the extra edges and not the polygon edges. But when we try to validate the  mesh errors will be reported:</p> <pre><code>ERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 0 needs missing edge (0, 1)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 0 needs missing edge (1, 2)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 0 needs missing edge (2, 3)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 0 needs missing edge (3, 0)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 1 needs missing edge (4, 3)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 1 needs missing edge (3, 2)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 1 needs missing edge (2, 4)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 2 needs missing edge (0, 5)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 2 needs missing edge (5, 1)\nERROR (bke.mesh): source/blender/blenkernel/intern/mesh_validate.cc:627 BKE_mesh_validate_arrays:   Poly 2 needs missing edge (1, 0)\n</code></pre> <p>So the polygon edges (which we did not specify) are being reported. In this case the <code>validate()</code> method will correct this and add the missing edges. But having errors reported for regular polygon edges makes it harder to detect any other issues with the mesh data. Hence, the <code>Mesh.update()</code> method provides the option <code>calc_edges</code>. By default this option is <code>False</code>,  but when set to <code>True</code> all edges in the mesh will be recalculated to be consistent with the available vertex indices, polygons and extra edges set.</p> <pre><code>...\n\n# Done, update mesh object and recalculate edges\nm.update(calc_edges=True)\n</code></pre> <p>Validation now succeeds:</p> <pre><code>&gt;&gt;&gt; m.validate(verbose=True)\nFalse\n</code></pre>"},{"location":"api/meshes/#accessing-mesh-data-object-mode","title":"Accessing mesh data (object mode)","text":"<p>Inspecting or using mesh data is straightforward. Here we use one of the meshes created with the low-level methods above and retrieve some of its data. Note that Blender provides a few values derived from the original arrays, such as <code>loop_indices</code> and <code>vertices</code> per polygon, which can be useful for certain operations.</p> <pre><code>m = bpy.data.meshes['my detailed mesh']\n\nlen(m.vertices)            =&gt; 8                            \nlen(m.polygons)            =&gt; 3\n# 2 triangles + 1 quad = 2*3 + 1*4 = 10\nlen(m.loops)               =&gt; 10\n# 8 implicit edges (for 2 triangles and 1 quad), shared edges only listed once\nlen(m.edges)               =&gt; 8                \n\nm.vertices[7].co           =&gt; Vector((2.0, -2.0, -1.0))         # Coordinate\nm.vertices[7].normal       =&gt; Vector((0.6.., -0.6.., -0.3..))   # Normal\nm.vertices[7].select       =&gt; True              # Selected (edit mode)\n\nm.polygons[2].index        =&gt; 2                 # Useful in 'for p in m.polygons'\nm.polygons[2].loop_start   =&gt; 7                 # First index in loops array\nm.polygons[2].loop_total   =&gt; 3                 # Number of vertices in loop\nm.polygons[2].loop_indices =&gt; [7, 8, 9]         # Indices in m.loops, reported as range(7, 10)\nm.loops[7].vertex_index    =&gt; 0\nm.loops[8].vertex_index    =&gt; 5\nm.loops[9].vertex_index    =&gt; 1\nm.polygons[2].vertices     =&gt; [0, 5, 1]         # Actual vertex indices\nm.polygons[2].select       =&gt; True              # Selected (edit mode)\nm.polygons[2].use_smooth   =&gt; False             # Smooth shading enabled\n\n# These are automatically computed\nm.polygons[2].area         =&gt; 1.4142135381698608\nm.polygons[2].normal       =&gt; Vector((0.0, -0.707..., 0.707...))   \nm.polygons[2].center       =&gt; Vector((1.0, -0.333..., -0.333...))  \n\nm.edges[0].vertices        =&gt; [2, 3]            # (bpy_prop_array)\n</code></pre> <p>Starting with Blender 3.1 there's new attributes <code>vertex_normals</code> and <code>polygon_normals</code> on <code>Mesh</code> objects to access normal vectors directly from the underlying array they're stored in:</p> <pre><code># Access per vertex, as above\n&gt;&gt;&gt; m.vertices[0].normal\nVector((-0.5773503184318542, -0.5773503184318542, -0.5773503184318542))\n\n# Access from array of vertex normals\n&gt;&gt;&gt; m.vertex_normals[0].vector\nVector((-0.5773503184318542, -0.5773503184318542, -0.5773503184318542))\n\n# Access per polygon, as above\n&gt;&gt;&gt; m.polygons[0].normal\nVector((-1.0, -0.0, 0.0))\n\n# Access from array of polygon normals\n&gt;&gt;&gt; m.polygon_normals[0].vector\nVector((-1.0, 0.0, 0.0))\n</code></pre> <p>The array-based normal access is more efficient that accessing the <code>normal</code> value of a <code>MeshVertex</code>. Note that <code>vertex_normals</code>  and <code>polygon_normals</code> only provide read-only access.</p>"},{"location":"api/meshes/#vertex-colors","title":"Vertex colors","text":"<p>A mesh can have multiple sets of vertex colors. Each set has a name and for each vertex the associated color (but see below on how this is specified in a per-face manner). By default meshes created in Blender do not have a vertex color layer, so it needs to be created explicitly.</p> <pre><code>&gt;&gt;&gt; m\nbpy.data.meshes['Cube']\n\n&gt;&gt;&gt; type(m.color_attributes)\n&lt;class 'bpy_prop_collection'&gt;\n\n# Create a new vertex color layer\n&gt;&gt;&gt; vcol_layer = m.color_attributes.new(name='My vertex colors', type='FLOAT_COLOR', domain='CORNER')\n&gt;&gt;&gt; vcol_layer\nbpy.data.meshes['Cube'].attributes[\"My vertex colors\"]\n\n# Name shown under Object Data -&gt; Color Attributes (and Attributes)\n&gt;&gt;&gt; vcol_layer.name\n'My vertex colors'\n\n&gt;&gt;&gt; list(m.color_attributes)\n[bpy.data.meshes['Cube'].attributes[\"My vertex colors\"]]\n</code></pre> <p>Attribute <code>vertex_colors</code> deprecated\" since Blender 3.2</p> <p>In previous versions of Blender vertex colors were accessed through the Mesh attribute <code>vertex_colors</code>, but this has been deprecated since Blender 3.2 in favor of using color attributes as shown above.</p> <p>Each created vertex color layer is listed under the Mesh properties both under <code>Color Attributes</code> as well as <code>Attributes</code>:</p> <p></p> <p>There are actually even more Mesh attributes than are listed in the UI, but some of these are for internal use:</p> <pre><code>&gt;&gt;&gt; len(m.attributes)\n10\n\n&gt;&gt;&gt; list(m.attributes)\n[bpy.data.meshes['Cube'].attributes[\"position\"], bpy.data.meshes['Cube'].attributes[\".select_vert\"], \nbpy.data.meshes['Cube'].attributes[\".edge_verts\"], bpy.data.meshes['Cube'].attributes[\".select_edge\"], \nbpy.data.meshes['Cube'].attributes[\".select_poly\"], bpy.data.meshes['Cube'].attributes[\"sharp_face\"], \nbpy.data.meshes['Cube'].attributes[\".corner_vert\"], bpy.data.meshes['Cube'].attributes[\".corner_edge\"], \nbpy.data.meshes['Cube'].attributes[\"My vertex colors\"], bpy.data.meshes['Cube'].attributes[\"UVMap\"]]\n</code></pre> <p>The vertex color values themselves are accessed through the <code>data</code> member of the color attribute layer:</p> <pre><code>&gt;&gt;&gt; type(vcol_layer.data)\n&lt;class 'bpy_prop_collection'&gt;\n\n&gt;&gt;&gt; len(vcol_layer.data)\n24\n\n&gt;&gt;&gt; type(vcol_layer.data[0].color)\n&lt;class 'bpy_prop_array'&gt;\n\n&gt;&gt;&gt; list(vcol_layer.data[0].color)\n[1.0, 0.0, 0.0, 1.0]\n\n&gt;&gt;&gt; len(m.polygons)\n6\n\n&gt;&gt;&gt; len(m.vertices)\n8\n\n&gt;&gt;&gt; len(m.loops)\n24\n</code></pre> <p>One thing to notice here is that the vertex color array has 24 entries. But the Cube object only has 8 vertices and 6 polygons. The reason for the higher number of vertex colors is that Blender stores separate vertex colors per polygon. So the Cube has 6 polygons, each defined using 4 vertices, hence 6*4=24 vertex colors in total (which is the same number as the length of the loops array). This is more flexible than what most 3D file formats allow, which usually only store one color per vertex, regardless of how many faces that vertex is used in. During import Blender will duplicate those colors to set the same color for a vertex in all polygons in which it is used. </p> <p>An example of how to take advantage of the added flexibility is that we can set a random color per cube face by setting each of the 4 vertex colors of a face to the same color:</p> <pre><code># Set different RGBA values for each cube face\nfor i in range(6):\n    r = random()\n    g = random()\n    b = random()\n    for j in range(4):\n        vcol_layer.data[4*i+j].color = (r, g, b, 1)\n</code></pre> <p></p> <p>A slightly more Blender-like (and robust) way to write the above code would be to take advantage of the polygon loop indices:</p> <pre><code>for p in m.polygons:\n    r = random()\n    g = random()\n    b = random()    \n    for i in p.loop_indices:\n        vcol_layer.data[i].color = (r, g, b, 1)\n</code></pre> <p>Vertex color space changed since Blender 3.2</p> <p>In Blender 3.2 the interpretation of vertex colors values was changed. Previously, vertex color RGB values were assumed to be in sRGB color space. But from 3.2 onwards they are assumed to be in scene linear color space. Specifically, the <code>vcol_attr.data[i].color</code> attribute assumes linear values are passed, while <code>vcol_attr.data[i].color_srgb</code> can be used to set sRGB values (the latter will use automatic conversion where needed).</p> <p>When passing the wrong values, i.e. sRGB instead of linear to the <code>.color</code> attribute, the difference in color can be subtle, but noticeable. Below is the same set of values, but one passed as sRGB (left), the other as linear (right):  </p> <p>To manually convert a color value between the two color spaces use the functions from <code>mathutils.Color</code>, specifically <code>from_scene_linear_to_srgb()</code> and <code>from_srgb_to_scene_linear()</code>.    </p>"},{"location":"api/meshes/#active-set","title":"Active set","text":"<p>As noted above a mesh can have more than one layer of vertex colors. Among the sets present on a mesh there can be only one that is active. The active vertex color layer set controls, for example, which vertex colors are visible in the 3D viewport and are edited in Vertex Paint mode. </p> <p>When adding a vertex color layer through the UI (and similar for UV maps described below) the active layer is changed to the newly added layer. Also, clicking in the Vertex Color layer UI changes the active layer. Below is the UI list showing 2 vertex color layers on a mesh, of which <code>Col</code> is the active one used in vertex paint mode. </p> <p></p> <p>The camera icon right of the vertex color names controls which layer is used during rendering by default (and which is set independently of the active status). But in most cases the shader used on an object will explicitly choose a vertex color layer using an <code>Attribute</code> node and so override the setting in the UI list.</p> <p>Controlling the active vertex color (or UV map, described below) layer can be done using the <code>active_color</code> property:</p> <pre><code>&gt;&gt;&gt; list(m.color_attributes)\n[bpy.data.meshes['Cube'].attributes[\"My vertex colors\"], \nbpy.data.meshes['Cube'].attributes[\"Another layer\"]]\n\n&gt;&gt;&gt; m.color_attributes.active_color\nbpy.data.meshes['Cube'].attributes[\"My vertex colors\"]\n\n&gt;&gt;&gt; m.color_attributes.active_color = m.color_attributes[1]\n&gt;&gt;&gt; m.color_attributes.active_color\nbpy.data.meshes['Cube'].attributes[\"Another layer\"]\n</code></pre> <p>Somewhat clumsy/buggy API</p> <p>There's also <code>active_index</code> and <code>active</code> attributes, but these don't do what one would expect. See this bug report for some details.</p>"},{"location":"api/meshes/#uv-coordinates","title":"UV coordinates","text":"<p>UV coordinates follow the same setup as vertex colors, but instead store a 2-tuple of floats per vertex per polygon. Note that just like for vertex colors UV coordinates are also specified per vertex per polygon.</p> <p>Meshes created in Blender will already have a UV map called <code>UVMap</code>:</p> <pre><code>&gt;&gt;&gt; m\nbpy.data.meshes['Cube']\n\n&gt;&gt;&gt; len(m.uv_layers)\n1\n\n&gt;&gt;&gt; m.uv_layers[0].name\n'UVMap'\n</code></pre> <p>The actual UV values are once again stored under the <code>data</code> member:</p> <pre><code>&gt;&gt;&gt; uv_map = m.uv_layers[0]\n&gt;&gt;&gt; uv_map\nbpy.data.meshes['Cube'].uv_layers[\"UVMap\"]\n\n&gt;&gt;&gt; type(uv_map.data)\n&lt;class 'bpy_prop_collection'&gt;\n\n&gt;&gt;&gt; len(uv_map.data)\n24\n\n&gt;&gt;&gt; type(uv_map.data[0])\n&lt;class 'bpy.types.MeshUVLoop'&gt;\n\n&gt;&gt;&gt; uv_map.data[0].uv\nVector((0.375, 0.0))\n</code></pre> <p>In general, UV maps are either set through importing or edited within Blender using the UV Editor, although there can be valid reasons for wanting to control them through the Python API.</p>"},{"location":"api/meshes/#bmesh","title":"BMesh","text":"<p>There is another method in Blender for creating meshes and accessing their data: the so-called BMesh, which is implemented by the <code>bmesh</code> module and its <code>BMesh</code> class. BMesh is especially interesting when you want to perform more complex geometric operations on an existing mesh, or build up a mesh polygon-by-polygon instead of providing the full mesh in one go as a set of arrays as shown above. Also, a large set of high- and low-level geometric operations on BMeshes is available, such as merging vertices within a given distance, face splitting, edge collapsing or generating a convex hull. These are provided in the <code>bmesh.ops</code> and <code>bmesh.utils</code> modules. These operations would be tedious and error prone to script manually.</p> <p>In this section we only give a brief overview of BMesh and refer to the API docs for all the details.</p> <p>The differences of BMesh compared to working with the native mesh data structure we showed above:</p> <ul> <li>A BMesh holds extra data on mesh connectivity, like the neighbours of a vertex, which can be easily queried for geometric editing. The trade-off is that a BMesh will use more memory to store all this extra data, but that is usually only a limiting factor for very large meshes. </li> </ul> <ul> <li>It is somewhat slower to create a (large) mesh using a BMesh, as each mesh element (vertex, edge, polygon) takes a Python call to create, plus needs extra calls and Python values to set up.</li> </ul> <ul> <li>A BMesh cannot be used directly in a scene, it first needs to be converted (or copied back) to a <code>Mesh</code>. So mesh data is present twice in memory at some point in time, in the two different forms.</li> </ul> <p>Here's a (verbose) example of create a BMesh from scratch that holds a single triangle and edge: </p> <pre><code>import bpy, bmesh \n\nbm = bmesh.new()\n\n# Create 4 vertices\nv1 = bm.verts.new((0, 0, 0))\nv2 = bm.verts.new((1, 0, 1))\nv3 = bm.verts.new((0, 1, 1))\nv4 = bm.verts.new((1, 1, 1))\n\n# Add a triangle\nbm.faces.new((v1, v2, v3))\n\n# Add a line edge\nbm.edges.new((v3, v4))\n\n# Done setting up the BMesh, now copy geometry to a regular Mesh\nm = bpy.data.meshes.new('mesh')\nbm.to_mesh(m)\n\n# Release BMesh data, bm will no longer be usable\nbm.free()\n\n# Add regular Mesh as object\no = bpy.data.objects.new('mesh', m) \nbpy.context.scene.collection.objects.link(o)\n</code></pre> <p>A <code>BMesh</code> can also be created from an existing <code>Mesh</code>, edited and then copied back to the <code>Mesh</code>:</p> <pre><code>o = bpy.context.active_object\nm = o.data\n\n# Create a new BMesh and copy geometry from the Mesh\nbm = bmesh.new()\nbm.from_mesh(m)\n\n# Edit some geometry\nbm.verts.ensure_lookup_table()\nbm.verts[4].co.x += 3.14\n\nbm.faces.ensure_lookup_table()\nbm.faces.remove(bm.faces[0])\n\n# Copy back to Mesh\nbm.to_mesh(m)\nbm.free()\n</code></pre> <p>If a <code>Mesh</code> is currently in edit mode you can still create a <code>BMesh</code> from it, edit that and the copy the changes back, while keeping the <code>Mesh</code> in edit mode:</p> <pre><code>o = bpy.context.active_object\nm = o.data\nassert m.mode == 'EDIT'\n\nbm = bmesh.new()\n# Note the different call, from_edit_mesh() instead of from_mesh()\nbm.from_edit_mesh(m)\n\n# &lt;edit BMesh&gt;\n\n# Update edit-mesh of Mesh (again, different call)\nbm.update_edit_mesh(m)\nbm.free()\n</code></pre> <p>This can be useful when you're working in edit mode on a mesh and also want to run a script on it that uses BMesh, but don't want to switch in and out of edit-mode to run the script.</p> <p>Warning</p> <p>There are some things to watch out for when synchronizing <code>BMesh</code> state to a <code>Mesh</code>, see here.</p> <p>Some examples of the geometric queries that you can do on a <code>BMesh</code> (see docs for more):</p> <pre><code>bm.verts[i]                 # Sequence of mesh vertices (read-only)\nbm.edges[i]                 # Sequence of mesh edges (read-only)\nbm.faces[i]                 # Sequence of mesh faces (read-only)\n\nbm.verts[i].co              # Vertex coordinate as a mathutils.Vector\nbm.verts[i].normal          # Vertex normal\nbm.verts[i].is_boundary     # True if vertex is at the mesh boundary\nbm.verts[i].is_wire         # True if vertex is not connected to any faces\nbm.verts[i].link_edges      # Sequence of edges connected to this vertex\nbm.verts[i].link_faces      # Sequence of faces connected to this vertex\nbm.verts[i].index           # Index in bm.verts\n\nbm.edges[i].calc_length()   # Length of the edge\nbm.edges[i].is_boundary     # True if edge is boundary of a face\nbm.edges[i].is_wire         # True if edge is not connected to any faces\nbm.edges[i].is_manifold     # True if edge is manifold (used in at most 2 faces)\nv = bm.edges[i].verts[0]    # Get one vertex of this edge\nbm.edges[i].other_vert(v)   # Get the other vertex\nbm.edges[i].link_faces      # Sequence of faces connected to this edge\nbm.edges[i].index           # Index in bm.edges\n\nbm.faces[i].calc_area()     # Face area\nbm.faces[i].calc_center_median()    # Median center\nbm.faces[i].edges           # Sequence of edges defining this face\nbm.faces[i].verts           # Sequence of vertices defining this face\nbm.faces[i].normal          # Face normal\nbm.faces[i].index           # Index in bm.faces\n</code></pre> <p>Indices</p> <p>The use of indices above, both to index the sequences of vertices/edges/faces as well as retrieving <code>.index</code> values, requires up-to-date indices. During operations on a BMesh the indices (and sequences) might become incorrect and need an update first.</p> <p>To ensure the <code>.index</code> values of vertices, edges and faces are correct call the respective <code>index_update()</code> method on their sequence: <pre><code>bm.verts.index_update()\nbm.edges.index_update()\nbm.faces.index_update()\n</code></pre></p> <p>To ensure you can correctly index <code>bm.verts</code>, <code>bm.edges</code> and <code>bm.faces</code> call the respective <code>ensure_lookup_table()</code> method: <pre><code>bm.verts.ensure_lookup_table()\nbm.edges.ensure_lookup_table()\nbm.faces.ensure_lookup_table()\n</code></pre></p> <p>A Blender mesh can contain polygons with an arbitrary number of vertices. Sometimes it can be desirable to work on triangles only. You can convert all non-triangle faces in a BMesh to triangles with a call to <code>bmesh.ops.triangulate()</code>:</p> <pre><code>bm = bmesh.new()\n\nv1 = bm.verts.new((0, 0, 0))\nv2 = bm.verts.new((1, 0, 1))\nv3 = bm.verts.new((0, 1, 1))\nv4 = bm.verts.new((1, 1, 1))\n\n# Add a quad\nbm.faces.new((v1, v2, v3, v4))\n\n# Ensure indices printed are correctly\nbm.verts.index_update()\n\nfor f in bm.faces:\n    print([v.index for v in f.verts])\n\n# Force triangulation. The list of faces can optionally be a subset of the faces in the mesh.\nbmesh.ops.triangulate(bm, faces=bm.faces[:])\n\nprint('After triangulation:')\nfor f in bm.faces:\n    print([v.index for v in f.verts])\n\n# Output:\n#\n# [0, 1, 2, 3]\n# After triangulation:\n# [0, 2, 3]\n# [0, 1, 2]\n</code></pre>"},{"location":"api/modifiers/","title":"Modifiers","text":"<p>We've shown use of modifiers in earlier chapters, especially for basic mesh editing. Modifiers are a very nice way of generating scene content or altering geometry, as they provide a non-destructive way to set up a series of operations. Modifier parameters can also easily be changed to see their effect.</p> <p>In the Python API there is good support to work with modifiers. The main entry point is to add a modifier to a scene object through the <code>modifiers</code> attribute. Suppose we have a scene with a Cube and Suzanne mesh, and we want to do a boolean subtraction of the monkey head from the Cube.</p> <p></p> <p>We add a modifier to the Cube, and set its parameters:</p> <pre><code>&gt;&gt;&gt; o = bpy.data.objects['Cube']\n# Add a Boolean modifier on the object and set its parameters\n&gt;&gt;&gt; mod = o.modifiers.new(name='boolmod', type='BOOLEAN')\n&gt;&gt;&gt; mod.object = bpy.data.objects['Suzanne']\n&gt;&gt;&gt; mod.operation = 'DIFFERENCE'\n\n# At this point the modifier is all set up. We hide\n# the object we subtract to make the boolean result visible.\n&gt;&gt;&gt; bpy.data.objects['Suzanne'].hide_viewport = True\n\n&gt;&gt;&gt; list(o.modifiers)\n[bpy.data.objects['Cube'].modifiers[\"boolmod\"]]\n\n# Could remove the modifier with\n# o.modifiers.remove(o.modifiers[0])\n</code></pre> <p>In the viewport the result of the modifier is directly visible:</p> <p></p> <p>We can even continue changing the modifier parameters from Python and see the result (which is analogue to setting the value through the UI):</p> <pre><code>&gt;&gt;&gt; mod.operation = 'INTERSECT'\n</code></pre> <p></p> <p>Applying a modifier, in essence updating an object to the result of the modifier and removing the modifier itself, needs to be done using an operator:</p> <pre><code># We need to pass the name of modifier to apply\n&gt;&gt;&gt; bpy.ops.object.modifier_apply(modifier=\"boolmod\")\n{'FINISHED'}\n\n&gt;&gt;&gt; list(o.modifiers)\n[]\n</code></pre>"},{"location":"api/object_transformations/","title":"Transforms and coordinates","text":""},{"location":"api/object_transformations/#object-to-world-transform","title":"Object-to-world transform","text":"<p>The <code>matrix_world</code> attribute of an <code>Object</code> contains the object-to-world transform that places the object in the 3D scene:</p> <pre><code>&gt;&gt;&gt; o = bpy.context.active_object\n&gt;&gt;&gt; o\nbpy.data.objects['Cube']\n\n&gt;&gt;&gt; o.matrix_world\nMatrix(((1.3376139402389526, 0.0, 0.0, 0.3065159320831299),\n        (0.0, 1.3376139402389526, 0.0, 2.2441697120666504),\n        (0.0, 0.0, 1.3376139402389526, 1.2577730417251587),\n        (0.0, 0.0, 0.0, 1.0)))\n</code></pre> <p>Comparing this matrix with the values set in the Transform panel, you can see the Location value is stored in the right-most column of the matrix and the scaling along the diagonal. If there was a rotation set on this object some of these values would not be as recognizable anymore. </p> <p></p> <p>The location, rotation (in radians) and scale values can also be inspected and set separately:</p> <pre><code>&gt;&gt;&gt; o.location\nVector((0.3065159320831299, 2.2441697120666504, 1.2577730417251587))\n\n&gt;&gt;&gt; o.rotation_euler\nEuler((0.0, 0.0, 0.0), 'XYZ')\n\n&gt;&gt;&gt; o.scale\nVector((1.3376139402389526, 1.3376139402389526, 1.3376139402389526))\n\n&gt;&gt;&gt; o.location = (1, 2, 3)\n# Rotations are set in radians\n&gt;&gt;&gt; o.rotation_euler.x = radians(45)\n&gt;&gt;&gt; o.scale = (2, 1, 1)\n&gt;&gt;&gt; o.matrix_world\nMatrix(((2.0, 0.0, 0.0, 1.0),\n        (0.0, 0.7071067690849304, -0.7071067690849304, 2.0),\n        (0.0, 0.7071067690849304, 0.7071067690849304, 3.0),\n        (0.0, 0.0, 0.0, 1.0)))\n</code></pre> <p>See the section on parenting for some subtle effects on transformations in case object parenting is used.</p>"},{"location":"api/object_transformations/#geometry-coordinates","title":"Geometry coordinates","text":"<p>Mesh geometry in Blender stores vertex coordinates (and other geometric information) in object-space coordinates. But a mesh (or object in general) will usually get transformed to a specific position, scaling and orientation in the scene. </p> <p>As described above the net transform from object-space to world-space coordinates, also called the object-to-world transform, is available through <code>matrix_world</code>. In cases where you need to have access to geometric data in world-space, say vertex coordinates, you need to apply the <code>matrix_world</code> transform manually. </p> <p>For example, given the cube transformed as shown above, with only vertex 7 selected (visible in white the bottom-left of the image):</p> <p></p> <pre><code>&gt;&gt;&gt; o\nbpy.data.objects['Cube']\n\n&gt;&gt;&gt; m = o.data\n&gt;&gt;&gt; o.matrix_world\nMatrix(((1.3376139402389526, 0.0, 0.0, 0.3065159320831299),\n        (0.0, 1.3376139402389526, 0.0, 2.2441697120666504),\n        (0.0, 0.0, 1.3376139402389526, 1.2577730417251587),\n        (0.0, 0.0, 0.0, 1.0)))\n\n# The object-space coordinate of this vertex\n&gt;&gt;&gt; m.vertices[7].co\nVector((-1.0, -1.0, -1.0))\n\n# The world-space coordinate of this vertex, which matches\n# what the Transform UI shows. Note the Global display mode\n# select in the UI. If we would select Local the values shown\n# would be (-1, -1, -1).\n&gt;&gt;&gt; o.matrix_world @ m.vertices[7].co\nVector((-1.0310980081558228, 0.9065557718276978, -0.07984089851379395))\n</code></pre>"},{"location":"api/often_used_values_and_operations/","title":"Often used values and operations","text":"<p>Here, we list some frequently used parts of the API, for varying types of data.</p> <p>Access instance attribute</p> <p>Note that where values are listed as attributes of classes, such as <code>Camera.lens</code> or <code>Scene.camera</code>, you need to access the attribute on an instance of that class, not on the class itself. </p> <p>Example: to get the active camera access <code>bpy.context.scene.camera</code> or <code>bpy.data.scenes[\"myscene\"]</code> (and not <code>Scene.camera</code>).</p>"},{"location":"api/often_used_values_and_operations/#scene","title":"Scene","text":"<ul> <li>Current scene: <code>bpy.context.scene</code> (read-only)</li> </ul>"},{"location":"api/often_used_values_and_operations/#objects","title":"Objects","text":"<ul> <li>Type of an object: <code>Object.type</code> (read-only)</li> <li>Active object: <code>bpy.context.active_object</code> (read-only)</li> <li>Selected objects: <code>bpy.context.selected_objects</code> (read-only)</li> <li>Delete selected objects: <code>bpy.ops.object.delete()</code></li> </ul>"},{"location":"api/often_used_values_and_operations/#camera","title":"Camera","text":"<ul> <li>Active camera object: <code>Scene.camera</code> (returns the camera object, not camera object data)</li> <li>Type: <code>Camera.type</code> (\"PERSP\", \"ORTHO\", ...)</li> <li>Lens unit: <code>Camera.lens_unit</code>  (either \"MILLIMETERS\" or \"FOV\")</li> <li>Focal length: <code>Camera.lens</code>  (in mm)</li> <li>Field of view (horizontal): <code>Camera.angle</code> (in radians)</li> <li>Clipping distances: <code>Camera.clip_start</code>, <code>Camera.clip_end</code></li> </ul>"},{"location":"api/often_used_values_and_operations/#rendering","title":"Rendering","text":"<ul> <li>Image resolution: <ul> <li>Width: <code>Scene.render.settings.resolution_x</code></li> <li>Height: <code>Scene.render.settings.resolution_y</code> </li> <li>Percentage: <code>Scene.render.settings.resolution_percentage</code></li> </ul> </li> <li>Output file: <code>Scene.render.filepath</code></li> <li>Image output type: <code>Scene.render.image_settings.file_format</code> (\"PNG\", \"JPEG\", ...)</li> <li>Number of samples per pixel (Cycles): <code>Scene.cycles.samples</code></li> <li>Render current scene: <code>bpy.ops.render.render()</code>. See parameters how to control the specific type of render (still image versus animation) and whether to save output</li> </ul>"},{"location":"api/often_used_values_and_operations/#animation","title":"Animation","text":"<ul> <li>Current frame <code>Scene.frame_current</code></li> <li>Frame range: <code>Scene.frame_start</code>, <code>Scene.frame_end</code></li> <li>Frame rate: <code>Scene.render.fps</code></li> </ul>"},{"location":"api/often_used_values_and_operations/#file-io","title":"File I/O","text":"<ul> <li>Save the current session to a specific file: <code>bpy.ops.wm.save_as_mainfile()</code></li> <li>Open Blend file <code>bpy.ops.wm.open_mainfile()</code></li> <li>Import a file (call depends on file type): <ul> <li><code>bpy.ops.wm.obj_import()</code> (OBJ scene), </li> <li><code>bpy.ops.wm.ply_import()</code> (PLY mesh), etc. </li> <li><code>bpy.ops.import_scene.gltf</code> (glTF scene)</li> <li>See here and here for more details.</li> </ul> </li> <li>Export a file (call depends on file type) follows the same call names. For scene export see here and here, for mesh export see here.</li> </ul>"},{"location":"api/operators/","title":"Operators","text":"<p>A special class of important API routines are the so-called operators. These are usually higher-level operations, such as adding a new cube mesh, deleting the current set of selected objects or running a file importer. As noted earlier many parts of the Blender UI are set up with Python scripts and in a lot of cases the operations you perform in the UI through menu actions or shortcut keys will simply call the relevant operator from Python to do the actual work. </p> <p>The Info area will show most operators as they get executed, but you can also check what API call is made for a certain UI element (this requires Python Tooltips to be enabled, see developer settings). For example, adding a plane mesh through the Add menu will call the operator <code>bpy.ops.mesh.primitive_plane_add()</code>, as the tooltip shows:</p> <p></p> <p>You can simply call the operator directly from Python to add a plane in exactly the same way as with the menu option:</p> <pre><code>&gt;&gt;&gt; bpy.data.objects.values()\n[]\n\n&gt;&gt;&gt; bpy.ops.mesh.primitive_plane_add()\n{'FINISHED'}\n\n# A plane mesh has now been added to the scene\n&gt;&gt;&gt; bpy.data.objects.values()\n[bpy.data.objects['Plane']]\n</code></pre> <p>Many of the operators take parameters, to influence the results. For example, with <code>bpy.ops.mesh.primitive_plane_add()</code> you can set the initial size and location of the plane (see the API docs for all the parameters):</p> <pre><code>&gt;&gt;&gt; bpy.ops.mesh.primitive_plane_add(size=3, location=(1,2,3))\n{'FINISHED'}\n</code></pre> <p>Info</p> <p>Note that operator parameters can only be passed using keyword arguments.</p>"},{"location":"api/operators/#operator-context","title":"Operator context","text":"<p>Operators are very nice and powerful, but they have a few inherent properties that can make them tricky to work with.</p> <p>An operator's execution crucially depends on the context in which it is called, where it gets most of the data it needs. As shown above simple parameter values can usually be passed, but values like the object(s) to operate on are retrieved implicitly. For example, to join a set of mesh objects into a single mesh you can call the operator <code>bpy.ops.object.join()</code>. But the current context needs to be correctly set for the operator to work:</p> <pre><code># We have no objects selected\n&gt;&gt;&gt; bpy.context.selected_objects\n[]\n\n&gt;&gt;&gt; bpy.ops.object.join()\nWarning: Active object is not a selected mesh\n{'CANCELLED'}\n\n# With 3 objects selected\n&gt;&gt;&gt; bpy.context.selected_objects\n[bpy.data.objects['Cube'], bpy.data.objects['Cube.001'], \nbpy.data.objects['Cube.002']]\n\n# Now it works\n&gt;&gt;&gt; bpy.ops.object.join()\n{'FINISHED'}\n</code></pre> <p>As can be seen above an operator call only returns a value indicating the execution status. When calling the operator in the Python Console as above some extra info is printed. But when calling operators from scripts the status return value is all you have to go on, as the extra message isn't printed when the script is executed. And in some cases the reason an operator fails can be quite unclear:</p> <pre><code>&gt;&gt;&gt; bpy.context.selected_objects\n[bpy.data.objects['Cube'], bpy.data.objects['Cube.001']]\n\n&gt;&gt;&gt; bpy.ops.mesh.intersect_boolean()\nTraceback (most recent call last):\n  File \"&lt;blender_console&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/share/blender/4.1/scripts/modules/bpy/ops.py\", line 109, in __call__\n    ret = _op_call(self.idname_py(), None, kw)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Operator bpy.ops.mesh.intersect_boolean.poll() failed, context is incorrect\n</code></pre> <p>This shows that the so-called <code>poll()</code> function failed, but what does that mean? The poll function is used by operators to determine if they can execute in the current context. They do this by checking certain preconditions on things like the selected object(s), the type of data or an object's mode. In this case the <code>bpy.ops.mesh.intersect_boolean()</code> operator can't perform a boolean intersection on multiple meshes, but only on the faces of a single object in edit mode, but this is not something you can tell from the error message (nor does the documentation make that clear):</p> <p></p> <p>One way to actually perform a boolean intersection on two objects from a Python script is to do what we would do in the UI: add a Boolean modifier on one of the objects and set its parameters, including a reference to the second object. We can take advantage of the Python Tooltips to see which operator we need:</p> <p></p> <p>This would suggest that using <code>bpy.ops.object.modifier_add(type='BOOLEAN')</code> would be what we need, but then setting the required parameters on the modifier (i.e. the object to subtract) would become tricky. So specifically for doing a boolean operation using a Boolean modifier is a better option, as shown in an earlier chapter.</p> <p>Unfortunately, certain operations can only be performed by calling operators. So there's a good chance that you will need to use them at some point when doing Python scripting. Hopefully this section gives some clues as how to work with them. </p> <p>See this section for more details on all the above subtleties and issues relating to working with operators. The <code>bpy.ops</code> documentation also contains useful information on operators, including how to override an operator's implicit context with values you set yourself.</p>"},{"location":"api/parenting/","title":"Parenting","text":"<p>An object's parent can be queried or set simply through its <code>parent</code> attribute, which needs to reference another Object (or <code>None</code>). </p> <p>But when parenting is involved the use of transformation matrices becomes somewhat more complex. Suppose we have two cubes above each other, the top cube transformed to Z=5 and the bottom cube to Z=2:</p> <p></p> <p>Using the 3D viewport we'll now parent the bottom cube to the top cube (<code>LMB</code> click bottom cube, <code>Shift-LMB</code> click top cube, <code>Ctrl-P</code>, select <code>Object</code>) and inspect the values in Python:</p> <pre><code>&gt;&gt;&gt; bpy.data.objects['Bottom cube'].parent\nbpy.data.objects['Top cube']\n\n# The bottom cube is still located in the scene at Z=2\n# in world space, even after parenting, as is expected\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].matrix_world\nMatrix(((1.0, 0.0, 0.0, 0.0),\n        (0.0, 1.0, 0.0, 0.0),\n        (0.0, 0.0, 1.0, 2.0),\n        (0.0, 0.0, 0.0, 1.0)))\n</code></pre> <p>If an object has a parent its <code>matrix_local</code> attribute will contain the transformation relative to its parent, while <code>matrix_world</code> will contain the resulting net object-to-world transformation. If no parent is set then <code>matrix_local</code> is equal to <code>matrix_world</code>. </p> <p>Let's check the bottom cube's local matrix value:</p> <pre><code># Correct, it is indeed -3 in Z relative to its parent\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].matrix_local\nMatrix(((1.0, 0.0, 0.0, 0.0),\n        (0.0, 1.0, 0.0, 0.0),\n        (0.0, 0.0, 1.0, -3.0),\n        (0.0, 0.0, 0.0, 1.0)))\n</code></pre> <p>As already shown above the <code>parent</code> attribute can be used to inspect and control the parenting relationship:</p> <pre><code>&gt;&gt;&gt; bpy.data.objects['Top cube'].parent\n# None\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].parent\nbpy.data.objects['Top cube']\n\n# Remove parent\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].parent = None\n</code></pre> <p>At this point the two cubes are no longer parented and are at Z=2 (Bottom cube) and Z=5 (Top cube) in the scene. But when we restore the parenting relationship from Python something funny happens <sup>1</sup>:</p> <pre><code># Set parent back to the top cube, as before\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].parent = bpy.data.objects['Top cube']\n</code></pre> <p></p> <p>The \"Bottom cube\" actually jumps in +Z direction as ends up on top of \"Top cube\". The reason for this behaviour is that when using the UI to set up a parenting relationship as earlier does more than just setting the <code>parent</code> attribute of the child object. There's also something called the parent-inverse matrix. Let's inspect it and the other matrix transforms we've already seen for the current (unexpected) scene:</p> <pre><code># Identity matrix, i.e. no transform\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].matrix_parent_inverse\nMatrix(((1.0, 0.0, 0.0, 0.0),\n        (0.0, 1.0, 0.0, 0.0),\n        (0.0, 0.0, 1.0, 0.0),\n        (0.0, 0.0, 0.0, 1.0)))\n\n# Hmmm, this places the \"Bottom cube\" 2 in Z *above* its parent at Z=5...\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].matrix_local\nMatrix(((1.0, 0.0, 0.0, 0.0),\n        (0.0, 1.0, 0.0, 0.0),\n        (0.0, 0.0, 1.0, 2.0),\n        (0.0, 0.0, 0.0, 1.0)))\n\n# ... so it indeed ends up at Z=7 as we saw (above \"Top cube\")\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].matrix_world\nMatrix(((1.0, 0.0, 0.0, 0.0),\n        (0.0, 1.0, 0.0, 0.0),\n        (0.0, 0.0, 1.0, 7.0),\n        (0.0, 0.0, 0.0, 1.0)))\n</code></pre> <p>So what happened here? Apparently the <code>matrix_local</code> matrix changed from its value of Z=-3 as we saw earlier. The answer is that when you set up a parenting relationship using the UI the parent-inverse matrix is set to the inverse of the current parent transformation (as the name suggests), while <code>matrix_local</code> is updated to <code>inverse(parent.matrix_world) @ to_become_child.matrix_world</code>.</p> <p>If we clear the <code>parent</code> value from Python and redo the parenting in the UI we can see this in the resulting transform matrices:</p> <pre><code>&gt;&gt;&gt; bpy.data.objects['Bottom cube'].parent = None\n\n# &lt;parent \"Bottom cube\" to \"Top cube\" in the UI&gt;\n\n# Was identity, is now indeed the inverse of transforming +5 in Z\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].matrix_parent_inverse\nMatrix(((1.0, -0.0, 0.0, -0.0),\n        (-0.0, 1.0, -0.0, 0.0),\n        (0.0, -0.0, 1.0, -5.0),\n        (-0.0, 0.0, -0.0, 1.0)))\n\n# Was Z=2, is now Z=2-5=-3\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].matrix_local\nMatrix(((1.0, 0.0, 0.0, 0.0),\n        (0.0, 1.0, 0.0, 0.0),\n        (0.0, 0.0, 1.0, -3.0),\n        (0.0, 0.0, 0.0, 1.0)))\n\n# Was Z=7\n&gt;&gt;&gt; bpy.data.objects['Bottom cube'].matrix_world\nMatrix(((1.0, 0.0, 0.0, 0.0),\n        (0.0, 1.0, 0.0, 0.0),\n        (0.0, 0.0, 1.0, 2.0),\n        (0.0, 0.0, 0.0, 1.0)))\n</code></pre> <p>The reason for this behaviour is that when doing parenting in the 3D viewport you usually do not want the object that you are setting as the child to move, which would happen if the child's existing transform is suddenly interpreted as being a transform relative to its parent. To overcome this the parent-inverse matrix set during the parenting steps is used to compensate, as it sits between the parent and child transforms. But when we simply set <code>parent</code> from Python, the <code>matrix_local</code> value is used as is, causing our bottom cube to suddenly move up, as it is used as the transform relative to its parent, while it actually would need a different value to stay in place. </p> <p>There's actually a worse side-effect, in that the location values shown in the Transform UI no longer seem to make sense:</p> <p></p> <p>Yes, that's right, the cube that's lower in Z (left image) shows a higher Z location value than the cube that's actually higher in Z (right image). Again, this is caused by the inconsistent parent-inverse matrix from doing \"manual\" parenting through Python.</p> <p>So when setting up parenting relations through Python it's best to be careful and keep the above in mind. There's actually quite a bit more going on with all the different parenting options available from the UI. See this page for more details.</p>"},{"location":"api/parenting/#children","title":"Children","text":"<p>To retrieve an object's children (i.e. the objects it is the parent of) one can use its <code>children</code> property. This only returns the direct children of that object, and so not children of its children, etc. Getting to the set of all children of an object (direct and indirect) was made slightly easier in Blender 3.1 with the addition of the <code>children_recursive</code> attribute.</p> <p>For example, given a Cube, Suzanne and Torus object, where Suzanne is parented to Cube, and the Torus is parented to Suzanne:</p> <pre><code>&gt;&gt;&gt; list(bpy.data.objects)\n[bpy.data.objects['Cube'], bpy.data.objects['Suzanne'], bpy.data.objects['Torus']]\n\n&gt;&gt;&gt; bpy.data.objects['Suzanne'].parent\nbpy.data.objects['Cube']\n\n&gt;&gt;&gt; bpy.data.objects['Torus'].parent\nbpy.data.objects['Suzanne']\n\n&gt;&gt;&gt; bpy.data.objects['Cube'].children\n(bpy.data.objects['Suzanne'],)\n\n&gt;&gt;&gt; bpy.data.objects['Suzanne'].children\n(bpy.data.objects['Torus'],)\n\n&gt;&gt;&gt; bpy.data.objects['Cube'].children_recursive\n[bpy.data.objects['Suzanne'], bpy.data.objects['Torus']]\n</code></pre> <p>These attributes are also available for collections.</p> <ol> <li> <p>The same thing happens when setting the parent in the UI using <code>Object Properties &gt; Relations &gt; Parent</code> \u21a9</p> </li> </ol>"},{"location":"api/selections/","title":"Selections","text":"<p>In a lot of cases you want to operate on a set of selected objects. You can access (read only) the current selection with <code>bpy.context.selected_objects</code>:</p> <pre><code>&gt;&gt;&gt; bpy.context.selected_objects\n[bpy.data.objects['Cube'], bpy.data.objects['Plane']]\n</code></pre> <p>Changing the current selection can be done in several ways. Selection state per object can be controlled with the <code>select_get()</code> and <code>select_set()</code> methods:</p> <pre><code>&gt;&gt;&gt; bpy.context.selected_objects\n[]\n\n&gt;&gt;&gt; bpy.data.objects['Camera'].select_get()\nFalse\n\n&gt;&gt;&gt; bpy.data.objects['Camera'].select_set(True)\n&gt;&gt;&gt; bpy.context.selected_objects\n[bpy.data.objects['Camera']]\n</code></pre> <p>The full selection set can also be changed:</p> <pre><code># Select all visible objects\n&gt;&gt;&gt; bpy.ops.object.select_all(action='SELECT')\n\n# Deselect all objects\n&gt;&gt;&gt; bpy.ops.object.select_all(action='DESELECT')\n\n# Toggle the selection state for each object\n&gt;&gt;&gt; bpy.ops.object.select_all(action='TOGGLE')\n</code></pre> <p>Note that the default mode for <code>bpy.ops.object.select_all()</code> when not specified is <code>TOGGLE</code>. </p> <p>Also note that the selection methods above operate only on objects that are currently visible objects in the scene (in terms of the outliner eye icon), just like for the selection hotkeys (such as <code>A</code>) in the 3D viewport. </p>"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/","title":"\ud83d\udcbb Interaction, selections, outliner","text":"<p>Here it's time for a first exercise! Follow the steps given below, which will let you work with Blender yourself and get to know the different methods of 3D scene interaction.</p> <p>Tip</p> <p>Summary of 3D view navigation:</p> <ul> <li><code>MMB</code> = rotate view</li> <li><code>Scrollwheel</code> or <code>Ctrl+MMB</code> = zoom view</li> <li><code>Shift+MMB</code> = translate view</li> <li><code>Home</code> = zoom out to show all objects</li> </ul> <p>See the cheat sheet to refresh your memory w.r.t. other view interaction and shortcut keys and mouse actions. </p>"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#viewpoints","title":"Viewpoints","text":"<ol> <li> <p>Load <code>motorbike.blend</code></p> <p>Tip</p> <p>This file will be in the data share under <code>data/basics/blender_basics</code></p> </li> <li> <p>In one of the two 3D views (your choice) manipulate the view to the following viewpoints:</p> <ul> <li>Alongside the motorbike, amongst the streamlines, looking in the   direction of travel.</li> <li>From the rider's point of view, just in front of the helmet,   looking ahead.</li> <li>An up-close point of view clearly showing the two streamlines that cross near   the rider's helmet on his/her right side, one going under the arm,   the other going over it.</li> </ul> </li> <li> <p>There is a single streamline that goes between the two rods of the steering column. Does that streamline terminate on the bike or does it continue past the bike? Try to get really close with the view so you can see where the streamline goes.</p> </li> </ol>"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#individual-selection","title":"Individual selection","text":"<ol> <li>Select all objects using the <code>A</code> key. As you've seen earlier this will introduce orange outlines surrounding selected objects.</li> <li>Check the outliner, specifically the color of the object names,    to see how the current selection is represented.</li> <li>In the 3D view deselect only the motorbike using <code>Shift-LMB</code> with the mouse    cursor at the appropriate position</li> <li>Again check the outliner status, do you notice a difference    in the name for the motorbike object?</li> <li>Add the motorbike back to the selection by using <code>Shift-LMB</code> over the bike    in the 3D view.</li> <li>Check the orange outline color of the motorbike (or the corresponding entry in the outliner) to verify that it is now the active object. It should be the only object with a light orange color.</li> <li>Use <code>Shift-LMB</code> with the mouse over the \"floor and walls\" object. What changed    in the selection? Specifically, what is now the active object?</li> <li>Once more use <code>Shift-LMB</code> on the \"floor ans walls\" object. What changed    this time in the selection status of the object?</li> </ol>"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#box-selection","title":"Box selection","text":"<ol> <li>Clear the selection with <code>Alt-A</code> (or double click the <code>A</code> key).</li> <li>Use box select (<code>LMB</code> drag) to select all objects in the scene.</li> <li>Clear the selection with the <code>Alt-A</code> key.</li> <li>Now try to select ONLY the motorbike using box select.     Check the outliner to make sure you're selecting just one object.     You can also check the status line at the bottom of the Blender window,     specifically the part that reads <code>Objects: #/#</code>, meaning selected / total.</li> </ol>"},{"location":"basics/blender_fundamentals/1_assignment_interaction_selections/#outliner-selection","title":"Outliner selection","text":"<ol> <li>Make sure no objects are currently selected.</li> <li> <p>Test with following actions in the outliner to get a good idea of     what actions it supports and how this influences the visual state     of the items in the outliner tree:</p> <ul> <li>Left-clicking on an item (possibly holding the <code>Shift</code> or <code>Ctrl</code> key)</li> <li>Using the keys <code>A</code> and <code>Alt-A</code> (note how these are similar   in functionality to what they do in the 3D view, but in the context   of the outliner items)</li> <li>Right-clicking on an item and choosing <code>Select</code> or <code>Deselect</code> </li> </ul> </li> <li> <p>How does the blue highlight of a line in the outliner relate to     the selection status of an object in the 3D view?</p> </li> </ol>"},{"location":"basics/blender_fundamentals/2_assignment_transformations/","title":"\ud83d\udcbb Transformations","text":"<p>Tip</p> <ul> <li>You can clear an object's translation to all zero with <code>Alt-G</code></li> </ul> <ul> <li>You can clear an object's rotation to all zero with <code>Alt-R</code></li> </ul> <ul> <li>You can clear an object's scale to all zero with <code>Alt-S</code></li> </ul> <ul> <li>You can undo a transformation with <code>Ctrl-Z</code> (or reload the file   to reset completely)</li> </ul> <ul> <li>See section Object Actions of the cheat sheet for more shortcut keys</li> </ul>"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#basic-transformations","title":"Basic transformations","text":"<ol> <li>Load <code>axes.blend</code></li> <li> <p>The Axes object in the scene is a 3D object just like any other.    Note that the axes object shows the local axes of the object.</p> </li> <li> <p>Try translating, rotating and scaling the axes object with the    different methods shown:</p> <ul> <li>The transform widgets (accessible from the toolbox on the upper-left)</li> <li>Using the <code>G</code>, <code>R</code> or <code>S</code> keys</li> <li>Entering values in the properties region in the upper-right of the view, under <code>Transform</code></li> </ul> </li> <li> <p>Activate one of the transform modes (e.g. <code>G</code> for translation) and    experiment with limiting a transformation to an axis with <code>X</code>, <code>Y</code> or <code>Z</code> keys,</p> </li> <li> <p>Activate one of the transform modes (e.g. <code>G</code> for translation) and experiment with limiting a transformation to a    plane with <code>Shift-X</code>, <code>Shift-Y</code> or <code>Shift-Z</code>.</p> </li> <li> <p>Reload the <code>axes.blend</code> file to get back the original scene.</p> </li> <li>Rotate the axes 30 degrees around (global) X.</li> <li>Now rotate the axes 45 degrees around the local Z axis.</li> </ol>"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#pivot-point-modes","title":"Pivot point modes","text":"<ol> <li>Load <code>transformations.blend</code></li> <li> <p>Select the cone, monkey, torus and sphere</p> </li> <li> <p>Set pivot mode to <code>Median Point</code> (using the Pivot Point pie menu,    which opens with the <code>.</code> key, i.e. period), if it isn't already.</p> </li> <li> <p>Press <code>S</code> to start scaling, then move the mouse to scale the objects apart</p> </li> <li> <p>Notice that as you scale up the objects increase in size and move apart, but only the torus' center point (the orange dot) moves below the plane. Why?</p> </li> <li> <p>Cancel the scale operation with <code>Esc</code> or a <code>RMB</code> click</p> </li> <li> <p>Enable the <code>Only Locations</code> option in the Pivot Point pie menu.    When this is enabled it will cause any transformation to be applied to the    locations of the objects (shown as orange circles), instead of to the objects themselves.</p> </li> <li> <p>Repeat the scaling of the four objects. Do you notice how the    objects now transform differently?</p> </li> <li> <p>Change the pivot mode to <code>Individual Origins</code> and disable the <code>Only Locations</code> option.    Do the scaling again, notice the difference.</p> </li> <li> <p>Enable the <code>Only Locations</code> setting. When you try to rotate the      objects around Z nothing happens. Why not?</p> </li> <li> <p>Change the pivot mode to <code>Median Point</code>, leave <code>Only Locations</code> enabled.</p> </li> <li> <p>Rotate the objects around the Z axis.</p> </li> <li> <p>Now disable the <code>Only Locations</code> option and rotate the objects     once again around the Z axis. Do you notice the subtle difference     in transformation?</p> </li> <li> <p>Experiment some more with different selections of objects and the different     Pivot Point modes, until you feel you get the hang of it.</p> </li> </ol>"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#rubiks-cube","title":"Rubik's cube","text":"<p>Tip</p> <ul> <li>You can add a cube object with <code>Shift-A &gt; Mesh &gt; Cube</code></li> <li>You can duplicate selected objects with <code>Shift-D</code>. This will also activate   grab mode after the duplication.</li> </ul> <ol> <li> <p>Start with an empty scene (<code>File &gt; New &gt; General</code>)</p> </li> <li> <p>Model a Rubik's cube: 3x3x3 Cube objects (minus the center cube) on a rectangular grid. Try to get the spacing between the Cube objects the same in all directions. </p> </li> <li> <p>Now select one face of the Rubik's cube (i.e. 3x3 cubes) and rotate it 30 degrees just like the real thing.</p> </li> </ol>"},{"location":"basics/blender_fundamentals/2_assignment_transformations/#bonus-2001-a-space-odyssey","title":"Bonus: 2001 - A Space Odyssey","text":"<ol> <li> <p>Start with an empty scene (<code>File &gt; New &gt; General</code>)</p> </li> <li> <p>Remember the scene from Space Odyssey 2001, with our primate ancestors looking up at the monolith? Recreate that scene :)</p> </li> </ol> <ul> <li>Add 4 or more monkey heads, surrounding a thin narrow box for the monolith</li> <li>Make the monkeys look up at the monolith</li> <li>If you want to go crazy add bodies to the monkeys using some scaled spheres</li> <li>Add a sun object + corresponding light somewhere in the sky.</li> </ul>"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/","title":"\ud83d\udcbb Cameras and views","text":"<ol> <li> <p>Open <code>cameras.blend</code></p> <p>This scene contains a bunny object, a sun light and two cameras: \"Close-up\" near the bunny's head and \"Overview\" further away.</p> </li> <li> <p>Select the Overview camera object, by either left-clicking on it in the 3D view or in the Outliner.</p> </li> <li>Make this camera the active camera with either the outliner (click on the green camera icon right of the name),    <code>View &gt; Cameras &gt; Set Active Object as Camera</code> or use <code>Ctrl-Numpad0</code>. Notice that the 3D view changes to the camera's viewpoint.</li> <li>Rotate the 3D view with <code>MMB</code> to exit the camera view. You are now back in    the normal 3D view interaction.</li> <li>Select the Close-up camera</li> <li>Switch to camera view by bringing up the View pie menu with <code>`</code> (backtick, usually below the <code>~</code>), then pick <code>View Camera</code>.</li> <li>What camera view are you now seeing, Close-up or Overview?</li> <li>So one thing to remember is that selecting a camera does not make it    the \"active camera\" (even though it can be the active object, confusingly).</li> <li>Change the active camera to Close-up</li> <li>Rotate away from the camera view to the normal 3D view</li> <li>For switching back to the active camera view there's two more methods apart from the pie menu, try them:<ul> <li>Using the View menu at the top of the 3D view area: <code>View &gt; Cameras &gt; Active Camera</code></li> <li>Press <code>Numpad0</code> </li> </ul> </li> <li>Experiment with the different camera controls until you find the ones you're comfortable with</li> <li>Rotate away from the camera view to a 3D view that shows both cameras.</li> <li>In the Scene properties tab  on the right-hand side of the window (and not the similar icon in the top bar left of Scene) there's     a drop-down menu Camera which lists the active camera. Change the     active camera using that selection box. Apart from the name listed under the Scene properties     do you notice how you can identify the active camera in the 3D view? Hint: it's subtle     and unrelated to the yellow/orange color used for highlighting selected objects.</li> </ol>"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/#camera-transformation","title":"Camera transformation","text":"<ol> <li>Make sure the Overview camera object is the only selected object</li> <li>Make the Overview camera the active camera and then switch to its view</li> <li>In the camera view use regular object transformations to     point the camera at the rabbit's tail. To refresh, in camera view with only the     camera selected:<ul> <li>Press <code>G</code> to translate, then move the mouse to change the view</li> <li>While still in move mode press <code>MMB</code> (or <code>Z</code> twice) to enter \"truck\" mode: this moves the camera forward/backward along the view axis. Pressing <code>X</code> twice will allow moving the camera sideways.</li> <li>Press <code>R</code> to rotate around the view axis        </li> <li>In rotate mode press <code>MMB</code> to \"look around\"</li> <li><code>LMB</code> to confirm, <code>Esc</code> to cancel</li> </ul> </li> <li>Another useful feature is when you like the current viewpoint in the 3D view     and want to match the active camera to this viewpoint. For this you can use     <code>Ctrl-Alt-Numpad0</code> (or with <code>View &gt; Align View &gt; Align Active Camera To View</code> in the header of the 3D view)</li> </ol>"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/#quad-view","title":"Quad view","text":"<ol> <li>Switch the 3D View to the so-called Quad View with <code>Ctrl-Alt-Q</code>.     You now have orthogonal 2D views along the three axes (Top, Front and Right Orthographic), plus a 3D view (Camera Perspective).     Note: the three axis views can only be translated and zoomed, not rotated    </li> <li>Change the upper-right quad to a camera view, if it isn't already</li> <li>Press <code>N</code> to show the sidebar on the right</li> <li>On the <code>View</code> tab, under <code>View Lock</code>  there's a Lock option called <code>Camera to View</code>.     Enable that option. You should now see a dotted red outline around the orange     camera rectangle in the <code>Camera Perspective</code> view.</li> <li>Hide the sidebar again (<code>N</code>), leaving the Lock option enabled</li> <li>Change the view in the <code>Camera Perspective</code> view using the regular 3D view mouse     interaction (<code>MMB</code> to rotate, <code>Shift-MMB</code> to translate, <code>Ctrl-MMB</code> to move forward/backward).     Observe what happens to the active camera in the other quadrants when you     alter the view.</li> <li>Use the sidebar again to disable the Lock <code>Camera to View</code> option</li> </ol>"},{"location":"basics/blender_fundamentals/3_assignment_camera_and_views/#fly-mode","title":"Fly mode","text":"<ol> <li>Add a camera to the scene (<code>Shift-A &gt; Camera</code>). It will be placed     at the position of the 3D cursor (the small red-white striped circle and axes).</li> <li>Change the upper-right view to this camera</li> <li>Activate fly mode with <code>Shift-`</code> (backtick). Use the <code>ASDWXQE</code> keys     to move this camera close to the two bunny ears and look between them.     You can change the fly speed with the mouse <code>Wheel</code>.     In fly mode you can confirm the current view with <code>LMB</code> or press <code>Enter</code>.     Press <code>Esc</code> to cancel and go back to the original view.</li> </ol>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/","title":"\u26a0\ufe0f Avoiding data loss","text":"<p>There are some things to be aware of when working with Blender that might behave a little different from other programs, or general expectations, and that can potentially cause you to loose work. </p>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#the-file-overwrite-prompt-is-very-subtle","title":"The file overwrite prompt is very subtle","text":"<p>Suppose we have saved our work to a file <code>scene.blend</code>. We then make some more changes in Blender to create a second version of our scene and save this as <code>scene2.blend</code>. Finally, we make a third version and intend to save this as <code>scene3.blend</code>, but we forget to change the file name in the save dialog and it stays at the current <code>scene2.blend</code>.  The Blender way of warning you that you are about to overwrite an existing file is really subtle:</p> <p></p> <p>Notice the red color behind the file name? That's the signal that the file name you entered is the same as an existing file in the current directory. There is no \"Are you sure you want to overwrite an existing file?\" dialog, or something to that effect.</p> <p>If we change the file name to something that doesn't exist yet the color becomes gray again:</p> <p></p> <p>The <code>File &gt; Save As</code> workflow (and similar for related file dialogs) is a somewhat double-edged sword:</p> <ul> <li>If you're aware of the above signal and intend to quickly overwrite the current file you can simply press <code>Enter</code> once in the dialog, and the file will be saved with no \"Overwriting are you sure?\" prompt is shown. So in this respect the UI stays out of your way and avoids an extra confirmation dialog.</li> <li>But if you miss the red prompt or are unaware of its meaning then it's easy to accidentally overwrite existing work.</li> </ul>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#easy-incremental-file-versions","title":"Easy incremental file versions","text":"<p>A nice way to save to successive versions of a file is using the <code>+</code> button right of the file name, as shown in the pictures above. Using the <code>+</code> (and <code>-</code>) you can easily change the version number at the end of a file name, e.g. <code>scene2.blend</code> to <code>scene3.blend</code>. The red overwrite indicator will also update depending on the existence of the chosen file name.</p> <p>Warning</p> <p>Using the <code>+</code> button merely increments the number in the file name. It does not guarantee that the  file does not exist yet (i.e. no check is made with what's on disk).</p>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#unused-data-blocks-in-the-scene-are-not-saved","title":"Unused data-blocks in the scene are not saved","text":"<p>Suppose you have a 3D scene and have created a material A that you use on some object. You then create a material B and assign it to the same object, causing material A to now be unused in the scene. If you save your scene to file at this point material A  will not get saved to the file, as it is not referenced by anything in the scene. This automatic \"garbage collection\" feature of Blender is somewhat controversial, and it is definitely good to be aware of this behaviour. </p> <p>For most scene elements used in the Basics part of this course garbage-collection-on-save does not really cause concern, as, for example, any 3D objects added to a scene are always linked from that scene. One exception is Materials (as described in the example above). For materials, and data blocks in general, you can see if they are unused by checking for a <code>0</code> left of their name when they appear in a list:</p> <p></p> <p>The quick fix in case you have a material that is currently not used in the scene, but that you definitely want to have saved to file, is to use the \"Fake User\" option by clicking the shield icon (be sure to enable this option for the right material!):</p> <p></p> <p>You can verify the material now has a fake user as intended by checking for an <code>F</code> left of its name:</p> <p></p> <p>Note that you can use the same Fake User option for some other types of scene elements as well, not just materials.</p> <p>We have a more detailed discussion of the garbage collection system in a section in the Python scripting reference. The behaviour described relates to the data block system that Blender uses internally and for normal use the description above should be sufficient, but can also be influenced from Python.</p>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#recovering-lost-work","title":"Recovering lost work","text":"<p>Murphy's Law usually strikes when you least expect it. Fortunately, there are several layers of defense in  case something goes unexpectedly wrong when saving files, or in case Blender crashes.  It depends on the situation you're trying to recover from which one of the options below provides the best results, if applicable.</p> <p>Please check what each of these features does, to make sure you don't accidentally make things worse by using one of the recover options within Blender in the wrong way.</p>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#those-blend1-files","title":"Those .blend1 files?","text":"<p>You might notice that when you overwrite an existing file, say <code>file.blend</code>, another file called <code>file.blend1</code> will now have appeared next to it in the same directory. This is Blender's method for automatically keeping around the previous version of the file you overwrote: it first moves the existing <code>file.blend</code> to <code>file.blend1</code>, and only then saves the new <code>file.blend</code>. </p> <p>So if you accidentally overwrite a file you can still get to the previous version (the .blend1 file), as long as you haven't overwritten more than once.</p> <p>Keeping more than 1 previous version</p> <p>You can actually have multiple previous versions kept around if you like. The preference setting for this is <code>Save &amp; Load &gt; Blend Files &gt; Save Versions</code>, which defaults to 1. If you would increase it then files with extensions .blend2, .blend3 and so on would be kept around.</p>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#auto-save","title":"Auto save","text":"<p>By default, Blender will automatically save your current session to a file in a temporary directory every few minutes (2 minutes by default). The settings that control this are under <code>Save &amp; Load &gt; Blend Files</code>, specifically the checkbox on <code>Auto-Save</code> to enable auto-save and the <code>Timer (Minutes)</code> setting under <code>Auto-Save</code>.</p> <p>This auto-save file is stored in your system's temporary directory, and uses the process ID of Blender in the file name, as well as the string <code>_autosave</code>. Here is an example from a Linux system, where <code>/tmp</code> is used and Blender's process ID is 66597:</p> <pre><code>melis@juggle 22:13:/tmp$ ps aux | grep blender\nmelis      66597  1.2  5.7 1838680 463920 ?      Sl   21:54   0:14 blender\n\nmelis@juggle 22:13:/tmp$ ls 66597*\ntest_66597_autosave.blend\n</code></pre> <p>See this section of the Blender manual on recovering a session from an auto-save file from the <code>File</code> manual (you can also copy or load the file manually, of course, there is nothing special about it).</p> <p>Edit mode data not saved</p> <p>If you happend to be in edit (or sculpt) mode at the time Blender does an auto-save, then the current updated state of the mesh will not get saved. This is a limitation of the auto-save feature.</p>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#last-session-accidental-quit-without-saving","title":"Last session (accidental quit without saving)","text":"<p>Whenever Blender quits normally (i.e. not a crash) it will save the current session state to a file called <code>quit.blend</code> in your system's temporary directory. You can easily load this <code>quit.blend</code> file with the <code>File &gt; Recover &gt; Last Session</code> option (or copy it to a different location and load it as any Blender file).</p> <p>One of the cases where this feature might come in handy is if you quit Blender, have unsaved changed, but accidentally click the <code>Don't Save</code> button in the <code>Save changes before closing?</code> dialog. The <code>quit.blend</code> file in this case will contain those unsaved changes. But be sure to make a copy of it before quitting Blender again, as that will overwrite it.</p> <p>Quit-on-save always enabled</p> <p>Note that there currently is no option to disable this Save-on-Quit feature. So for large scenes this will incur a (usually short) delay when exiting.</p>"},{"location":"basics/blender_fundamentals/avoiding_data_loss/#blender-crash","title":"Blender crash","text":"<p>In case Blender crashes it usually does not manage to save the current scene to a recovery file. So in this case you are hopefully able to recover any lost work using the files available from the features described above.</p>"},{"location":"basics/blender_fundamentals/cameras_and_views/","title":"Cameras and views","text":"<p>This section shows cameras and how to work with them. In the exercise after this section you get to try a lot  of the operations shown, so following the video along isn't strictly needed. But if you do want to then the file used is <code>data/blender_basics/cameras.blend</code>.</p>"},{"location":"basics/blender_fundamentals/command_line/","title":"Command-line rendering","text":"<p>When your scene becomes more complex the rendering becomes more time consuming. Then you want to have more control over where and when you render your shots. You also might want to render on another more powerful machine. In those cases you want to know how to run Blender from the command-line in a terminal. Lets see how easy you can achieve this.</p> <p></p>"},{"location":"basics/blender_fundamentals/command_line/#starting-blender-from-the-console","title":"Starting Blender from the console","text":"<p>In order to use Blender from the console, you need to know where to find the executable. Depending on the operating system you're working on the location can differ:</p> OS Executable name Location (default) Windows <code>blender.exe</code> <code>C:\\Program Files\\Blender Foundation\\Blender</code> macOS <code>Blender</code> <code>/Applications/Blender.app/Contents/MacOS/Blender</code> Linux <code>blender</code> <code>/usr/bin/blender</code> <p>Tip</p> <p>Note that the executable location might be on the <code>PATH</code>, so you can just call the executable itself, without specifying the full path.</p> <p>The general format format for calling Blender from the command-line (with <code>blender</code> being the executable from above) is</p> <pre><code>blender [args ...] [file] [args ...]\n</code></pre> <p>So if you wanted to start Blender and open a file in one step you would call</p> <pre><code>blender myfile.blend\n</code></pre>"},{"location":"basics/blender_fundamentals/command_line/#executing-blender-without-gui","title":"Executing Blender without GUI","text":"<p>In a lot of cases when you're using Blender from the command-line, such as rendering a frame to an image file, you do not want use the graphical user interface. For this you can use the <code>-b</code> option:</p> <p><code>blender -b myfile.blend [...]</code></p> <p>This command simply opens Blender and quits it right away, because it runs in the background with nothing to do:</p> <pre><code>$ blender -b myfile.blend \nBlender 4.1.1 (hash e1743a0317bc built 2024-04-15 23:47:45)\nRead prefs: \"/home/melis/.config/blender/4.1/config/userpref.blend\"\nRead blend: \"/tmp/myfile.blend\"\n\nBlender quit\n</code></pre>"},{"location":"basics/blender_fundamentals/command_line/#rendering-a-frame-or-more","title":"Rendering a frame (or more)","text":"<p>If we add, say <code>-f 1</code>, we ask Blender to render the first frame and save it to disk:</p> <pre><code>$ blender -b myfile.blend -f 1\nBlender 4.1.1 (hash e1743a0317bc built 2024-04-15 23:47:45)\nRead prefs: \"/home/melis/.config/blender/4.1/config/userpref.blend\"\nRead blend: \"/tmp/myfile.blend\"\nFra:1 Mem:43.56M (Peak 44.63M) | Time:00:00.10 | Syncing Cube\nFra:1 Mem:43.60M (Peak 44.63M) | Time:00:00.11 | Syncing Light\nFra:1 Mem:43.60M (Peak 44.63M) | Time:00:00.11 | Syncing Camera\nFra:1 Mem:43.60M (Peak 44.63M) | Time:00:00.11 | Rendering 1 / 64 samples\nFra:1 Mem:43.61M (Peak 44.63M) | Time:00:00.15 | Rendering 26 / 64 samples\nFra:1 Mem:43.61M (Peak 44.63M) | Time:00:00.18 | Rendering 51 / 64 samples\nFra:1 Mem:43.61M (Peak 44.63M) | Time:00:00.19 | Rendering 64 / 64 samples\nSaved: '/tmp/0001.png'\nTime: 00:00.48 (Saving: 00:00.27)\n\n\nBlender quit\n</code></pre> <p>We can render a range of frames with <code>-f &lt;first&gt;..&lt;last&gt;</code>, or specific frames with <code>-f &lt;i&gt;,&lt;j&gt;,&lt;k&gt;,...</code>.</p>"},{"location":"basics/blender_fundamentals/command_line/#arguments-are-executed-in-order-given","title":"Arguments are executed in order given","text":"<p>It is important to know that any arguments provided (either before or after the filename) are executed in the order given. Take this command, for example:</p> <pre><code>$ blender -b myfile.blend -f 10 -o frame.png\nBlender 4.1.1 (hash e1743a0317bc built 2024-04-15 23:47:45)\nRead prefs: \"/home/melis/.config/blender/4.1/config/userpref.blend\"\nRead blend: \"/tmp/myfile.blend\"\nFra:10 Mem:43.56M (Peak 44.63M) | Time:00:00.10 | Syncing Cube\nFra:10 Mem:43.60M (Peak 44.63M) | Time:00:00.11 | Syncing Light\nFra:10 Mem:43.60M (Peak 44.63M) | Time:00:00.11 | Syncing Camera\nFra:10 Mem:43.60M (Peak 44.63M) | Time:00:00.11 | Rendering 1 / 64 samples\nFra:10 Mem:43.61M (Peak 44.63M) | Time:00:00.14 | Rendering 26 / 64 samples\nFra:10 Mem:43.61M (Peak 44.63M) | Time:00:00.17 | Rendering 51 / 64 samples\nFra:10 Mem:43.61M (Peak 44.63M) | Time:00:00.19 | Rendering 64 / 64 samples\nSaved: '/tmp/0010.png'\nTime: 00:00.47 (Saving: 00:00.27)\n\n\nBlender quit\n</code></pre> <p>This starts Blender in background mode (<code>-b</code>), opens myfile.blend, renders frame 10 (<code>-f 10</code>)... and then sets the output file name to <code>frame.png</code> using <code>-o</code>. But because the <code>-f</code> comes before the <code>-o</code>, the output of the render is written to the output location set in the Output Properties in the blend file, not what is specified by <code>-f</code>.</p> <p>On the other hand, when <code>-o</code> comes before <code>-f</code> the command more or less does what you expect and probably want: <pre><code>$ blender -b myfile.blend -o frame.png -f 10\n...\nFra:10 Mem:43.61M (Peak 44.63M) | Time:00:00.19 | Rendering 64 / 64 samples\nSaved: 'frame.png0010.png'\n...\n</code></pre></p> <p>However, notice the output file name is <code>frame.png0010.png</code>, which is not the <code>frame.png</code> we specified. This is due to the output file set with <code>-o</code> actually being a template, not a real file name. When rendering a frame the frame number is filled in in the template to determine the actual output file name. We can use <code>#</code> markers in the template to fix this:</p> <pre><code>$ blender -b myfile.blend -o frame####.png -f 10\n...\nFra:10 Mem:43.61M (Peak 44.63M) | Time:00:00.18 | Rendering 64 / 64 samples\nSaved: 'frame0010.png'\n...\n</code></pre> <p>Another useful element in the template is <code>//</code>. This is replaced with the path of the current Blender file, so it provides an easy way to render frames to the same directory as the Blender file (or a sub-directory). For example:</p> <pre><code>$ blender -b myfile.blend -o //frames/ -f 10\n...\nFra:10 Mem:43.61M (Peak 44.63M) | Time:00:00.17 | Rendering 64 / 64 samples\nSaved: '/tmp/frames/0010.png'\n</code></pre>"},{"location":"basics/blender_fundamentals/command_line/#rendering-an-animation","title":"Rendering an animation","text":"<p>You can render an animation in a Blender file using <code>-a</code> and set start and end frames to render with <code>-s</code> and <code>-e</code>. For example:</p> <p><code>blender -b my_animation.blend -s 5 -e 35 -a</code></p> <p>This will render the animation, in the background, from frame 5 to 35 and output the images to the output specified in the blend file.</p>"},{"location":"basics/blender_fundamentals/command_line/#figuring-out-more-commands","title":"Figuring out more commands","text":"<p>There are many more command-line options, which can be listed with <code>blender --help</code>. For example, there are options to run Python scripts, switch Blender scenes, set the output image format, or choose the render engine.</p> <p>You can also read more on rendering using the command-line on this page in the official Blender documentation.</p>"},{"location":"basics/blender_fundamentals/first_steps/","title":"First steps in the user interface","text":"<p>Tip</p> <p>A lot of new concepts and UI elements will be introduced in the upcoming videos. It probably works best to watch video(s) limited to a certain topic, try the operations shown and corresponding exercise(s) in Blender yourself, before moving on to the next topic.</p>"},{"location":"basics/blender_fundamentals/first_steps/#starting-blender","title":"Starting Blender","text":"<p>In general Blender isn't different to start than any other GUI application. </p> <p>However, warning and error messages will be printed to the console window. It depends  on the operating system you're working on how the console window is available:</p> <ul> <li>(All operating systems) If you start Blender from a terminal window, e.g. xterm or Windows Command window, then Blender output will be printed in that window</li> </ul> <ul> <li>(Windows only) If Blender was started from the Start menu, or using a desktop icon, then you can toggle the associated console window using the <code>Window &gt; Toggle System Console</code> option</li> </ul> <p>See this section in the Blender manual for more details on starting Blender from the command line and details specific for each operating system.</p>"},{"location":"basics/blender_fundamentals/first_steps/#user-interface-fundamentals","title":"User interface fundamentals","text":"<p>We will go over fundamentals of the user interface in terms of interaction and areas, specifically the 3D view and Outliner. We also touch on a number of often-performed operations, such as rendering an image and changing the set of selected objects. We also look a bit closer into keyboard shortcuts and menus.</p> <p>It's probably best to follow along in Blender on your own system while viewing the video. The files used in the video can be found under <code>data/blender_basics</code>.</p> <p>Slow 3D interaction</p> <p>If the interaction in the 3D view isn't as smooth as seen in the video on your own PC/laptop something might be wrong in the setup of your system.  Please contact us if this appears to be the case.</p> <p>Accidental 'Edit mode'</p> <p>If the 3D view (or some of the other areas) suddenly appear to behave strangely, or you now see your mesh with all kinds of dots or lines then you might have accidentally entered the so-called \"Edit Mode\" or any of the other modes available (<code>Tab</code> and <code>Ctrl-Tab</code> are used for this).  Check the menu in the upper left of the 3D view, which should read <code>Object Mode</code>:</p> <p></p> <p>In the Basics module we will use only Object Mode (and briefly use Vertex Paint mode in one of the exercises). You can use the drop-down menu shown above (or the <code>Ctrl-Tab</code> menu in the 3D view) and  pick <code>Object Mode</code> to get back to the correct mode.   </p> <p>Accidental workspace switch</p> <p>Another thing that might happen is that you accidentally clicked one of the tabs at the top of the screen, which then completely changes the layout of your user interface. These tabs are used to switch between workspaces, where each workspace allows a different layout to focus on a certain task (e.g. 3D modeling, versus shader editing, versus animation).  The default workspace is <code>Layout</code> and you might have to switch back to that one:</p> <p></p>"},{"location":"basics/blender_fundamentals/first_steps/#some-user-interface-tips","title":"Some user interface tips","text":"<ul> <li>The location of the mouse pointer controls the current area in focus and any keyboard actions are applied in the active area first.</li> </ul> <ul> <li>To bring up the relevant section of the official Blender manual for (almost) any user interface element, e.g. button, setting or menu: right-click with the mouse on that element and click <code>Online Manual</code>. This will start a web browser showing the relevant manual page.</li> </ul> <ul> <li>You can hover with the mouse over pretty much any UI element to get a tooltip with a short description, including shortcut key(s) if available.</li> </ul> <ul> <li>The keyboard and mouse shortcuts for object selection, editing, view interaction, etc work mostly the same in all Blender editors. So <code>G</code> to grab, <code>X</code> to delete,  <code>LMB</code> to select, <code>Shift-MMB</code> to translate, <code>Wheel</code> to zoom, etc.</li> </ul> <ul> <li>You can maximize a user interface area by using <code>Ctrl+Spacebar</code>, having the mouse in the area you want to maximize. This can sometimes be useful to temporarily get a larger area to work with. You can use the same shortcut to toggle the area back to its original size, or use the <code>Back to Previous</code> button at the top of the screen.</li> </ul>"},{"location":"basics/blender_fundamentals/first_steps/#changes-to-default-preferences-settings","title":"Changes to default preferences settings","text":"<p>Optional</p> <p>It's not required to change these defaults, but we find they help us in working with Blender, and so might be useful for you as well</p> <p>Here we suggest some preferences settings to change from their default value.</p> <p>Under <code>Edit &gt; Preferences</code>, in the <code>Interface</code> tab:</p> <ul> <li>Under <code>Display</code> disable <code>Splash Screen</code>. This will save you a click to get rid of the splash screen each time you start Blender. If you ever want to look at the splash again you can use the Blender logo icon in the top-level of the window and use <code>Splash Screen</code>.</li> <li>Under <code>Editors &gt; Status Bar</code> enable <code>Scene Statistics</code>, <code>System Memory</code> and <code>Video Memory</code>. This will show some useful scene statistics in the status bar. Another way to do this is to right-click on the status bar and enable the same options.</li> <li>Under <code>Editors &gt; Temporary Editors</code> set <code>Render In</code> to <code>Image Editor</code>. This will cause the rendered image to be displayed as a replacement of the 3D view, instead of in a separate window. After rendering press <code>Escape</code> to get back the 3D view that was replaced by the rendered output.</li> <li>In case you find that Blender's user interface elements, such as buttons or menu text, are too small you can scale the UI elements with a single setting under <code>Display &gt; Resolution Scale</code>. If you change the value you can see the changes in the UI immediately.</li> </ul> <p>Orbit around selection</p> <p>Another option which you might consider enabling is <code>Orbit Around Selection</code> on the <code>Navigation</code> tab under <code>Orbit &amp; Pan</code>. By default this is turned off and in that mode any rotation of the 3D viewport will be around the center of the view, which might cause selected objects to go out of view. When the option is turned on viewport rotation will be around the selected object(s), always keeping them in view.</p> <p>Turntable versus trackball</p> <p>Another setting on the <code>Navigation</code> tab under <code>Orbit &amp; Pan</code> is the Orbit Mode. The default is <code>Turntable</code>, which always keeps the Z axis of the scene pointing upwards when rotating the view of the scene. For a lot of scenes this makes sense, but you can also switch this option to <code>Trackball</code>, in which case the scene can be rotated in all directions freely.</p>"},{"location":"basics/blender_fundamentals/introduction/","title":"Introduction","text":"<p>This first part of the course is meant to introduce you to Blender, its user interface  and basic features. We'll start with a brief look into some of the background of Blender and challenges in learning it.</p>"},{"location":"basics/blender_fundamentals/objects_3d_cursor_undo/","title":"Objects, 3D cursor, Undo","text":"<p>A short section on how to add, duplicate or delete objects. What the 3D cursor is and what role it plays, plus the undo system.</p>"},{"location":"basics/blender_fundamentals/scene_hierarchy/","title":"Scene hierarchy","text":"<p>We briefly look a the way a scene is organized and how this interacts with the properties  panel. </p> <p>The above actually isn't the full story, as we only briefly mention collections. In the official Blender manual you can find more detail on collections here, in case you want to know more.</p>"},{"location":"basics/blender_fundamentals/transformations/","title":"Transformations","text":"<p>This might be a bit more of a technical subject and deals with the way 3D objects can be transformed in a scene. The transformations exercise will allow you to try most of these operations yourself. But if you wanto to follow along with the video then the file used is <code>data/blender_basics/three_objects.blend</code>.</p>"},{"location":"basics/blender_fundamentals/transformations/#summary-of-shortcut-keys","title":"Summary of shortcut keys","text":"<ul> <li><code>G</code> to enter translation mode (\"grab\")</li> <li><code>S</code> to enter scale mode</li> <li><code>R</code> to enter rotation mode</li> <li><code>LMB</code> or <code>Enter</code> to confirm the current transformation, <code>Escape</code> to cancel while still one of the transformation modes</li> <li>While in a transformation mode press <code>X</code>, <code>Y</code> or <code>Z</code> to constrain the transformation to the X, Y or Z axis, respectively. </li> <li>While in a transformation mode press <code>Shift+X</code>, <code>Shift+Y</code> or <code>Shift+Z</code> to constrain the transformation to the plane perpendicular to the X, Y or Z axis, respectively. </li> </ul>"},{"location":"basics/blender_fundamentals/ui/","title":"User interface configuration","text":"<p>A short section on how the Blender user interface system works and how to configure it to your liking. This is useful to know as the current UI layout is saved in a Blender file, so files you get from some other source might look very different.</p>"},{"location":"basics/importing_data/exercise_vertex_colors/","title":"\ud83d\udcbb Vertex colors","text":"<p>This exercise uses a file exported from the ParaView scientific visualization package, and uses some of the workflow needed to get it into Blender.</p> <p>X3D Importer</p> <p>Check if you have a menu option to import X3D format. For this, go to <code>File -&gt; Import</code> and check if there is an entry <code>X3D Extensible 3D (.x3d/.wrl)</code>.</p> <p>If you do NOT have the X3D import option then perform the following steps to enable the X3D add-on (otherwise continue to step 3):</p> <ul> <li>Open the preferences window with <code>Edit -&gt; Preferences</code></li> <li>Switch to the <code>Add-ons</code> tab</li> <li>In the search field (with the little spyglass) enter \"X3D\", the list should   get reduced to just one entry</li> <li>Enable the checkbox left of \"Import-Export: Web3D X3D/VRML2 format\"</li> <li>Close the preferences window (it saves the settings automatically)</li> <li>Under <code>File -&gt; Import</code> there should now be a new entry <code>X3D Extensible 3D (.x3d/.wrl)</code></li> </ul> <ol> <li> <p>Importing data always adds to the current scene. So start with an empty scene, i.e. delete all objects. </p> </li> <li> <p>Make sure Blender is set to use <code>Cycles</code> as the renderer. For this,    switch to the Render tab  in the properties area.    Check the <code>Render Engine</code> drop-down, it should be set to <code>Cycles</code>.</p> </li> <li> <p>Import file <code>glyphs.x3d</code> using <code>File &gt; Import &gt; X3D Extensible 3D</code>, using the    importer settings (on the right side of the window when selecting the file to import): <code>Forward: Y Forward</code>,    <code>Up: Z Up</code>. The latter will make sure the imported model is oriented correctly.</p> </li> <li> <p>This X3D file holds a scene exported from ParaView. Check out the objects in the scene to get some idea of what it contains.</p> </li> <li> <p>Delete all the lights in the scene to clear everything up a bit. Add a single sun light in return.</p> </li> </ol>"},{"location":"basics/importing_data/exercise_vertex_colors/#inspecting-the-vertex-colors","title":"Inspecting the vertex colors","text":"<p>This 3D model has so-called \"vertex colors\". This means that each vertex of the geometry has an associated RGB color, which is a common way to show data values in a (scientific) visualization.</p> <p>There are a few ways to inspect if, and what, vertex colors a model has. First, there is the so-called Vertex Paint mode. In this mode vertex colors are shown when they are available and can even be edited (\"painted\").</p> <p>To enable Vertex Paint mode:</p> <ol> <li>Select the 3D arrows in the scene (as the only single selected object)</li> <li>Open the Mode pie menu with <code>Ctrl-TAB</code> and switch to <code>Vertex Paint</code>. An alternative is    to use the menu showing <code>Object Mode</code> in the upper-left of the 3D view header and select <code>Vertex Paint</code> there.</li> <li>The 3D View should now show the arrow geometry colored by its vertex colors. The colors shown are    velocity values from a computational flow simulation, using the well-known rainbow color scale (low-to-high value range: blue \u2192 green \u2192 yellow \u2192 orange \u2192 red)</li> </ol>"},{"location":"basics/importing_data/exercise_vertex_colors/#altering-vertex-colors","title":"Altering vertex colors","text":"<p>You might have noticed two things have changed in the interface: 1) the cursor is now a circle, and 2) the tool shelf on the left now shows color operations (paint brush = Draw, drop = Blur, ...)</p> <p>As this is Vertex Paint mode you can actually alter the vertex colors. This works quite similar to a normal paint program, like Photoshop or the GIMP, but in 3D. Although it may not make much sense to change colors that are based on simulation output (like these CFD results) it can  still be interesting to clean up or highlight vertex-colored geometry in certain situations.</p> <ol> <li>Experiment with vertex painting: move the cursor over part of the arrow geometry, press and hold <code>LMB</code> and move the mouse. See what happens.</li> <li>Switch to the Tool settings  tab in the properties area on the right-hand side of the window </li> <li>You can change the color you're painting with with the colored box directly right of the <code>Draw</code> in the bar at the top of the 3D view area (it's also shown and editable in the bar at the top of the 3D view). Click the color to bring up the color chooser. You can also change the radius and strength settings to influence the vertex painting.</li> <li>Change back to <code>Object Mode</code> using the <code>Ctrl-TAB</code> mode menu when you're done playing around. Note that the arrows no longer show the vertex colors.</li> </ol>"},{"location":"basics/importing_data/exercise_vertex_colors/#rendering","title":"Rendering","text":"<p>The second way to use vertex colors is to apply them during rendering.</p> <ol> <li>If you've screwed up the vertex colors really badly in the previous steps you might want to reimport the model...</li> <li>Make the 3D arrows in the scene the single selected object</li> <li>Switch to the <code>Object Data</code>  tab in the properties</li> <li>Check that there is an entry \"Col\" in the list under <code>Color Attributes</code>. A model can have multiple sets of vertex colors, but this file has only one set called \"Col\", which has domain <code>Face Corner</code> and type <code>Byte Color</code>.</li> </ol> <p>Now we will set up a material using the vertex colors stored in the \"Col\" layer.</p> <ol> <li>Go to the Material tab </li> <li>Select the material called \"Material\" in the drop-down list left of the New button. This set the (grey) material \"Material\" on the arrows geometry.</li> <li>Press <code>F12</code> (or use interactive render) to get a rendered view of the current scene.</li> </ol> <p>You'll notice that all the geometry is grey/white, i.e. no vertex colors are used. We'll now alter the material to use vertex colors.</p> <ol> <li>In the settings of the material there is a field called \"Base Color\" with a white area right of it. This setting controls the color of the geometry.</li> <li>Click the button left of the color area (it has a small yellow circle in it)</li> <li>Pick <code>Attribute</code> from the left-most column labeled <code>Input</code>. This specifies that the material color should be based on an attribute value.</li> <li>Base Color is now set to <code>Attribute | Color</code>. Directly below the entry there is a <code>Name</code> field. Pick \"Col\" from the list. This specifies that the attribute to use is called \"Col\" and comes from the mesh geometry (i.e. our vertex colors).</li> <li>Now render the scene again</li> <li>The rendered image should now be showing the arrow geometry colored by vertex colors</li> </ol> <p></p>"},{"location":"basics/importing_data/exercise_your_data/","title":"\ud83d\udcbb Your own data","text":"<p>Info</p> <p>If you do not have data that you want to import in Blender then you can skip this part.</p> <ol> <li> <p>Think about your own data</p> <ul> <li>What is the goal for importing the data?</li> <li>What visual representation(s) of the data do you aim for?</li> <li>What scene object types do you need for this?</li> <li>What approach would you use to get it into Blender?</li> <li>Challenges?</li> <li>Problems?</li> </ul> </li> <li> <p>Try to import your own data, or a representative subset, using your chosen approach. </p> </li> </ol>"},{"location":"basics/importing_data/introduction/","title":"Introduction","text":"<p>This chapter will present a lot of information on getting data into Blender through importing. It will describe the overall approach, available file formats and their relative strengths/weaknesses and look closer into handling specific types of data, specifically point data and volumetric data.</p> <p>Most of this chapter consists of the video presentation below, which covers quite a few subjects. After you are done viewing the video there is a first exercise  on vertex colors, which uses data we provide. While the second exercise is more of a  guideline for when you want to import your own data.</p> <p>As mentioned in the presentation the PDF slides for this chapter contain some more reference material on getting data from ParaView, VisIt and VTK.</p> <p>Point cloud primitive (3.1+)</p> <p>As shown in the video, one way to render point data is to use instancing for placing a simple primitive like a sphere at each point location. Working with such instanced geometry is somewhat limited, as it introduces a hit on performance and memory usage, both for interactive work in the user interface, as well as rendering in Cycles.</p> <p>Starting with Blender 3.1 Cycles now has dedicated support for rendering large numbers (millions) of points as spheres directly. However, there is currently in 3.1 no way to directly create a point cloud primitive by importing a file, and the only alternative is using Geometry Nodes to generate a point cloud primitive from a vertex-only mesh. But Geometry Nodes are not a topic in this Basics part of the course.</p> <p>Availability of importers/exporters (Linux distributions)</p> <p>When using the official Blender binaries from https://www.blender.org all supported importers and exporters will be included.</p> <p>But especially when using a Linux distribution's Blender package some features might not be available, usually to libraries not being enabled when the package was built. For example, currently (May 2022) on Arch Linux the USD import/export support is not available in the Arch Blender package.</p> <p>If you run into such issues, please download and use the official binaries instead.</p> <p>Drag-and-drop for import (4.1+)</p> <p>Since Blender 4.1 you can drag-and-drop a file to import on the 3D Viewport (or Outliner). This will then show the import dialog for the specific file format being imported. Currently supported file formats are Alembic, Collada, Grease Pencil SVG, OBJ, OpenUSD, PLY, and STL.</p> <p>Importing even works when dropping multiple files (of the same file type).</p>"},{"location":"basics/rendering_lighting_materials/composition/","title":"Composition","text":"<p>Below you'll find supplementary video on image composition. It is supplementary in a way that you won't need it to do the exercises but it might help you with your future Blender renders. This video will give you some practical guidelines that could give your final renders the extra edge it needs to stand out:</p>"},{"location":"basics/rendering_lighting_materials/gpu_rendering/","title":"GPU-based rendering","text":"<p>In general using Cycles with GPU-based rendering is a lot faster than rendering on a multi-core CPU. For example, here are render times on one of our workstations for the scene with the 3 monkey heads used in the video from the last chapter (showing camera settings and depth-of-field):</p> Type Device Render time* CPU Intel Core i9-12900H 9.34 s GPU NVIDIA RTX3080 Ti 1.32 s * 960x540 pixels, 128 samples per pixel <p>On this particular scene, with these settings and on this hardware using GPU rendering is roughly 7.1x faster! However, only by making a comparison on your particular system can you really find out if GPU rendering is a good option for you (for example, you might not have a very powerful GPU in your laptop or workstation).</p> <p>Apart from performance there are some other aspects to consider with GPU rendering:</p> <ul> <li>When doing a GPU render your desktop environment might become less responsive, although this has become less of a problem with recent Blender versions</li> <li>A GPU usually has less memory available, which might cause problems with really large scenes</li> </ul> <p>For the Cycles renderer the way to switch between CPU and GPU rendering is under the scene settings:</p> <p></p> <p>If you don't see a <code>GPU Compute</code> entry there then you first need to enable a GPU device under the global  system preferences. Go to the Preferences window (<code>Edit &gt; Preferences</code>) and then the <code>System</code> tab. The settings available under <code>Cycles Render Devices</code> are somewhat dependent on the hardware in your system but should look a little like this:</p> <p></p> <p>See below for the meaning of the different (system-dependent) options.</p>"},{"location":"basics/rendering_lighting_materials/gpu_rendering/#system-specific-settings","title":"System-specific settings","text":"<p>GPU rendering in Blender has slightly different support depending on whether you're on Windows, Linux or macOS. Below, we summarize the options you can encounter. The most up-to-date official reference for this information is this page from the Blender manual.</p>"},{"location":"basics/rendering_lighting_materials/gpu_rendering/#windows-linux","title":"Windows, Linux","text":"<p>By default <code>None</code> will be active, meaning no GPU acceleration is used for rendering and it all happens on the CPU.</p> <p>In general, on a PC/Laptop with an NVIDIA GPU the <code>CUDA</code> option is available and to be preferred, although <code>OptiX</code> might work well as a faster alternative for NVIDIA GPUs with RTX cores.</p> <p>On systems with an AMD GPU the option <code>HIP</code> might be available. On systems with an Intel GPU the option <code>oneAPI</code> might be available, with limited support Intel Arc GPUs. In that case it is definitely worth a try to see the performance of those options.</p>"},{"location":"basics/rendering_lighting_materials/gpu_rendering/#macos","title":"macOS","text":"<p>For macOS systems only the <code>Metal</code> option will be available, apart from the default <code>None</code>. Metal is supported on Apple computers with Apple Silicon, AMD and Intel graphics cards. </p> <p>macOS 13.0 or newer is required to support all features and graphics cards. </p> <p>GPU rendering versus acceleration</p> <p>This above info is about GPU rendering in Cycles on macOS, which is different from GPU acceleration for the Blender user interface and EEVEE rendering (see below). So even though your macOS system might not provide GPU rendering in Cycles, it might still work fine for Blender usage with a GPU-accelerated 3D viewport, while using CPU-based rendering for Cycles.</p>"},{"location":"basics/rendering_lighting_materials/gpu_rendering/#limitations","title":"Limitations","text":"<p>Some limitations do apply, when using GPU rendering. But these are very specific situations that you probably won't run into anytime soon.</p>"},{"location":"basics/rendering_lighting_materials/gpu_rendering/#faq","title":"FAQ","text":"<p>See the FAQ in the Blender manual  for some common issues when using GPU rendering with Cycles.</p>"},{"location":"basics/rendering_lighting_materials/gpu_rendering/#a-thing-called-eevee","title":"A thing called EEVEE?","text":"<p>When consulting other Blender materials, specifically on rendering, you may see references to EEVEE. This is another render engine available in Blender, which is different from the Cycles engine we will be using in this course. </p> <p>Even though EEVEE is meant for fast and highly interactive rendering work, even more so than the Cycles preview render  we showed so far, we do not use EEVEE in this course. The reasons for this are:</p> <ul> <li>We personally find Cycles to be more intuitive to work with and explain, as it is built  around the path tracing algorithm, which is easy to understand while providing a very versatile set of rendering and lighting features. EEVEE's rendering setup is somewhat more complex, as it uses a combination of different techniques that needs more separate controls.</li> <li>Cycles can render both on CPU and GPU, whereas EEVEE can only render on a GPU (more specifically, it needs OpenGL)</li> <li>EEVEE doesn't support headless rendering on Windows and macOS (and only Linux since 3.4), i.e. when starting a Blender render from the command-line without showing the user interface. This is especially relevant when rendering long animations on an HPC system, or other cluster environment without a GPU-accelerated display environment.</li> <li>Cycles is more feature-complete, whereas EEVEE has some limitations compared to Cycles, although the situation improves with each Blender release</li> <li>Although Cycles and EEVEE are getting closer in features with every Blender release they are still not fully equivalent. They also use separate controls in the UI for certain features. This would mean having to dedicate extra course material on these differences</li> </ul> <p>If you do like more information on EEVEE then please check this section in the Blender manual.</p>"},{"location":"basics/rendering_lighting_materials/introduction/","title":"Introduction","text":"<p>This part of the course is all about the aesthetics, the last part of the pipeline. Now that you have the basic knowledge and you are able to import some scientific data within Blender, the final thing that is left is to make the final image look great. What will the surface of your 3D-model look like, what tangible texture and colors it has, how it will be illuminated and finally how it will be composed. All these things go hand-in-hand and these things need to be in balance to create an aesthetically pleasing image.</p> <p>Before you start with the exercises the following video will give you the theoretical and practical background to make these exercises. In this video there are some Blender walk-throughs, if you want to follow along you can use the walk-through files in the <code>walkthroughs/basics/06-rendering-lighting-and-materials</code> directory.</p> <p>Blender 4.1 changes</p> <p>The walkthrough has been patched to reflect the changes for Blender 3.1, however the current version (Blender 4.1) used for the this course has some additional changes and new features. Most are minor UI changes which will not conflict with the course material, however the material section of the walktahrough does have some changes. These changes are:</p> <ul> <li>In the materials walkthrough, the Principled BSDF menu subdivision into sub-menus giving it a better overview. Some settings like <code>Subsurface</code>, <code>Transmission</code> and <code>Emission</code> are now found under their corresponding sub-menu and then under either <code>Weight</code> or <code>Color</code>.</li> <li>The <code>Transmission roughness</code> is missing from the Principled BSDF (https://projects.blender.org/blender/blender/issues/114956).</li> <li>The denoiser is significantly better and is recommended when you want to have fast renders.</li> </ul>"},{"location":"basics/rendering_lighting_materials/rlm_assignment/","title":"\ud83d\udcbb Rendering, lighting and materials","text":"<p>Open the <code>rlm_assignment.blend</code> file and you'll see several objects in the scene: a ground plane, a plateau, Suzanne (the monkey head) and 3 knots.</p> <p>The goal of this assignment is to place some lights, set the camera parameters to your liking, add materials to the objects and render the final image. We'll do this in steps.</p> <p>Tip</p> <p>To view your result with realistic lighting and materials use the <code>Shading</code> pie menu, which opens with the <code>Z</code> key:</p> <ul> <li>Option <code>Rendered</code> shows realistic lighting and materials, with slower interaction</li> <li>Option <code>Solid</code> shows simple colors and lighting, with faster interaction</li> </ul>"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#lighting-creating-light-sources","title":"Lighting - Creating light sources","text":"<p>To see what we are doing in Rendered shading (<code>Z</code>-<code>Rendered</code>) we first need to add the lighting.</p> <ol> <li>Add one or two sun lights by either using the 3D view menu in the header (<code>Add &gt; Light &gt; Sun</code>) or use <code>Shift-A &gt; Light &gt; Sun</code> in the 3D view</li> <li>Try to position and rotate the lights so that they light the objects under a bit of an angle (<code>G</code> and <code>R</code> keys).</li> <li>Before we change the appearance of the lights we need to switch to <code>Rendered</code> using the <code>Shading</code> pie menu (<code>Z &gt; Rendered</code>)</li> <li>Now adjust the <code>Color</code> and <code>Strength</code> settings under the Object Data properties tab  in the Properties panel, perhaps try to give one the lights a warm yellowish sun-like color and the other a more less strong blueish and cold color.</li> <li>In the same properties panel tab, try to adjust the <code>Angle</code> (or <code>Radius</code> or <code>Size</code> for the other light types) of the sun light and see how it affects the shadows. Small angles (or radii or sizes) create hard shadows, which are ideal to see minor details and large angles (or radii or sizes) create soft shadows, which is more suited to reduce the overall contrast and make it less straining on the eye.</li> <li>Now in the same properties editor tab, try out some different lamp types (<code>Point</code>, <code>Sun</code>, ...) to experiment with the different lighting effects they produce.</li> </ol> <p>Bonus: If setting up the lamps is too cumbersome, you can go to the <code>World</code> tab  in the properties editor and click the little globe drop-down menu button at the top and select the <code>HDRIWorldLighting</code>. This will enable predefined environment lighting using a 360 image of somebody's living room. Do make sure that you de-activate the lamps ( in the Outliner) or remove them to see the full effect.</p>"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#camera-setting-the-starting-point-of-the-light-paths-or-rather-camera-paths","title":"Camera - Setting the starting point of the light paths (or rather camera paths)","text":"<p>With the lighting setup, we can now see what each of the camera settings does. Or from the light ray paths perspective: configure the starting point of the light rays.</p> <ol> <li>First you need to be in the camera view to be able to see the changes of the camera settings by selecting with the <code>View</code> pie menu (<code>`</code> button) the <code>View Camera</code> option or through the 3D view menu in the header (<code>View &gt; Viewpoint &gt; Camera</code>). The former way is a toggle interaction so when you are already in the camera view you will toggle it off.</li> <li>Try changing the camera's focal length. For this, select the camera Camera and go the <code>Lens</code> settings in the Object Data properties tab  in the properties panel. There you can find the <code>Focal Length</code> setting, try for example values 18, 50 and 100 and see what effect this has. Notice that when you set the <code>Focal Length</code> to a lower value you might see clipping (the scene is cut off from a certain distance). This can be changed by setting the <code>Clip Start</code> in the same <code>Lens</code> section to a lower value, e.g. 0.01. Finally set the focal length to the desired value.</li> <li>Next we are going to bring the focus to a chosen object in the scene with the depth of field settings. For this, select the camera, scroll down in the Object Data properties tab  in the properties panel to the <code>Depth of Field</code> settings. Check the check-box before <code>Depth of Field</code> to activate the depth of field. Now try to set the <code>Focus on Object</code> value to the <code>Suzanne</code> object and test different values for the <code>Aperture &gt; F-Stop</code> setting.</li> <li>When you are done, disable depth of field again. This makes the material editing easier.</li> </ol> <p>If the lighting gives the desired effect looking through the configured camera then you can give the objects the look you want with materials in the next section.</p>"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#materials-how-will-the-light-paths-bounce","title":"Materials - How will the light paths bounce?","text":"<p>To design how the light is reflected or refracted off the objects you are going to give each object a different material.</p> <ol> <li>For each object (including ground plane and plateau):<ul> <li>Select the object and go to the Material tab  in the properties editor.</li> <li>In the Material tab click the <code>New</code> button.</li> <li>Then under the <code>Surface</code> section set the <code>Surface</code> parameter to either <code>Diffuse BSDF</code>, <code>Glossy BSDF</code> or <code>Principled BSDF</code>.</li> </ul> </li> <li>Try to play with the material settings <code>Roughness</code> and <code>Color</code> (the latter is called <code>Base Color</code> for the Principled BSDF)</li> </ol> <p>Bonus: When you feel that the roughness and the color alone didn't give you the look that you want with the <code>Principled BSDF</code> then also try and have a look at the other, in the slides, mentioned settings, Metallic, Transmission, IOR and Subsurface.</p>"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#rendering-creating-your-final-image","title":"Rendering - Creating your final image","text":"<p>Lights, camera, (materials,) set aaaaaaand action!... Now you will set the desired render settings to generate the final image!</p> <ol> <li>Go to the properties editor and set the following settings:<ul> <li>Render properties tab <ul> <li>Set <code>Device</code> to <code>GPU Compute</code>. If your device doesn't have a (powerful) GPU set it to <code>CPU</code>.</li> <li><code>Sampling</code> section: set <code>Render &gt; Samples</code> to 128</li> <li><code>Light Paths</code> section: set <code>Clamping &gt; Indirect Light</code> to 1.0</li> </ul> </li> <li>Output tab <ul> <li><code>Format</code> section: set <code>Resolution</code> to 1920x1080, 100%.</li> </ul> </li> </ul> </li> <li>If everything is set, press <code>F12</code>.</li> </ol> <p>Now the Image editor will replace the 3D view and your image will slowly be rendered in parts called \"tiles\".</p> <ol> <li>Finally when the image looks the way you want don't forget to save it! In the Image editor go to the <code>Image</code> menu and click on <code>Save As...</code> and choose a location and save the image.</li> </ol>"},{"location":"basics/rendering_lighting_materials/rlm_assignment/#performance-speed-up-those-renders","title":"Performance - Speed up those renders","text":"<p>Now that we know how to improve the look of the scene and save the final render we will improve the speed of the render.</p> <ol> <li>Write down the render time shown in the upper left corner of the Image editor (Example: <code>Frame:1 | Time:00:09:84 | Mem:6.09M, Peak: 164.29M</code>).</li> <li>Close the Image editor if it is still open.</li> <li>Change the following settings in the Render properties tab :<ul> <li><code>Sampling</code> section: set <code>Render &gt; Samples</code> to 32</li> <li><code>Sampling</code> section: turn on the denoiser with, <code>Render &gt; Denoise</code>.</li> </ul> </li> <li>Now press <code>F12</code> again to render another image.</li> <li>As you can see when comparing the render time of our previous render and this one, this one is significantly faster.</li> </ol> <p>Render quality when using denoise features</p> <p>One thing to keep in mind is that when you are using the denoise feature you will lose a little detail.</p> <p>Noise Threshold</p> <p>Blender 3.0 introduced another feature to reduce the render times called Noise Threshold. Turning it on and giving it a value between 0.1 to 0.001 will prematurely terminated the sampling when a pixel reaches a certain noise threshold and by doing so reduces the render time.</p>"},{"location":"basics/simple_mesh_editing/introduction/","title":"Introduction","text":"<p>This chapter will introduce the basic mesh editing tools available within Blender. The basic mesh editing will be performed with the so called modifiers, these modifiers make it relatively easy to do large mesh editing operations that can greatly impact the visual representation of your 3D-models. Below you'll find a video that will give you a theoretical introduction followed by a practical walk-through in Blender. If you want to follow along with the walk-through you can find the Blend files in the walk-through directory <code>walkthroughs/basics/04-simple-mesh-editing</code>.</p> <p>Blender 4.1 modifier menu changes</p> <p>The walkthrough video was recorded with Blender 2.92, fundamentally the content is the same for Blender 4.1. Visually however there are some minor changes and one of them should be mentioned. The <code>Add modifier</code> button will now show a context menu where the sub-menu hierarchy is the same as the column hierarchy in Blender 2.92. Visually, everything else relevant to this walkthrough is the same.</p> <p></p> <p>After you watched the video about simple mesh-editing you are ready for the exercises!</p>"},{"location":"basics/simple_mesh_editing/sme_assignment/","title":"\ud83d\udcbb Simple mesh editing","text":"<p>In this exercise you will use some mesh modifiers on an iso-surface of a CT scan of a fish and try to see if you can reveal the insides.</p> <p>Once you opened the exercise blend file <code>sme_assignment.blend</code> you'll see the fish iso-surface above a plane.</p> <p>Info</p> <p>This exercise uses a somewhat large 3D model, at around 155,000 triangles. On most modern PCs and laptops this should not pose a problem, so it is a good test to see if your system is able to handle this (which might indicate some limitation).</p>"},{"location":"basics/simple_mesh_editing/sme_assignment/#decimate-reducing-the-triangles","title":"Decimate - Reducing the triangles","text":"<p>The fish 3D model, for your convenience, has been divided into two parts: the fishskin and fishbones. Combined, this model has a large amount of triangles (155k for the fishskin and 573k for the fishbones). On lower end devices this can slow everything down to a crawl. In order to be able to add modifiers or edit the meshes with reasonable interactivity you first need to decimate the meshes. The decimation is for reducing the number of triangles, by merging adjacent triangles together into one, iteratively.</p> <ol> <li>Select the fishskin by clicking on the fishskin with <code>LMB</code>.</li> <li>Once selected go the Modifiers tab  in the properties editor.</li> <li>Click <code>Add Modifier</code> and add the <code>Decimate</code> modifier (it's in the Generate sub-menu).</li> <li>Keep the decimation type set to <code>Collapse</code>, set the <code>Ratio</code> to <code>0.5</code> and press <code>Enter</code>. The mesh processing will take a couple of seconds but will immediately reduce the number of triangles to ~77k, which is visible in the modifier under <code>Face Count</code>. You can even reduce it to a lower number but it might affect the appearance and shape of the model negatively by creating hard edges on the surface.</li> <li>Once you are satisfied with the results press <code>Apply</code>, under the dropdown-menu arrow to the right of Decimate , or by pressing <code>Ctrl-A</code>, while focused on the Decimate modifier, to make the changes permanent. Again, this can take a few seconds.</li> <li>Now that the fishskin triangles have been reduced, select it and press <code>H</code> to hide it or click the  icon in the Outliner. This simultaneously hides the fishskin and reveal the fishbones.</li> <li>Preform the same steps the fishbones and try to reduce the triangle count significantly without affecting the appearance of the model.</li> <li>Now unhide the fishskin again for the next assignment by clicking the  icon.</li> </ol>"},{"location":"basics/simple_mesh_editing/sme_assignment/#smooth-ironing-the-creases","title":"Smooth - Ironing the creases","text":"<p>The geometry of the fishskin and the fishbones both look a bit rough because of the iso-surface extraction algorithm. If that is not desired, the rough edges can be smoothed out with the Smooth Modifier.</p> <ol> <li>Select the fishskin model by clicking on the fishskin with <code>LMB</code>.</li> <li>Go to the Modifiers tab  in the properties editor.</li> <li>Click <code>Add Modifier</code> and add the <code>Smooth</code> modifier (it's in the Deform sub-menu).</li> <li>Keep the <code>Factor</code> at <code>0.5</code> but increase the <code>Repeat</code> to <code>5</code>. Watch out with using the slider, every change re-triggers the modifier and when you accidentally slide to a high number it will take a while to calculate.</li> </ol> <p>Unfortunately you will notice that the Smooth modifier creates tears along the skin model, this conveniently revealed that the underlying mesh triangles are not fully connected and are present in connected patches. These patches stem from the creation of this model where the calculation of the geometry was done in multiple processors and each patch was created by a separate process, and then combined. This can be fixed in the Edit-mode but that will be covered in the Advanced course.</p> <ol> <li>The <code>Factor</code> is good as it is, but changing the value shows what kind of drastic effect it has.</li> <li>Once you are satisfied with the smoothness of the fishskin press <code>Apply</code> and try to do the same with the fishbones.</li> </ol>"},{"location":"basics/simple_mesh_editing/sme_assignment/#boolean-slicing-the-geometry","title":"Boolean - Slicing the geometry","text":"<p>If you wanna both show the inside of the fish with the context of the outside you can use slice through the fishskin model and reveal the insides of the fish by using a Boolean Modifier.</p> <ol> <li>Before you add the Boolean Modifier you first need to reveal the <code>fishskin</code> mesh object again by clicking the  icon in the Outliner.</li> <li>Select the <code>fishskin</code> mesh object and go to the Modifiers tab  in the properties editor to add a <code>Boolean</code> modifier (it's in the Generate sub-menu).</li> </ol> <p>Now that the Boolean modifier is added we still miss another 3D mesh object to perform the Boolean operation with. You are now gonna prepare the other mesh object.</p> <ol> <li>Move the mouse into the 3D view and add a new UV sphere with <code>Shift-A &gt; Mesh &gt; UV Sphere</code></li> <li>Scale and translate (<code>S</code> and <code>G</code> keys) the UV sphere so that it overlaps a part of the fish which you want to clip away.</li> <li>The UV sphere is now shown as a solid surface, which is not desirable when you want to use it for clipping because you want to see through it. You can change the representation of an object in the 3D view using the Object  properties under <code>Viewport Display</code>: set <code>Display As</code> to  <code>Wire</code>.</li> <li>Also when you want to look at the results in Rendered mode you need to make the sphere invisible using the <code>Ray Visibility</code> settings under <code>Visibility</code>: disable all check-boxes (Camera, Diffuse, Glossy, Transmission, Volume Scatter and Shadow)</li> </ol> <p>Now that you prepared the mesh object to preform the Boolean operation with, you can continue setting up the Boolean modifier.</p> <ol> <li>Select the <code>fishskin</code> mesh object and go to the Modifiers tab  in the properties editor to reveal the already added Boolean modifier.</li> <li>Now under <code>Object</code>, select the <code>Sphere</code> mesh object.</li> <li>Before we want to start moving the clipping Sphere around we want to change the <code>Solver</code> to <code>Fast</code>. This is more simpler and better performing solver and in our case, with the underlying broken patched mesh, also a better option since this solver is able to handle this type of geometry.</li> <li>Now if you select the <code>Sphere</code> object and translate and scale it over the <code>fishskin</code> mesh object you can clip away any desired part as the Boolean modifier updates in real time.</li> </ol> <p>As you might have noticed, this Boolean modifier does have some problems with this current mesh and placement of the clipping sphere must be precise. This off course is not always the cause but it should be kept in mind when working with the Boolean modifier.</p> <p>Finally you can view your results with Cycles with Rendered shading (<code>Z &gt; Rendered</code>) for better lighting and materials. Or you can give the camera a better position and make a nice final render.</p>"},{"location":"examples/black_outline/","title":"Giving your models a stylized black outline in EEVEE","text":"<p> Figure 1: examples of black outlines around some simple shapes. </p> <p>A black and possibly \u2018cartoony\u2019 or \u2018toon-like\u2019 style border around your models is a nice way to make them stand out with a bit of character. Note that the setup used below works with the EEVEE renderer, but not Cycles.</p>"},{"location":"examples/black_outline/#step-1-setting-the-scene-and-viewport","title":"Step 1: Setting the scene and viewport","text":"<p> Figure 2: changing the viewport to <code>Rendered</code> </p> <p>Start with the cube you always get with a new file in Blender. And you can switch the Viewport shading to <code>Rendered</code> (see Fig. 2), so we can see the results.</p>"},{"location":"examples/black_outline/#step-2-materials-properties","title":"Step 2: Materials properties","text":"<p> Figure 3: Adding a second material. </p> <p>Next we add a second material to the cube, see Fig. 3:</p> <ol> <li>Go to the Material tab in the properties editor</li> <li>Click on the <code>+</code> sign to add a new material slot</li> <li>Click on <code>New</code> to add a new material (we name it \"Rim material\")</li> <li>Under Surface, change the surface of the new material to <code>Emission</code></li> <li>Set the Color to Black. Or anything you want of course :)</li> <li>Tick the Backface Culling option under Settings. In step 3.3 below you we see why we tick this property</li> <li>Set the Shadow Mode to <code>None</code>. We do not want the border we are making to cast a shadow on the cube.</li> </ol>"},{"location":"examples/black_outline/#step-3-adding-a-modifier","title":"Step 3: Adding a modifier","text":"<p> Figure 4: adding a solidify modifier. </p> <p>The last thing we need to do is add a solidify modifier, see Fig. 4:</p> <ol> <li>Go to the Modifiers properties tab and add a Solidify modifier. The Solidify modifier takes the surface of any mesh and adds depth, thickness to it. In essence, we're creating our black outline using a thickened version of the original Cube object.</li> <li>Set the Thickness to -0.04 m, or another value that gives the look that you want. This will set the thickness of the black border. When using a negative value here the black border extends outwards around the object, while a positive value will cause the black border to go inwards on top of the object.</li> <li>Enable <code>Flip</code>, under the Normals section. This will orientate the normals of the faces added by the modifier to point inwards. Together with the backface culling, we set for the material, this means you only see the material on the inside of the faces that are added by the modifier.</li> <li>Set the Material Offset, under the Materials section, to 1. This tells Blender to use the second material, in the list of materials of the object (see the list in Fig. 3), for the geometry added by the solidify modifier.</li> </ol> <p>I hope this was helpful and that you will be creating crazy Kirby characters soon\u2026 or something more serious of course.</p> <p>Cheers, Ben</p>"},{"location":"examples/custom_data/part1/","title":"Part 1: loading data into attributes","text":"<p>In this 3-part example we will use scientific data to make a 3D visualization. A nice way to use your own data in Blender is to use attributes. Once you have your data as an attribute in Blender you can use it in Geometry Nodes and in your shaders. </p> <p>We will use data from the particle accelerator at CERN and make the visualizations shown in Fig. 1a and 1b below. The small blocks making up the showers indicate detectors surrounding the collision and the color of the blocks indicate the energy reading of the detector. The data shown is generated by the GEANT4 toolkit based on the Linear Collider Detector. We only show data from the calorimeter that is sensitive to particles that interact through the electromagnetic force (ECAL readings). Thus the showers you see, four in this case, could be from for example photons or electrons being absorbed in the calorimeter.</p> <p> Figure 1a: a visualization of particle showers created by collisions in the CERN particle accelerator </p> <p> Figure 1b: animated version of Fig. 1a showing several different showers </p> <p>I will explain how to create Fig. 1a and 1b. The first part, this page, covers how to get the data into attributes in Blender. The second part will show how to use attributes in Blender Geometry Nodes. And lastly, the third part will dive into using your attributes in shaders.</p>"},{"location":"examples/custom_data/part1/#the-data","title":"The data","text":"<p>We use generated data from the particle accelerator at CERN. The data is from simulations where electrons and positrons (the electron anti-particle) are collided with each other. Out of such collisions come multiple particles. These particles are absorbed by detectors surrounding the collision, called calorimeters. These detectors absorb the particle to measure for example their energies. Calorimeters do not absorb the particle in one go, but stop and interact with the particles through multiple layers of material and detectors. This causes a so called \u2018shower\u2019 of absorptions inside the many detectors that make up the calorimeter. The many detectors are located around the point of the original collision. The structure of the data coming from one calorimeter is a cube of dimensions 51x51x25 and each point in this data cube represents a detector that holds an energy value. Fig. 1a shows a representation of four \u2018showers\u2019 detected in four calorimeters surrounding a collision.</p> <p>You can get more information on the data from this recent paper by Khattak et al. 2022. And the data can be downloaded from Zenodo here. The data is stored in the Hierarchical Data Format. In order to open these .h5 files in Python you can use the <code>h5py</code> package. To open the file and select one shower in Python would look something like this:</p> <pre><code>import h5py\nwith h5py.File(filename, 'r') as f:\n    # This can be handy in order to see the keys available\n    print('Keys: %s' % f.keys())\n    # You can select the energies from one shower (with index 10) like this\n    one_shower = f['ECAL'][10] # one_shower will be of shape 51x51x25\n</code></pre>"},{"location":"examples/custom_data/part1/#scripting","title":"Scripting","text":"<p>Now, let's get right to it and look at the script that puts the data into a Blender attribute. First we import <code>numpy</code> and <code>bpy</code>. Next we use NumPy to load a data cube for one shower.</p> <pre><code>import os\nimport numpy as np\nimport bpy\n\n# We load the data using numpy\nnr = '8_9_5'\nshower = np.load(os.path.join(directory, 'example_shower_'+str(nr)+'.npy'))\n</code></pre> <p>If we would run <code>print(shower.shape)</code> we can see the data cube has dimensions (51, 51, 25). Here the last index, with 25 entries, is the one in the radial, outwards direction of the shower. We will use the indices of the element in the data cube as vertices. Thus our model of one shower will contain 51x51x25 vertices.</p> <p>One cube is data from one shower detected in one calorimeter, but the collision center is surrounded by a cylinder made up of multiple calorimeters. We need to translate the coordinates in de cube (<code>local_coords</code>) to coordinates relative to the center line of the collider (called vertices). We first center the coordinate system of the cube to its center by shifting it by (-25, -25, 0). Also we rotate the data cube to point in the radial outwards direction by an angle <code>phi</code> (if you want the details on how the data is oriented, please see the paper). </p> <p>Lastly we translate the data radially outwards by <code>R_tube</code> to make room for the inner cylinder where the collision takes place. And we must not forget to save the energy values from the data cube in <code>energiesFloat</code>.</p> <pre><code># This is the angle from the x axis. We take this random\nphi = (random.uniform(0,360)/360) * 2*math.pi\n# Distance from the origin for the shower\nR_tube = 0.5 \n\n# The local_coords are the coordinates in the data cube\n# before rotation\nlocal_coords, vertices, energiesFloat = [], [], []\nfor index, elem in np.ndenumerate(shower):\n    # We save the energy of the current detector\n    energiesFloat.append(elem)\n    # We use the indices of the cube as local coordinates\n    # of the data. We center the data around the middle of\n    # the cube.\n    local_z = index[2]\n    local_y = index[1] -25\n    local_x = index[0] -25\n    # This next bit is to rotate the data cube and transform\n    # the local coordinates to the global coordinates of the\n    # scene. Please see the paper if you want to understand \n    # this in detail.\n    R = math.sqrt(local_x**2 + local_z**2)\n    if R == 0:\n      z = R_tube*math.cos(phi)\n      x = R_tube*math.sin(phi)\n      else:\n        if local_x &gt;= 0:\n            phi_local = math.acos(local_z/R)        \n        else:\n            phi_local = -1*math.acos(local_z/R)   \n\n        z = R_tube*math.cos(phi) + R*math.cos(phi + phi_local)\n        x = R_tube*math.sin(phi) + R*math.sin(phi + phi_local)\n\n      y = local_y\n      # And save the coordinates of the vertices.\n      vertices.append((x,y,z))\n</code></pre> <p>We can now make the object and mesh in Blender and use <code>from_pydata()</code> to create the geometry. We also link the new object to a collection. If you are new to making geometry through Python code, see this section in the Advanced materials.</p> <pre><code># Create a new mesh\nob_name = 'shower_'+str(nr)\nmesh = bpy.data.meshes.new(ob_name + '_mesh')\n\n# Create a new object and add the mesh\nob = bpy.data.objects.new(ob_name, mesh)\n\n# Add the geometry to the mesh\nmesh.from_pydata(vertices, [], [])\n\n# Link the object to the collection\nshower_collection_name = 'Showers'\nbpy.data.collections[shower_collection_name].objects.link(ob)\n</code></pre> <p>And lastly we will add the energy values as attributes to the Blender object. The Blender docs describe attributes as [...] a generic term to describe data stored per-element in a geometry data-block. Examples of attributes you might know are: Vertex groups, UV maps and Color Attributes. But, as we will do now, you can also create your own custom attributes.</p> <p>The way I think about it is that attributes allow you to save data (for example values or vectors) for every element (say per vertex, edge or for example face) of an object. In our example every detector reading is a vertex and for every vertex we want to save the energy readout as a float. In script, the attributes are stored in ob.data.attributes and a new attribute can be created by calling ob.data.attributes.new. The type of the attribute will be float and its domain will be point. The point domain means we store a data value for every vertex. For example, other domains let you store values per edge or face.</p> <pre><code># Create a new attribute for the object\nob.data.attributes.new(name=\u2019myAttributeFloat\u2019, type=\u2019FLOAT\u2019, domain=\u2019POINT\u2019)\n\n# Store the energies in the new attribute\nob.data.attributes['myAttributeFloat'].data.foreach_set('value', energiesFloat)\n</code></pre> <p>The data in <code>ob.data.attributes['myAttributeFloat']</code> is of the type <code>bpy_prop_collection</code>. The elements of the collection are of the type <code>bpy_struct</code> and in our case where we have floats it is of the type <code>FloatAttributeValue(bpy_struct)</code>. Because of this, setting the elements of a collection is somewhat involved and the preferred way to do it is through the <code>foreach_set</code> method. Have a look at the documentation for collections to learn more. Now you can run the script and you should see something like Fig. 2.</p> <p> Figure 2: the vertices generated from the 51x51x25 sized data cube. </p>"},{"location":"examples/custom_data/part1/#inspecting-attributes-in-blender","title":"Inspecting Attributes in Blender","text":"<p>There are at least two easy ways to inspect attributes in Blender. The first is through the Properties Editor and its Object Data Properties tab (see Fig. 3). There you see a list of standard attributes like Vertex Colors, but under Attributes you see the attribute <code>myAttributeFloat</code> that we just made. You also see that per vertex it stores a float.</p> <p> Figure 3: you can inspect the attributes of your model in the Properties Editor and the Object Data Properties tab. </p> <p>If you want to see the values stored in the attribute, there is another way to inspect attributes. This is through the Spreadsheet editor which is by default shown in the Geometry Nodes Workspace. In Fig. 4 you can see the spreadsheet and the attribute <code>myAttributeFloat</code> that we just made. You can see that it has a value for every vertex in the cube. We will talk more about this spreadsheet in part 2, which covers using Attributes in Geometry Nodes.</p> <p> Figure 4: you can inspect attributes in Blender in the Spreadsheet Editor, by default found in the Geometry Nodes Workspace. </p>"},{"location":"examples/custom_data/part1/#closing","title":"Closing","text":"<p>This was the first part in a series of three. We have learned how we can create attributes from our own custom data. In Fig. 1a we actually show four showers from four different data cubes, instead of one as in Fig. 2.</p> <p>In the next part we will learn how to use the attributes in Geometry Nodes in order to create, alter and remove geometry. This wil be the next step in making Fig. 2 look like Fig. 1a! Hope this was helpful.</p>"},{"location":"examples/custom_data/part2/","title":"Part 2: using custom attributes in Geometry Nodes","text":"<p>In this second part of our three-part series, we will continue using scientific data to make a 3D visualization in Blender. In the first part of this series, we loaded data into Blender attributes using the Python API. This second part will show you how to use your custom attributes within Geometry Nodes to create, delete and alter geometry.</p> <p> Figure 1: The animation we're working towards </p>"},{"location":"examples/custom_data/part2/#recap-of-what-we-did-in-the-first-part","title":"Recap of what we did in the first part","text":"<p>In the previous part we used particle collision data from the CERN accelerator, and we ended up with many vertices in a cube-like arrangement. Furthermore, every vertex has a float value associated with it through the attribute <code>myAttributeFloat</code>. This value represents the energy recorded by the detector belonging to the vertex. See Figure 2 below:</p> <p> Figure 2: our starting point for this part. On the left you see the very useful Spreadsheet editor included in the Geometry Nodes workspace. </p>"},{"location":"examples/custom_data/part2/#if-you-are-totally-new-to-geometry-nodes","title":"If you are totally new to Geometry Nodes","text":"<p>This series is not meant as an introduction to Geometry Nodes, but is to show an example of how you can use Geometry Nodes to manipulate geometry and your own custom data. If you are new to Geometry Nodes you can read the relevant chapter in the advanced part of the course. Or have a look at the first free video of a course from Blender Studio. That will give you enough background for now.</p>"},{"location":"examples/custom_data/part2/#three-step-plan-towards-the-animation","title":"Three-step plan towards the animation","text":"<p>In three steps we will create the basis for the animation in Fig. 1:</p> <ol> <li>We remove detectors/vertices that did not detect anything.</li> <li>We replace the remaining detectors/vertices with cube geometry. We do this so that we can shade them in the next part.</li> <li>We want to be able to make detectors (dis)appear so that we can make the animation as in Fig. 1.</li> </ol>"},{"location":"examples/custom_data/part2/#step-1-removing-non-detections","title":"Step 1: removing non-detections.","text":"<p>Many vertices seen in Fig. 2 represent detectors that detected no energy from a particle. We want to remove those vertices that have a <code>myAttributeFloat</code> value lower than a certain threshold. We can remove geometry using the Delete Geometry node. In this case we want to remove vertices, so we set it to the <code>Points</code> domain. We now need to feed in a list of booleans into the <code>Selection</code> socket, which determines what gets deleted. See Figure 4 below.</p> <p> Figure 4: first version of the Geometry Node setup we are building. Here we use the attribute <code>myAttributeFloat</code> to remove detectors that did not measure any energy from particles. </p> <p>To get access to the attribute <code>myAttributeFloat</code> we use a Named Attribute node and fill in the attribute name. Next we want to compare the <code>myAttributeFloat</code> value to a threshold and for this we use a Compare node and set its mode to <code>Less Than</code> and feed the attribute into socket A, and set socket B to a threshold value of 0.1.  If we now connect the Delete Geometry node to the Group Output node we see the result as in Fig:</p> <p> Figure 5: the result from the Geometry Node network seen in Fig. 4, showing only the vertices that had a detection value over the treshold </p>"},{"location":"examples/custom_data/part2/#step-2-replace-vertices-with-cubes","title":"Step 2: replace vertices with cubes.","text":"<p>To be able to shade every detector, we generate a piece of geometry, in this case a cube, at the location of every vertex/detector. We do this by creating an instance of a cube at every vertex using the Instance on Points node. But because this node requires a point cloud as input we first convert our vertices coming out of the Delete Geometry node into points using the Mesh to Points node. </p> <p> Figure 6: second version of the Geometry node we are building. We now make instances of cubes from the vertices we had. </p> <p>We add a Cube node which we connect to the <code>Instance</code> socket, which results in this output:</p> <p> Figure 7: here is the result from the Geometry Node in Fig. 6, the vertices are now turned into cubes </p>"},{"location":"examples/custom_data/part2/#step-3-make-detectors-appear-and-disappear","title":"Step 3: make detectors appear and disappear","text":"<p>To make the animation seen in Fig. 1, we also need to make the cubes appear and disappear. We will make them appear from the center outwards (in the radial direction). Thus, we need to be able to cut away detectors located at a radius larger than a certain threshold. And instead of hard-coding this threshold, we will make a new Geometry Node input for the threshold, which will make it possible to animate the threshold.</p> <p>We can access the position of the detectors by adding a Position node (see Fig. 8). We retrieve the length of the points (i.e. distance from the origin) using the Vector Math node and set its mode to <code>Length</code>. We will feed it into a Compare node, set to <code>Greater Than or Equal</code>, which will be fed into a new Delete Geometry node together with the threshold value:</p> <p> Figure 8: version three of the Geometry Node we are making. We can now also remove detectors at locations on the x axis larger and smaller than a certain threshold. </p> <p>We now need to implement the threshold that determines which detectors are removed. We will not hard-code this threshold like we did for the other Compare and Delete Geometry nodes. We will make a new Group Input node instead and we will use the socket that has no name. If we connect the unnamed socket to the B socket of the Compare node, we will see the unnamed socket is given a default name (for example B).</p> <p>The new input also shows up in the Modifier Properties tab in the Properties editor, at the right side of the window (see Fig. 8). You can also press the <code>N</code> key in the Geometry Node Editor window to open the Sidebar (or go to the View tab and tick the Sidebar option) and then navigate to the Group tab in the Sidebar to see the list of inputs and outputs of this Geometry Node modifier (also see Fig. 8). Here you will see, and have the possibility to rename, the new input we just created. I named it <code>Cutoff for animation</code>. You can see an example result in Fig. 9.</p> <p> Figure 9: result from the Geometry Node in Fig. 8, with cubes further away from the origin than the threshold removed </p> <p>In the next part we will use the fact that you can animate the inputs of modifiers. We can click the small dot next to the value of <code>Cutoff for animation</code> in the Modifier Properties tab to start animating the threshold. For now, you can play with the value of <code>Cutoff for animation</code> to see what it looks like.</p>"},{"location":"examples/custom_data/part2/#a-last-addition","title":"A last addition","text":"<p>In the next part we want to give all the detectors/cubes we just made a different color depending on the value of the <code>myAttributeFloat</code> attribute. But now all the cubes are an instance of one and the same cube. We need to make every instance a real geometry and we do this using the Realize Instances node. Now this does not change how the scene looks just yet, but it is essential for the shading we do in the next blog of this series.</p> <p> Figure 10: we use Realize Instances to turn all the cube instances into real geometry data. </p>"},{"location":"examples/custom_data/part3/","title":"Part 3: using custom attributes in your shaders","text":"<p>In this third and last part of our series we will use our custom attributes in a shader to give the particle shower some color and shine.</p> <p> Figure 1: visualization of five showers from a particle collider at CERN. </p> <p>We start by going to the Shading workspace and we go to the Material Properties tab in the Properties Editor on the right. We click on the <code>+</code>-sign to add a material slot. We link the material slot to the Object (instead of the Object Data) using the menu encircled in red in Fig. 2. XXX why?</p> <p>Now we click the <code>New</code> button to make a new material. We will keep the shader simple and in the Shader Editor we delete the <code>Principled BSDF</code> and instead we add an <code>Emission</code> shader (see Fig. 2).</p> <p>Now we create an Attribute Node and link the <code>Color</code> socket to the <code>Color</code> socket of the Emission shader. We only need to fill in the name of the Attribute we want to use into the <code>Name</code> field in the node. For some reason Blender does not give a selection menu for this and you need to copy and paste the name of the attribute. One way to find the attribute name is by going to the Object Data Properties tab in the Properties Editor and go to the tab Attributes. Here is a list of custom attributes and we see the <code>myAttributeFloat</code> attribute we made in the first part.</p> <p> Figure 2: adding a new material and using our custom attributes to control the Emission shader. </p> <p>Now you have the shader shown in Fig. 2 and you see already some grey scaled coloring of the particle shower. Now we want to bring some color to the data. For this we add a Map Range Node to scale the attribute data to a range of one to zero (see Fig. 3). By inspecting the data I know the data in the <code>myAttributeFloat</code> attribute varies roughly between 0 and 13. So we use those values for the <code>From Min</code> and the <code>From Max</code> settings.</p> <p>Lastly, we will add some color by adding a ColorRamp Node. We select red and blue and we see that the cubes in the shower with low energy values color blue and those with higher energy, color red:</p> <p> Figure 3: adding some color and contrast to the material from Fig. 2. </p> <p>This is the basics when using custom attributes in your shaders. I used multiple of the showers from different data sources to make Fig. 1 and the animation in Fig. 4 below.</p> <p> Figure 4: animated version of multiple showers </p> <p>Pretty nice! I hope this helps.</p> <p>Cheers, Ben</p>"},{"location":"examples/spherical_harmonics/","title":"Animating Spherical Harmonics with Python and Shape Keys","text":"<p>Here's an example in the captivating world of scripting in Blender! We\u2019ll dive into the art of animating mesh vertices using Blender\u2019s Python API and Shape Keys. But this won\u2019t be your typical tutorial; we\u2019re about to bring mathematics to life. Picture this: spherical harmonic functions breathing life into a seemingly static sphere, making it dance to the rhythm of oscillating modes. Intrigued? Learn how to make the mesmerizing animation in Fig. 1 using Shape Keys and the Python API in Blender.</p> <p> Figure 1: an animation of Spherical Harmonics in three different modes. From left to right the modes are (l=3, m=0), (l=10, m=3) and (l=10, m=0) </p>"},{"location":"examples/spherical_harmonics/#introduction","title":"Introduction","text":"<p>Spherical harmonics are functions defined on the surface of a sphere. They appear in various natural phenomena, such as the mathematical description of the \u2018orbit\u2019 of an electron in a hydrogen atom. However, you do not need to know more about these functions to follow along in this blog. We will utilize the spherical harmonics available in the Python package SciPy. These functions will serve as standing waves on the surface of a sphere (see Fig. 1), similar to standing waves on a guitar string.</p>"},{"location":"examples/spherical_harmonics/#start-of-our-script","title":"Start of our script","text":"<p>To begin, we will import essential Python packages into our script. We need <code>bpy</code> to access the Blender Python API, <code>numpy</code> for various mathematical functions, and Blender's <code>mathutils</code> to work with vectors. Additionally, we need the spherical harmonics module from <code>scipy.special</code>.</p> <pre><code>import bpy\nimport numpy as np\nfrom scipy.special import sph_harm \nimport mathutils\n</code></pre> <p>Much like a string, a sphere can oscillate in different modes. The modes of spherical harmonics are defined using two integers: \\(l\\) and \\(m\\). Value \\(l\\) ranges from 0, 1, 2, 3, and so on, and for each \\(l\\) the value \\(m\\) can vary from \\(-l\\), .., 0, to \\(l\\). For instance, when \\(l\\) is set to 3, \\(m\\) can take on the values -3, -2, -1, 0, 1, 2, and 3. We will create two Python variables to define \\(l\\) and \\(m\\).</p> <pre><code>l, m = 3,1\n</code></pre> <p>Our objective is to visualize spherical harmonics as standing waves on a sphere. To achieve this, we\u2019ll start by creating a UV sphere in Blender using the code below. Afterward, we\u2019ll obtain the new sphere object by referencing the active object in Blender and assigning it a distinctive name using <code>obj.name</code>.</p> <pre><code>radius = 3\n\nbpy.ops.mesh.primitive_uv_sphere_add(segments=128, ring_count=128, \\\n               radius=radius, calc_uvs=True, enter_editmode=False, \\\n               align='WORLD', location=(0.0, 0.0, 0.0), \\\n               rotation=(0.0, 0.0, 0.0), scale=(1.,1.,1.))\n\nobj =  bpy.context.active_object\nobj.name = str(l)+\"_\"+str(m)\n</code></pre>"},{"location":"examples/spherical_harmonics/#getting-the-coordinates-to-work","title":"Getting the coordinates to work","text":"<p>The spherical harmonic function in SciPy uses polar coordinates, whereas Blender employs Cartesian coordinates. Hence, we must be capable of converting between polar and Cartesian coordinates, as well as performing the reverse conversion. It\u2019s not essential to have a deep understanding of these conversions, particularly the transformation from Cartesian to polar coordinates. (This is tricky near the poles of the sphere and mapping the entire sphere can be challenging, considering that trigonometric functions are defined on only a portion of the sphere.)</p> <pre><code>def getCart(r, theta, phi):\n    x = r * np.cos(phi) * np.sin(theta)\n    y = r * np.sin(phi) * np.sin(theta)\n    z = r * np.cos(theta)\n    return (x, y, z)\n\ndef getPolar(x, y, z):\n    # Round out small errors from the trigonometric functions\n    x, y, z = round(x, 5), round(y, 5), round(z, 5) \n\n    r = np.sqrt(x**2 + y**2 + z**2)\n    theta = np.arccos(z/r) #[0,pi]\n    xy = round(r * np.sin(theta), 5)\n    if xy == 0.0:\n        phi = 0.0\n    else:\n        if x &gt;= 0:\n            phi = np.arcsin(y/xy) #[-1/2pi, 1/2pi]\n        else:# x &lt; 0:\n            phi = np.arcsin(y/xy) #[-1/2pi, 1/2pi]  \n            phi = np.pi - phi\n    return (r, theta, phi)\n</code></pre> <p>In our case, we consider \\(\u03b8\\) as the angle measured from the z-axis, and \\(\u03d5\\) as the angle measured from the x-axis. It\u2019s necessary to round off some values because trigonometric functions don\u2019t consistently yield precise zeros. The \\(arccosine\\) function has a range from zero to \\(\u03c0\\), while the \\(arcsine\\) function covers only the range from \\(-1/2 \u03c0\\) to \\(1/2 \u03c0\\). As a result, we need to manually ensure that we account for both sides of the sphere, including the positive and negative sides of the x-axis.</p>"},{"location":"examples/spherical_harmonics/#shape-keys","title":"Shape Keys","text":"<p>Shape Keys are a method to alter the form of an object and are particularly useful for animation. You can access Shape Keys in the Properties editor under the Object Data tab and the Shape Keys tab (see Fig. 2).</p> <p> Figure 2: a screenshot of where the Shape Keys can be found in Blenders interface. The Shape Keys tab is located within the Object Data tab in the Properties Editor. </p> <p>Using the <code>+</code>` button, you can add Keys to the list. The initial Key you add is known as the Basis, representing the object\u2019s shape when all other Keys are set to zero. Adding additional Keys allows you to define alternative shapes. Ensure that the Key is selected in the Object Data tab and then switch to edit mode. Here, you can make adjustments, such as modifying the location of the vertices (in the next section we will do the same but through code).</p> <p>Each Key is associated with a numerical value (called Value in the Shape Key tab, see Fig. 2). When this value is set to zero, the Key has no impact on the object\u2019s shape. Setting it to 1 results in the object taking on the shape defined by that Key. Intermediate values produce shapes that interpolate between the Basis shape and the Key shape.</p> <p>You\u2019ll notice that the Value associated with the Key has a dot next to the field. This signifies that you can animate this value, thereby animating the object\u2019s shape.</p>"},{"location":"examples/spherical_harmonics/#using-shape-keys-in-the-python-api","title":"Using Shape Keys in the Python API","text":"<p>We\u2019ve created a sphere in Python, and we\u2019ve imported the spherical harmonics from the SciPy library. These spherical harmonic functions provide us with the amplitude (named \\(Amp\\) below) at every \\(\u03b8\\) and \\(\u03d5\\) coordinate of the vertices on the sphere. For the animation, we\u2019ll use these amplitudes within Blender\u2019s Shape Keys.</p> <p>First, we need to create the Basis and a second Shape Key programmatically in Python. Shape Keys are part of the object\u2019s data and can be added using the <code>obj.shape_key_add(name='Basis')</code> method. We then specify that we want linear interpolation between these Keys, and we want the Keys to be relative to the Basis Key (see the code below). Subsequently, we add a second Shape Key and set its interpolation to linear as well.</p> <pre><code># Make the basis shape key at minimum amplitude\nsk_basis = obj.shape_key_add(name='Basis')\nsk_basis.interpolation = 'KEY_LINEAR'\nobj.data.shape_keys.use_relative = True#False\n\n# The next shape key is at max amplitude\nsk2 = obj.shape_key_add(name='Deform')\nsk2.interpolation = 'KEY_LINEAR'\n</code></pre> <p>Next, we proceed to modify the vertices within both Shape Keys. To achieve this, we iterate over the vertices of the object (<code>obj.data.vertices</code>). We convert the Cartesian coordinates of these vertices into polar coordinates and calculate the amplitude using spherical harmonics. Since spherical harmonics are complex functions, it\u2019s crucial to consider either the real or imaginary part of the function.</p> <pre><code>for i in range(len(obj.data.vertices)):\n    x, y, z = obj.data.vertices[i].co\n\n    r, theta, phi = getPolar(x, y, z)\n\n    Amp = sph_harm(m, l, phi, theta).imag if m&lt;0 \\\n            else sph_harm(m, l, phi, theta).real\n\n    VertMin = mathutils.Vector(getCart(r - Amp, theta, phi))\n    VertMax = mathutils.Vector(getCart(r + Amp, theta, phi))\n\n    sk_basis.data[i].co = VertMin\n    sk2.data[i].co = VertMax\n</code></pre> <p>Subsequently, we calculate the vertex positions corresponding to the minimum amplitude (\\(r - Amp\\)) and maximum amplitude (\\(r + Amp\\)). Afterward, we convert these positions back into Cartesian coordinates and update the vertex positions in the \u2018Basis\u2019 Key to reflect the minimum values and in the second Key to represent the maximum values. As a result, we\u2019ve successfully created our Shape Keys.</p> <p>After running the script, you can navigate to the Object Data properties and adjust the <code>Value</code> associated with the second Key (see Fig. 2) to observe the resulting spherical harmonic transformation.</p>"},{"location":"examples/spherical_harmonics/#animation","title":"Animation","text":"<p>XXX In an upcoming blog, I will demonstrate how to automate the animation of these spheres using the Python API. However, for this blog, we will create animations manually in order to achieve what you see in Fig. 1. The simplest way to proceed is by navigating to the Animation Workspace. At the bottom of the interface, you\u2019ll find the \u2018Dope Sheet\u2019 (see Fig. 3). I\u2019ll assume you\u2019re already familiar with animating objects; if not, you can refer to our course site for guidance.</p> <p> Figure 3: the Dope Sheet Editor. </p> <p>Begin by setting the current frame to 0. Now select the \u2018Deform Shape\u2019 in the Object Data Properties tab and set the \u2018Value\u2019 field to zero (see Fig. 2). Use the dot next to the \u2018Value\u2019 field (see Fig. 2) to create a keyframe. You\u2019ll notice a keyframe marker appearing at frame 0 on the timeline in the Dope Sheet, indicating the creation of a keyframe.</p> <p>Proceed to frame 20, set the \u2018Value\u2019 for the Shape Key to 1, and press the diamond-shaped icon (formerly a dot) located next to the \u2018Value\u2019 field to create another keyframe. Repeat this process for frame 40, setting the \u2018Value\u2019 back to zero. Finally, set the \u2018End\u2019 frame of the animation to 40. You can locate this setting at the bottom-right corner of the Dope Sheet (as depicted in Fig. 3). Now, start the animation and enjoy the result.</p>"},{"location":"examples/spherical_harmonics/#materials","title":"Materials","text":"<p>If you\u2019re curious about the type of material I\u2019ve used to create Fig. 1, it\u2019s actually quite straightforward. Navigate to the Shading Workspace and add a Glass BSDF shader. Then, connect it to the output. For information regarding the shader\u2019s settings, see Fig. 4.</p> <p> Figure 4: the shader nodes used for the animation in Fig. 1. </p>"},{"location":"examples/spherical_harmonics/#conclusion-and-exercises","title":"Conclusion and exercises","text":"<p>I hope this blog has helped you understand what Shape Keys and the Python API can do for you in Blender. It would be great if you could apply these techniques to your scientific visualization or any type of visualization project you are working on. I would love to hear from you and see what you have created. In the next blog we will explore how we can automate the animation and shading of these spheres using the Python API in Blender. Until then, keep creating and exploring!</p> <p>Cheers, Ben</p>"},{"location":"news/","title":"News","text":""},{"location":"news/2023/09/19/new-courses-being-planned-for-q4-2023/","title":"New courses being planned for Q4 2023","text":"<p>We are in the process of finalizing dates for a new set of Basics and Advanced Blender courses at the end of 2023. These will be held online. Watch this news section, or the schedule.</p>"},{"location":"news/2023/11/20/basics-course-starting-4-december-2023/","title":"Basics course starting 4 December 2023","text":"<p>A new Basics course will be held starting 4 December 2023.  The course is self-paced using our online materials, supported by a kick-off meeting followed by  weekly check-in moments. The course runs over a 3-week period and will be held online.</p> <p>See the schedule for precise dates and times. You can register for the course through this page.</p>"},{"location":"news/2024/04/03/advanced-course-starting-25-april-2024/","title":"Advanced course starting 25 April 2024","text":"<p>A new Advanced course will be held starting 25 April 2024, running until 3 May. </p> <p>The course is self-paced using our online materials, supported by a kick-off meeting followed by  weekly check-in moments. The course runs over a 2-week period and will be held online, with support from SURF during the course period.</p> <p>You can register for the course through this page.</p>"},{"location":"news/2024/04/03/basics-course-in-person-on-23-april-2024/","title":"Basics course (in-person) on 23 April 2024","text":"<p>A new Basics course will be held on 23 April 2024 at SURF Amsterdam. This is a full-day in-person course.</p> <p>You can register for the course through this page.</p>"},{"location":"news/2024/08/28/basics-course-in-person-on-15-october-2024/","title":"Basics course (in-person) on 15 October 2024","text":"<p>A new Basics course will be held on 15 October 2024 at SURF Amsterdam. This is a full-day in-person course.</p> <p>You can register for the course through this page.</p>"},{"location":"overview/about/","title":"About us","text":"<p>We are members of the High-Performance Computing &amp; Visualization (HPCV) group at SURF, and are based in Amsterdam. SURF is a cooperative association of Dutch educational and research institutions  in which the members combine their strengths to acquire or develop digital services, and to encourage knowledge  sharing through continuous innovation.  </p> <p>Within the HPCV group we support users of the Dutch National compute infrastructure with visualization  expertise and software development, on topics such as data visualization, remote visualization, 3D modeling and rendering and use of eXtended Reality (XR) for research and education.</p> <p>Part of our jobs is to provide courses on topics related to visualization in HPC. This Blender course was originally created for the PRACE Training Center and first provided (in-person) in 2018,  and has since been repeated at least once a year. The course is currently organized as part of the  EuroCC The Netherlands training activities.</p>"},{"location":"overview/about/#paul-melis","title":"Paul Melis","text":"<p>Paul Melis has an MSc in Computer Science from the University of Twente in The Netherlands and worked on topics in scientific visualization  and VR at the University of Groningen and University of Amsterdam before joining SURFsara in 2009 (which has since become part of SURF).</p> <p>At SURF he is involved in several activities related to visualization, including realizing visualization projects for end-users, teaching courses and providing user support for visualization tasks on our HPC systems. As part of the SURF innovation portfolio he is involved in the use of extended reality (XR) for research and education. He likes to use Blender for all things 3D, but also works with ParaView, and sometimes develops a bit of code in Python, C++ or Julia.</p>"},{"location":"overview/about/#casper-van-leeuwen","title":"Casper van Leeuwen","text":"<p>Casper has a MSc in Computer Science from Delft University of Technology where he graduated on the topic of medical visualization. He has been at SURFsara since 2014.</p> <p>He mainly works on web-based 2D/3D visualization, including Jupyter Notebooks and loves to work on Blender projects when the goal is to make something look aesthetic! Besides that he also knows his way around Unity and Unreal Engine.</p>"},{"location":"overview/about/#ben-de-vries","title":"Ben de Vries","text":"<p>Ben de Vries has a PhD in Astrophysics from KU Leuven. He joined SURF in 2019. He focuses on 2D/3D visualization projects using Blender, Unity and general 3D programming.</p>"},{"location":"overview/contents/","title":"Contents","text":"<p>This Blender course consists of two parts, that are each taught separately online over the course of a number of weeks:</p> <ul> <li> <p>In the Basics module we assume no prior knowledge of Blender. We will introduce  Blender from the ground up, starting with the user interface and basic functionality. We cover the 3D scene, cameras,  lights and materials and some basic mesh editing.</p> <p>It helps to have some familiarity with basic 3D graphics concepts, such as 3D geometry, transformations and rendering. But if not, you will probably pick those up quite quickly during the course.</p> </li> </ul> <ul> <li> <p>In the Advanced module of the course we assume participants already have basic knowledge of Blender, preferably by following our basics course. We assume participants are familiar with the Blender user interface, basic functionality and concepts like the 3D scene, cameras, lights, materials and some basic mesh editing.</p> <p>The advanced part goes into detail on the Python API for scripting, node-based materials, mesh editing and first steps in animation. The main goal of the Advanced course is for you to realize your own project with Blender, based on data you choose.</p> </li> </ul>"},{"location":"overview/contents/#blender-version","title":"Blender version","text":"<p>We currently use Blender 4.1 for this course and the materials provided. This also means that any links to the Blender manuals are for that specific version, to be consistent.</p> <p>Blender as a software package is a fast moving target, usually with lots of shiny new features and bug fixes in each release (and multiple releases per year). This is great, of course, but with each release usually also a lot of small tweaks and improvements are made, especially in the  user interface and workflow. </p> <p>Course videos using previous Blender versions</p> <p>Some of the videos used in the course might still show an earlier Blender version. In those cases we have estimated that the video is still (largely) up-to-date and have chosen not to update the video, as this is quite time-consuming.    </p>"},{"location":"overview/contents/#lts-versions","title":"LTS versions","text":"<p>We originally planned to only base this course on the Blender LTS (Long-Term Support) releases, which remain more-or-less unchanged regarding UI and features for roughly 2 years. But each new release brings some major and minor improvements, that would only become available in the next LTS release much later. Hence, we chose to update the course more regularly.</p>"},{"location":"overview/contents/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>A system (PC or laptop) to work on. This can be a Linux, macOS or Windows system. It is preferred to use a system with a somewhat recent GPU (or at most 10 years old) with working OpenGL 4.3 support. See the section \"Hardware Requirements\" on this page for the official requirements for running Blender.</li> </ul> <ul> <li> <p>Blender 4.1 installed on the above system. You can download it from here, or you can use your system package manager to install it. </p> <p>A Linux distribution's package for Blender</p> <p>Sometimes a the blender package from a distro gets built with slightly different versions of software libraries, compared to the official Blender distribution. This is known to sometimes cause different behaviour or even bugs, for example in the handling of video files by the FFmpeg library. In case you find strange issues or bugs with your distro's Blender you might want to try downloading the official Blender binaries to see if that fixes those issues.</p> <p>Minor version difference okay</p> <p>It is in general not recommended to use a wildly different Blender version for this course, due to possible mismatches in the user interface and functionality with the course material. A different minor version, e.g. 4.1.2 instead of 4.1.0 if it should become available, should not cause issues, but a future 4.x release might have some major changes.</p> </li> </ul> <ul> <li>Please test the Blender installation before the course starts using the instructions sent by e-mail. This will tell you if Blender is working correctly and can save you (and us) time fixing any system-related issues during the course period.</li> </ul> <p>Recommended:</p> <ul> <li>Using a 3-button mouse is preferred, as not all Blender functionality is easily used through 2-button mouse or laptop track-pad.</li> </ul>"},{"location":"overview/conventions/","title":"Text conventions","text":"<p>The conventions on these pages follow those used in the official Blender 4.1 documentation as much as possible:</p> <ul> <li>Keyboard and mouse actions, menu names, literal text to enter, etc. are shown in monospaced bold. Examples: <code>X</code> or <code>Shift-MMB</code>. Note that an upper-case letter indicates that particular key (e.g. <code>A</code> means the key with A on it), while entering an uppercase letter would be indicated with a shifted letter (e.g. <code>Shift+A</code>).</li> </ul> <ul> <li><code>LMB</code> = left mouse button, <code>MMB</code> = middle mouse button, <code>RMB</code> = right mouse button, <code>Wheel</code> = scrolling the mouse wheel</li> </ul> <ul> <li>Menu actions are shown as <code>View &gt; Cameras &gt; Set Active Object as Camera</code>, for View menu, Cameras submenu, \"Set Active Object as Camera\" option.</li> </ul>"},{"location":"overview/conventions/#exercises","title":"Exercises","text":"<p>We highlight exercise sections by prefixing their titles with a \ud83d\udcbb symbol.</p>"},{"location":"overview/data_files/","title":"Data files","text":"<p>Most of the exercises require you to load a Blender scene file that we provide. These files can be  found at https://edu.nl/8n7en. </p> <p>It is best to download the full content of the share to your local system using the <code>Download</code> button in the upper-right.</p> <p>This share contains:</p> <ul> <li><code>data</code> - Blender files (and other data) for the assignments, split into <code>basics</code> and <code>advanced</code> parts, with a sub-directory per chapter</li> <li><code>slides</code> - The slides (in PDF)</li> <li><code>walkthroughs</code> - Some of the files used in the videos, again split split by basics and advanced</li> <li><code>cheat-sheat-4.1.pdf</code> - A 2-page cheat sheet with often used operations and their short cuts</li> </ul> <p>Data file size</p> <p>The current size of the data share to download is quite high (\u2248 1 GB), partly caused by having the data files for both the Basics and Advanced modules combined. </p> <p>This is something we're aware of and want to improve in the near future, by separating the data in two parts for the different modules.</p>"},{"location":"overview/issues_and_feedback/","title":"Issues &amp; feedback","text":""},{"location":"overview/issues_and_feedback/#issues-with-course-materials","title":"Issues with course materials","text":"<p>We try to keep this course up to date to match the specific Blender version mentioned under Contents. But we might have missed small things. Also, since the videos we use take substantial effort to create and keep up-to-date we don't update them in the same pace as these written pages.</p> <p>If you do find issues with the materials then please let us know through Github by reporting an issue.</p> <p>If you don't have a Github account, or would rather not create one, then telling us through Discord is fine as well.</p>"},{"location":"overview/issues_and_feedback/#feedback","title":"Feedback","text":"<p>We always like to hear what you think of the course! We usually ask for feedback on the course in the online sessions, but if you have remarks then please let us know. You can do this either through Github by reporting an issue, or in the Discord sessions.</p>"},{"location":"overview/schedule/","title":"Schedule","text":""},{"location":"overview/schedule/#basics-course-15-october-2024-in-person","title":"Basics course (15 October 2024, in-person)","text":"When What Where Purpose  Tue 15-10-24 09:30 - 17:00 SURF Amsterdam Full-day introductory course <p>Registration link</p>"},{"location":"overview/schedule/#advanced-course","title":"Advanced course","text":"<p>Currently not planned.</p>"},{"location":"overview/setup/","title":"Course setup","text":"<p>We use a combination of different media within the course to guide you, but  the basis is for you to follow the training in your own pace over a period of two weeks. During this period we provide support where needed.</p> <p>The online material consists of:</p> <ul> <li>Videos that introduce and demonstrate new concepts and features within Blender.</li> <li>Slides (also presented as part of the videos) for explanations. These are basically presentations we would otherwise do plenary.</li> <li>Exercises for you to explore new topics and to train your skills, based on example files we provide.</li> </ul> <p>During a course period we schedule a few short plenary online sessions where you can ask questions, and for us to provide general feedback and/or guidance. </p> <p>Support during specific course periods</p> <p>Although this course material is available online at any time, we only provide the support mentioned at scheduled course periods throughout the year. Please check the EuroCC Training Agenda when the next Blender course is scheduled.</p>"},{"location":"overview/setup/#support","title":"Support","text":"<p>During the course period we provide support through our Discord server, see this page. On Discord there's a plenary chat channel, but also the possibility to have a 1-on-1 video chat in cases where we need to look more closely over your shoulder to solve a particular issue.</p>"},{"location":"overview/support/","title":"Support","text":"<p>Detailed interaction and support during the course periods is provided through our Discord server (and mail as a backup).  Here you can ask questions by chat, upload an image or, if needed, start a video session or share your screen with one of us.</p> <p>Support hours</p> <p>We will be active on Discord during office hours (CET time zone) and will try to also be on-line outside of those hours. Note that this is all on a best-effort basis.</p> <p></p> <p>Depending on the course you're following (basics or advanced) you need to use the category called <code>BASICS BLENDER COURSE</code> or <code>ADVANCED BLENDER COURSE</code>, respectively. Within these categories you will find two support channels:</p> <ul> <li>A shared text chat channel (e.g. <code>2023-12-blender-basics</code>) for interacting with the course teachers and other course participants. Here you can ask questions, show your work, or anything else you feel like sharing.</li> <li>A video channel (<code>video channel</code>), in case we want to share something through Discord</li> </ul> <p>For one-on-one contact, including the option for screen sharing, right-click on one of our names as shown in the picture above and pick either the button for voice chat or video chat.</p>"},{"location":"overview/support/#outside-of-course-periods","title":"Outside of course periods","text":"<p>Support requests outside of scheduled course periods is done a best-effort basis.</p>"},{"location":"overview/target_audience/","title":"Target audience","text":"<p>This course is aimed at scientists and researchers of all levels. We don't make many assumptions on use cases for Blender, but do assume the context to be an academic setting. So we won't go into creating visual effects for putting a massive CGI tornado in your backyard that scoops up your neighbours. But if you happen to write a tornado simulation for your research we will be more than happy to see how we can use Blender to make attractive visuals of the data. </p> <p>This doesn't mean that we only assume to apply Blender to existing scientific data. Sometimes certain concepts are best explained by creating a 3D scene, say to produce a nice looking cover image for your PhD thesis, or to illustrate or visualize a certain concept. </p>"},{"location":"overview/target_audience/#bring-your-own-data","title":"Bring your own data!","text":"<p>From previous editions of the course we know many participants bring their own data and want to apply Blender to it.  We encourage you to do that as well, as it will also help in providing some focus to your use of Blender.</p>"},{"location":"overview/time_investment/","title":"Time investment","text":"<p>The precise amount of time needed to follow this course depends for a large part on how much effort you  devote to each topic, your available time, your learning pace, etc. As a reference, the in-person Basics course  is a full-day course (with a moderately high pace).</p> <p>For the Basics course the time spent on the different subjects and their assignments in that setup is shown below. This might give you some idea on the relative depth of the topics.</p> Topic Time in schedule (in-person course) Videos (this course) Introduction 30 minutes 5 minutes Blender basics 120 minutes 45 minutes Importing data 30 minutes 30 minutes Rendering, lighting &amp; materials 105 minutes 65 minutes Simple mesh editing 30 minutes 20 minutes <p>For the Advanced course it is hard to give a general indication of the expected time investment needed for the course. It depends partially on your own goals and ambitions for the main task: the project of visualizing your own data in the way you see fit. </p> <p>In terms of topics the Advanced materials and Animation chapters are relatively straightforward and can probably be completed in a day. In contrast, Python scripting in Blender is a very extensive topic and can end up taking a lot of time if you want to work with the more complex parts of the API.</p>"},{"location":"references/cheat_sheet/","title":"Cheat sheet","text":"<p>With this course we provide a 2-page cheat sheet that lists basic and often-used operations and their shortcut keys. It also includes a summarize of major interface elements.</p> <p>The cheat sheet can be found here as a double-sided PDF, which can easily be printed.</p>"},{"location":"references/community/","title":"Community resources","text":"<p>On blenderartists.org lots of Blender users and artists are hanging out. There you can ask questions or feedback, show off your work or check out the vast amount of knowledge, tips and Blender renderings in the forums.</p> <p>BlenderNation gathers information on different topics and includes video tutorials, blog posts on art created with Blender and a lot more.</p> <p>The Blender subreddit contains many different posts, ranging from simple questions to artists show off their amazing work.</p> <p>Well-known artists and gurus working with Blender are:</p> <ul> <li>Jan van den Hemel shares many tips and tricks through Twitter, both on Blender usage as well as making a scene look a certain way. He also publishes these tricks in an e-book.</li> <li>Andrew Price (twitter) aka \"Blender Guru\" provides many cool tutorials on https://www.blenderguru.com/ and his YouTube channel. He is well-known for a multi-part tutorial series on modeling a realistic donut!</li> <li>Glex Alexandrov (twitter and twitter) aka \"Creative shrimp\" has some very creative and inspirational tutorials on his YouTube channel.</li> <li>Ian Hubert (YouTube and twitter), famous for his Lazy tutorials (very efficient 1 minute tutorials), has videos on advanced green screen techniques and VFX in Blender.</li> <li>Simon Thommes (twitter and YouTube) is a materials wizard, he is able to create complex geometry out of one cube or sphere with just the Shader editor.</li> <li>Steve Lund has some great Blender tutorials on his YouTube channel.</li> <li>Zach Reinhardt has some great modeling, texturing and VFX tutorials on his YouTube channel</li> <li>Peter France is the Blender artist at the Corridor Crew which just started his own YouTube channel with some instructive tutorials.</li> <li>YanSculpts does not fit this course material perse but it goes to show how versatile Blender can be, this artist creates some amazing sculptures in Blender of which he shows the process on his YouTube channel.</li> <li>Josh Gambrell shares a lot of tips and tricks for advanced mesh editing on his youtube channel (mostly hard surface modeling).</li> </ul>"},{"location":"references/interface/","title":"User Interface elements","text":"<p>The default layout of the Blender user interface is shown below. Note that the layout is fully  configurable.</p> <p></p> <p>* Scene statistics</p> <p>By default the status bar at the bottom only shows the Blender version number. You can add extra statistics, such as the number of 3D objects in the scene and memory usage in the preferences. </p> <p>You can either right-click on the status bar to enable display of extra values. Or use the application menu <code>Edit &gt; Preferences</code>, select the <code>Interface</code> tab, in the <code>Editors &gt; Status Bar</code> section and check all marks (<code>Scene Statistics</code>, <code>Scene Duration</code>, <code>System Memory</code>, <code>Video Memory</code>, <code>Blender Version</code>).</p>"},{"location":"references/interface/#editor-type-menu","title":"Editor type menu","text":"<p>The yellow highlight indicates often used ones for this course</p> <p></p>"},{"location":"references/official/","title":"Official sources","text":"<p>The official home for Blender is blender.org</p>"},{"location":"references/official/#manuals","title":"Manuals","text":"<p>The Blender Reference Manual for version 4.1 can be found here. The documentation on the Python API is here.</p> <p>Access help from within Blender</p> <p>You can open the Blender documentation pages from within Blender itself, using the options in the <code>Help</code> menu.</p>"},{"location":"references/official/#demo-files","title":"Demo files","text":"<p>Official demo files showing off lots of cool features and scenes can be found here, including the scene files used to render the splash images of different Blender versions.</p>"},{"location":"references/official/#blender-development-and-news","title":"Blender development and news","text":"<p>If you are interested in following recent development in Blender  then the weekly Blender Today Live sessions on YouTube are a good resource.</p> <p>Videos on lots of different topics, including videos from the yearly Blender Conference, can be found on the official Blender YouTube channel.</p> <p>Blender has official accounts on Mastodon an Twitter/X. The hashtag to use for Blender is <code>#b3d</code> (although sometimes also <code>#blender</code>).</p>"},{"location":"references/official/#mastodon","title":"Mastodon","text":"<p>On Mastodon the official account is @blender@mastodon.social.</p>"},{"location":"references/official/#twitterx","title":"Twitter/X","text":"<p>On Twitter you can follow @Blender for official Blender news or @BlenderDev for more in-depth development information. </p>"},{"location":"references/scene/","title":"Scene resources (3D models, materials, textures)","text":"<p>Here we list a number of online resources for 3D models, textures, shaders, etc.</p> <p>In general certain 3D models might be free for download, while others might only be available paid (usually for a small amount).  Usually, the nicer the 3D model the higher the cost. Also, different licenses are used for the models and these will describe how you can use the models and any attribution you might need to give when using it.</p>"},{"location":"references/scene/#official-demo-file-and-examples","title":"Official demo file and examples","text":"<p>Blender provides a set of demo files, either made by artists or to demonstrate new features. They can be found here.</p> <p>These contain interesting (and sometimes complex) scene files demonstrating particular features, such as simulation and geometry nodes or hair. You can also find the Blender files used for each release's splash screen here.</p> <p>An asset bundle with various human base meshes is made available, containing generic human bodies, heads and skulls, hands and feet, and eyeballs. The assets can be found here. This YouTube video explains how to use them.</p>"},{"location":"references/scene/#3d-models","title":"3D Models","text":"<ul> <li>Turbosquid is one of the oldest 3D model websites and provides models in all sort of topics, some free, some paid. </li> </ul> <ul> <li>Sketchfab hosts a large collection of 3D models from many different categories. Many 3D models are textured and some are even animated.</li> </ul> <ul> <li>3D Model Haven distributes freely usable 3D models, many of them textured. It is not as extensive as other websites, but the upside is that all models can be freely used.</li> </ul> <ul> <li>CGTrader also hosts many 3D models, some of them free, some paid</li> </ul> <ul> <li>There's a section on BlenderNation where Blender models are shared. Again, some of these might be free, others will involve some payment.</li> </ul> <ul> <li>BlenderMarket contains a section with 3D models</li> </ul> <ul> <li>Quixel's Megascans is a great \"paid\" source for 3D models as well as textures which can be used for free when it's attached to an Epic account and the assets are only used for an Unreal Engine application. It's great for personal use but if you publish anything containing an asset from Quixel without Unreal Engine attached to it you have to pay for the asset.</li> </ul>"},{"location":"references/scene/#textures-and-images","title":"Textures and images","text":"<ul> <li>Texture Haven provides textures to be used in materials and shaders. All textures available are free.</li> </ul> <ul> <li>CC0 Textures has many high-quality textures</li> </ul> <ul> <li>BlenderMarket has a section with shaders, materials and textures.</li> </ul> <ul> <li>HDRI Haven is similar to Texture Haven, but contains many freely available HDRI 360 images that can be used for realistic environment lighting in Blender</li> </ul> <ul> <li>Poliigon, where the CEO is the Blender Guru himself, has some great looking free samples and otherwise high quality paid textures.</li> </ul> <ul> <li>textures.com has some high quality, high resolution, movie grade textures under a paid subscription or credit-based payment model.</li> </ul>"},{"location":"references/scene/#blenderkit","title":"BlenderKit","text":"<p>BlenderKit is an online repository of materials, 3D models and a few other things. It used to come bundled with Blender as an add-on, but since Blender 3.0 this is no longer the case. You need to download and install the add-on yourself, for which instructions can be found here.</p> <p>When the add-on is installed and enabled it provides some extra elements in the Blender interface for searching, say a material or 3D model, by name, which can then be easily used in a Blender scene:</p> <p></p> <p>Note that many of the assets in BlenderKit are free, but some are only available by buying a subscription.</p> <p>The add-on has quite a few options and performs certain operations that you would otherwise do manually or maybe not use at all. As such, it can set up the scene in more exotic ways, for  example by linking to another Blender file. Also, the materials provided by BlenderKit can use pretty complex shader graphs, involving multiple layers of textures, or advanced node setups.</p> <p>Warning</p> <p>When applying a BlenderKit material on your own object the rendering might not look like the material preview in all cases. Especially use of displaced materials involves specific settings for the Cycles renderer and use of subdivision on the object.</p> <p>Warning</p> <p>Textures from BlenderKit are by default stored in a separate directory on your system (<code>~/blenderkit_data</code> on Linux). There is an option to pack the textures within the Blender file, making it larger in size but also completely independent of any external files, which is useful if you want to transfer the Blender file to a different system. The option for packing files is <code>File &gt; External Data &gt; Pack All into .blend</code>.</p>"},{"location":"news/archive/2024/","title":"2024","text":""},{"location":"news/archive/2023/","title":"2023","text":""}]}